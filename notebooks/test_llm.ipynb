{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1686756-542e-4bee-a8d5-853cd856da1d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80554bb-2109-4c67-ad73-eb083a06f21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding /Users/yonatanlou/dev/meaningful-transparency/src to sys.path\n"
     ]
    }
   ],
   "source": [
    "# for importing code\n",
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\", \"src\"))\n",
    "if module_path not in sys.path:\n",
    "    print(f\"Adding {module_path} to sys.path\")\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75032f2-d3fd-49c5-8046-4e4127edc3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Username",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreateDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Biased",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Keyword",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_words_no_urls_mentions_hashtags",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "extracted_urls",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flag_link_only",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "flag_starts_with_mention",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "n_mentions_in_text",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_hashtags_in_text",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_urls_in_text",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "no_text_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flag_empty_after_cleanup",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "flag_non_twitter_url",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "flag_twitter_url",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "Text_core",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "14092dad-fcce-4d36-941b-a877c98dc918",
       "rows": [
        [
         "3056",
         "1227888265887440897",
         "Hasan08931491",
         "2020-02-13 09:32:30+00:00",
         "1",
         "ZioNazi",
         "#Apartheid zionazist... Ethnic cleansing, land seizure, home demolition, military occupation, bombing of Gaza.. Israel is copying South Africa very well  #SobatPalestinaINA  #IsraeliCrimes https://t.co/vYnnnQIG8U",
         "2020",
         "23",
         "19",
         "['https://t.co/vYnnnQIG8U']",
         "False",
         "False",
         "0",
         "3",
         "1",
         "0.1739130434782608",
         "False",
         "False",
         "True",
         "zionazist... Ethnic cleansing, land seizure, home demolition, military occupation, bombing of Gaza.. Israel is copying South Africa very well"
        ],
        [
         "3417",
         "1393637799597801474",
         "mir_638",
         "2021-05-15 18:41:58+00:00",
         "1",
         "Israel",
         "RT @MohsinIslam: Israel is using internationally prohibited bullets known as Expanding or Dumdum Bullets against Palestinians.  The bullets expand to combat over-penetration and to produce a larger wound to destroy organs and break bones #GazaUnderAttack #IsraeliTerrorism https://t.co/4lW3YWgGNz",
         "2021",
         "37",
         "34",
         "['https://t.co/4lW3YWgGNz']",
         "False",
         "False",
         "1",
         "2",
         "1",
         "0.1052631578947368",
         "False",
         "False",
         "True",
         "RT : Israel is using internationally prohibited bullets known as Expanding or Dumdum Bullets against Palestinians. The bullets expand to combat over-penetration and to produce a larger wound to destroy organs and break bones"
        ],
        [
         "2167",
         "1104601295523000323",
         "mps051960",
         "2019-03-10 04:34:06+00:00",
         "1",
         "Kikes",
         "We'll soon hear Kikes in our time. She &amp; they know how to alienate classes of American's. Insane thinking is a direct result of TDS. 2020 Dems will eat own members after election loss, just like Rat's do... https://t.co/BdnFSjeHRM",
         "2019",
         "39",
         "38",
         "['https://t.co/BdnFSjeHRM']",
         "False",
         "False",
         "0",
         "0",
         "1",
         "0.0256410256410256",
         "False",
         "False",
         "True",
         "We'll soon hear Kikes in our time. She &amp; they know how to alienate classes of American's. Insane thinking is a direct result of TDS. 2020 Dems will eat own members after election loss, just like Rat's do..."
        ],
        [
         "3274",
         "1395156785028222978",
         "agarfilms",
         "2021-05-19 23:17:52+00:00",
         "1",
         "Israel",
         "RT @rafaelshimunov: The Israeli Interior Ministry maintains a list of demographics in Gaza. So during approval process to bomb, the info can be easily available to them, maybe explaining the disproportionate targeting of homes with large families, wiping them out entirely.  https://t.co/unpqNNGJ2z",
         "2021",
         "42",
         "41",
         "['https://t.co/unpqNNGJ2z']",
         "False",
         "False",
         "1",
         "0",
         "1",
         "0.0465116279069767",
         "False",
         "False",
         "True",
         "RT : The Israeli Interior Ministry maintains a list of demographics in Gaza. So during approval process to bomb, the info can be easily available to them, maybe explaining the disproportionate targeting of homes with large families, wiping them out entirely."
        ],
        [
         "2316",
         "1228143125643714561",
         "Suket_T3k1",
         "2020-02-14 02:25:13+00:00",
         "1",
         "ZioNazi",
         "#Apartheid zionazist...\\nEthnic cleansing, land seizure, home demolition, military occupation, bombing of Gaza..\\nIsrael is copying South Africa very well\\n\\n#SobatPalestinaINA \\n#BelaBaitulMaqdis_AlQuds\\nhttps://t.co/JXXuu7oas7",
         "2020",
         "19",
         "18",
         "['https://t.co/JXXuu7oas7']",
         "False",
         "False",
         "0",
         "1",
         "1",
         "0.1",
         "False",
         "False",
         "True",
         "zionazist...\\nEthnic cleansing, land seizure, home demolition, military occupation, bombing of Gaza..\\nIsrael is copying South Africa very well\\n\\n#SobatPalestinaINA \\n#BelaBaitulMaqdis_AlQuds\\n"
        ],
        [
         "1876",
         "1395295293030207494",
         "SallyfromSaar",
         "2021-05-20 08:28:15+00:00",
         "1",
         "Jews",
         "Such growing noble sentiments by US Jews/Israelis may be the only light at the end of the tunnel to this ongoing tragedy https://t.co/RpWJHaKb91",
         "2021",
         "23",
         "22",
         "['https://t.co/RpWJHaKb91']",
         "False",
         "False",
         "0",
         "0",
         "1",
         "0.0434782608695652",
         "False",
         "False",
         "True",
         "Such growing noble sentiments by US Jews/Israelis may be the only light at the end of the tunnel to this ongoing tragedy"
        ],
        [
         "3001",
         "1102700703682715648",
         "iclayca",
         "2019-03-04 22:41:49+00:00",
         "1",
         "Kikes",
         "RT @USSLibincident: Ethno nationalism is only considered ok when kikes do it. https://t.co/bbwr7zFNlN",
         "2019",
         "13",
         "12",
         "['https://t.co/bbwr7zFNlN']",
         "False",
         "False",
         "1",
         "0",
         "1",
         "0.1428571428571428",
         "False",
         "False",
         "True",
         "RT : Ethno nationalism is only considered ok when kikes do it."
        ],
        [
         "3058",
         "1227929698040074240",
         "GunGunG49169853",
         "2020-02-13 12:17:08+00:00",
         "1",
         "ZioNazi",
         "#Apartheid zionazist... Ethnic cleansing, land seizure, home demolition, military occupation, bombing of Gaza.. Israel is copying South Africa very well  #SobatPalestinaINA  #IsraeliCrimes https://t.co/vYnnnQIG8U",
         "2020",
         "23",
         "19",
         "['https://t.co/vYnnnQIG8U']",
         "False",
         "False",
         "0",
         "3",
         "1",
         "0.1739130434782608",
         "False",
         "False",
         "True",
         "zionazist... Ethnic cleansing, land seizure, home demolition, military occupation, bombing of Gaza.. Israel is copying South Africa very well"
        ],
        [
         "3434",
         "1392400276842323968",
         "ZohaibKhalidPT",
         "2021-05-12 08:44:30+00:00",
         "1",
         "Israel",
         "Retweet and show the world if you agree with me that Israel is a terrorist State and killing so many Innocent #Palestine #GazaUnderAttack  #IsraeliTerrorism #TelAviv   #AllahuAkbar #Hamas #IsraeliTerrorism  #PalestinianLivesMatter  #ÙÙ„Ø³Ø·ÙŠÙ†_Ù‚Ø¶ÙŠØªÙ†Ø§_Ø§Ù„Ø£ÙˆÙ„Ù‰  #FreePalestine https://t.co/hhQ7feQduJ",
         "2021",
         "32",
         "22",
         "['https://t.co/hhQ7feQduJ']",
         "False",
         "False",
         "0",
         "10",
         "1",
         "0.3333333333333333",
         "False",
         "False",
         "True",
         "Retweet and show the world if you agree with me that Israel is a terrorist State and killing so many Innocent Ù„Ø³Ø·ÙŠÙ†_Ù‚Ø¶ÙŠØªÙ†Ø§_Ø§Ù„Ø£ÙˆÙ„Ù‰"
        ],
        [
         "2238",
         "1152595302274105345",
         "kiwianna111",
         "2019-07-20 15:05:08+00:00",
         "1",
         "ZioNazi",
         "@reallygraceful More BS from the delusional paranoid psychopathic thieving zionazi sinagogue of satan,  You cannot be a thief without being a liar as well. https://t.co/3EiR0vHOlP",
         "2019",
         "25",
         "23",
         "['https://t.co/3EiR0vHOlP']",
         "False",
         "True",
         "1",
         "0",
         "1",
         "0.08",
         "False",
         "False",
         "True",
         "More BS from the delusional paranoid psychopathic thieving zionazi sinagogue of satan, You cannot be a thief without being a liar as well."
        ]
       ],
       "shape": {
        "columns": 20,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Username</th>\n",
       "      <th>CreateDate</th>\n",
       "      <th>Biased</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Text</th>\n",
       "      <th>Year</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_words_no_urls_mentions_hashtags</th>\n",
       "      <th>extracted_urls</th>\n",
       "      <th>flag_link_only</th>\n",
       "      <th>flag_starts_with_mention</th>\n",
       "      <th>n_mentions_in_text</th>\n",
       "      <th>n_hashtags_in_text</th>\n",
       "      <th>n_urls_in_text</th>\n",
       "      <th>no_text_ratio</th>\n",
       "      <th>flag_empty_after_cleanup</th>\n",
       "      <th>flag_non_twitter_url</th>\n",
       "      <th>flag_twitter_url</th>\n",
       "      <th>Text_core</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>1227888265887440897</td>\n",
       "      <td>Hasan08931491</td>\n",
       "      <td>2020-02-13 09:32:30+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>ZioNazi</td>\n",
       "      <td>#Apartheid zionazist... Ethnic cleansing, land...</td>\n",
       "      <td>2020</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>['https://t.co/vYnnnQIG8U']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>zionazist... Ethnic cleansing, land seizure, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3417</th>\n",
       "      <td>1393637799597801474</td>\n",
       "      <td>mir_638</td>\n",
       "      <td>2021-05-15 18:41:58+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Israel</td>\n",
       "      <td>RT @MohsinIslam: Israel is using international...</td>\n",
       "      <td>2021</td>\n",
       "      <td>37</td>\n",
       "      <td>34</td>\n",
       "      <td>['https://t.co/4lW3YWgGNz']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>RT : Israel is using internationally prohibite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>1104601295523000323</td>\n",
       "      <td>mps051960</td>\n",
       "      <td>2019-03-10 04:34:06+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Kikes</td>\n",
       "      <td>We'll soon hear Kikes in our time. She &amp;amp; t...</td>\n",
       "      <td>2019</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>['https://t.co/BdnFSjeHRM']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>We'll soon hear Kikes in our time. She &amp;amp; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>1395156785028222978</td>\n",
       "      <td>agarfilms</td>\n",
       "      <td>2021-05-19 23:17:52+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Israel</td>\n",
       "      <td>RT @rafaelshimunov: The Israeli Interior Minis...</td>\n",
       "      <td>2021</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "      <td>['https://t.co/unpqNNGJ2z']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>RT : The Israeli Interior Ministry maintains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>1228143125643714561</td>\n",
       "      <td>Suket_T3k1</td>\n",
       "      <td>2020-02-14 02:25:13+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>ZioNazi</td>\n",
       "      <td>#Apartheid zionazist...\\nEthnic cleansing, lan...</td>\n",
       "      <td>2020</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>['https://t.co/JXXuu7oas7']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>zionazist...\\nEthnic cleansing, land seizure, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>1395295293030207494</td>\n",
       "      <td>SallyfromSaar</td>\n",
       "      <td>2021-05-20 08:28:15+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Jews</td>\n",
       "      <td>Such growing noble sentiments by US Jews/Israe...</td>\n",
       "      <td>2021</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>['https://t.co/RpWJHaKb91']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Such growing noble sentiments by US Jews/Israe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>1102700703682715648</td>\n",
       "      <td>iclayca</td>\n",
       "      <td>2019-03-04 22:41:49+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Kikes</td>\n",
       "      <td>RT @USSLibincident: Ethno nationalism is only ...</td>\n",
       "      <td>2019</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>['https://t.co/bbwr7zFNlN']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>RT : Ethno nationalism is only considered ok w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>1227929698040074240</td>\n",
       "      <td>GunGunG49169853</td>\n",
       "      <td>2020-02-13 12:17:08+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>ZioNazi</td>\n",
       "      <td>#Apartheid zionazist... Ethnic cleansing, land...</td>\n",
       "      <td>2020</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>['https://t.co/vYnnnQIG8U']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>zionazist... Ethnic cleansing, land seizure, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>1392400276842323968</td>\n",
       "      <td>ZohaibKhalidPT</td>\n",
       "      <td>2021-05-12 08:44:30+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Israel</td>\n",
       "      <td>Retweet and show the world if you agree with m...</td>\n",
       "      <td>2021</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>['https://t.co/hhQ7feQduJ']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Retweet and show the world if you agree with m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>1152595302274105345</td>\n",
       "      <td>kiwianna111</td>\n",
       "      <td>2019-07-20 15:05:08+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>ZioNazi</td>\n",
       "      <td>@reallygraceful More BS from the delusional pa...</td>\n",
       "      <td>2019</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>['https://t.co/3EiR0vHOlP']</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>More BS from the delusional paranoid psychopat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID         Username                 CreateDate  Biased  \\\n",
       "3056  1227888265887440897    Hasan08931491  2020-02-13 09:32:30+00:00       1   \n",
       "3417  1393637799597801474          mir_638  2021-05-15 18:41:58+00:00       1   \n",
       "2167  1104601295523000323        mps051960  2019-03-10 04:34:06+00:00       1   \n",
       "3274  1395156785028222978        agarfilms  2021-05-19 23:17:52+00:00       1   \n",
       "2316  1228143125643714561       Suket_T3k1  2020-02-14 02:25:13+00:00       1   \n",
       "1876  1395295293030207494    SallyfromSaar  2021-05-20 08:28:15+00:00       1   \n",
       "3001  1102700703682715648          iclayca  2019-03-04 22:41:49+00:00       1   \n",
       "3058  1227929698040074240  GunGunG49169853  2020-02-13 12:17:08+00:00       1   \n",
       "3434  1392400276842323968   ZohaibKhalidPT  2021-05-12 08:44:30+00:00       1   \n",
       "2238  1152595302274105345      kiwianna111  2019-07-20 15:05:08+00:00       1   \n",
       "\n",
       "      Keyword                                               Text  Year  \\\n",
       "3056  ZioNazi  #Apartheid zionazist... Ethnic cleansing, land...  2020   \n",
       "3417   Israel  RT @MohsinIslam: Israel is using international...  2021   \n",
       "2167    Kikes  We'll soon hear Kikes in our time. She &amp; t...  2019   \n",
       "3274   Israel  RT @rafaelshimunov: The Israeli Interior Minis...  2021   \n",
       "2316  ZioNazi  #Apartheid zionazist...\\nEthnic cleansing, lan...  2020   \n",
       "1876     Jews  Such growing noble sentiments by US Jews/Israe...  2021   \n",
       "3001    Kikes  RT @USSLibincident: Ethno nationalism is only ...  2019   \n",
       "3058  ZioNazi  #Apartheid zionazist... Ethnic cleansing, land...  2020   \n",
       "3434   Israel  Retweet and show the world if you agree with m...  2021   \n",
       "2238  ZioNazi  @reallygraceful More BS from the delusional pa...  2019   \n",
       "\n",
       "      n_words  n_words_no_urls_mentions_hashtags               extracted_urls  \\\n",
       "3056       23                                 19  ['https://t.co/vYnnnQIG8U']   \n",
       "3417       37                                 34  ['https://t.co/4lW3YWgGNz']   \n",
       "2167       39                                 38  ['https://t.co/BdnFSjeHRM']   \n",
       "3274       42                                 41  ['https://t.co/unpqNNGJ2z']   \n",
       "2316       19                                 18  ['https://t.co/JXXuu7oas7']   \n",
       "1876       23                                 22  ['https://t.co/RpWJHaKb91']   \n",
       "3001       13                                 12  ['https://t.co/bbwr7zFNlN']   \n",
       "3058       23                                 19  ['https://t.co/vYnnnQIG8U']   \n",
       "3434       32                                 22  ['https://t.co/hhQ7feQduJ']   \n",
       "2238       25                                 23  ['https://t.co/3EiR0vHOlP']   \n",
       "\n",
       "      flag_link_only  flag_starts_with_mention  n_mentions_in_text  \\\n",
       "3056           False                     False                   0   \n",
       "3417           False                     False                   1   \n",
       "2167           False                     False                   0   \n",
       "3274           False                     False                   1   \n",
       "2316           False                     False                   0   \n",
       "1876           False                     False                   0   \n",
       "3001           False                     False                   1   \n",
       "3058           False                     False                   0   \n",
       "3434           False                     False                   0   \n",
       "2238           False                      True                   1   \n",
       "\n",
       "      n_hashtags_in_text  n_urls_in_text  no_text_ratio  \\\n",
       "3056                   3               1       0.173913   \n",
       "3417                   2               1       0.105263   \n",
       "2167                   0               1       0.025641   \n",
       "3274                   0               1       0.046512   \n",
       "2316                   1               1       0.100000   \n",
       "1876                   0               1       0.043478   \n",
       "3001                   0               1       0.142857   \n",
       "3058                   3               1       0.173913   \n",
       "3434                  10               1       0.333333   \n",
       "2238                   0               1       0.080000   \n",
       "\n",
       "      flag_empty_after_cleanup  flag_non_twitter_url  flag_twitter_url  \\\n",
       "3056                     False                 False              True   \n",
       "3417                     False                 False              True   \n",
       "2167                     False                 False              True   \n",
       "3274                     False                 False              True   \n",
       "2316                     False                 False              True   \n",
       "1876                     False                 False              True   \n",
       "3001                     False                 False              True   \n",
       "3058                     False                 False              True   \n",
       "3434                     False                 False              True   \n",
       "2238                     False                 False              True   \n",
       "\n",
       "                                              Text_core  \n",
       "3056  zionazist... Ethnic cleansing, land seizure, h...  \n",
       "3417  RT : Israel is using internationally prohibite...  \n",
       "2167  We'll soon hear Kikes in our time. She &amp; t...  \n",
       "3274  RT : The Israeli Interior Ministry maintains a...  \n",
       "2316  zionazist...\\nEthnic cleansing, land seizure, ...  \n",
       "1876  Such growing noble sentiments by US Jews/Israe...  \n",
       "3001  RT : Ethno nationalism is only considered ok w...  \n",
       "3058  zionazist... Ethnic cleansing, land seizure, h...  \n",
       "3434  Retweet and show the world if you agree with m...  \n",
       "2238  More BS from the delusional paranoid psychopat...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from constants import PROJECT_ROOT\n",
    "\n",
    "train_df = pd.read_csv(PROJECT_ROOT / \"datasets\" / \"train_test_datasets\" / \"train.csv\")\n",
    "samples = train_df[train_df[\"Biased\"]==1].sample(10, random_state=42)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31e75b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "3056",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "93738f63-772e-4cfb-9d59-15998d1b6585",
       "rows": [
        [
         "ID",
         "1227888265887440897"
        ],
        [
         "Username",
         "Hasan08931491"
        ],
        [
         "CreateDate",
         "2020-02-13 09:32:30+00:00"
        ],
        [
         "Biased",
         "1"
        ],
        [
         "Keyword",
         "ZioNazi"
        ],
        [
         "Text",
         "#Apartheid zionazist... Ethnic cleansing, land seizure, home demolition, military occupation, bombing of Gaza.. Israel is copying South Africa very well  #SobatPalestinaINA  #IsraeliCrimes https://t.co/vYnnnQIG8U"
        ],
        [
         "Year",
         "2020"
        ],
        [
         "n_words",
         "23"
        ],
        [
         "n_words_no_urls_mentions_hashtags",
         "19"
        ],
        [
         "extracted_urls",
         "['https://t.co/vYnnnQIG8U']"
        ],
        [
         "flag_link_only",
         "False"
        ],
        [
         "flag_starts_with_mention",
         "False"
        ],
        [
         "n_mentions_in_text",
         "0"
        ],
        [
         "n_hashtags_in_text",
         "3"
        ],
        [
         "n_urls_in_text",
         "1"
        ],
        [
         "no_text_ratio",
         "0.1739130434782608"
        ],
        [
         "flag_empty_after_cleanup",
         "False"
        ],
        [
         "flag_non_twitter_url",
         "False"
        ],
        [
         "flag_twitter_url",
         "True"
        ],
        [
         "Text_core",
         "zionazist... Ethnic cleansing, land seizure, home demolition, military occupation, bombing of Gaza.. Israel is copying South Africa very well"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 20
       }
      },
      "text/plain": [
       "ID                                                                 1227888265887440897\n",
       "Username                                                                 Hasan08931491\n",
       "CreateDate                                                   2020-02-13 09:32:30+00:00\n",
       "Biased                                                                               1\n",
       "Keyword                                                                        ZioNazi\n",
       "Text                                 #Apartheid zionazist... Ethnic cleansing, land...\n",
       "Year                                                                              2020\n",
       "n_words                                                                             23\n",
       "n_words_no_urls_mentions_hashtags                                                   19\n",
       "extracted_urls                                             ['https://t.co/vYnnnQIG8U']\n",
       "flag_link_only                                                                   False\n",
       "flag_starts_with_mention                                                         False\n",
       "n_mentions_in_text                                                                   0\n",
       "n_hashtags_in_text                                                                   3\n",
       "n_urls_in_text                                                                       1\n",
       "no_text_ratio                                                                 0.173913\n",
       "flag_empty_after_cleanup                                                         False\n",
       "flag_non_twitter_url                                                             False\n",
       "flag_twitter_url                                                                  True\n",
       "Text_core                            zionazist... Ethnic cleansing, land seizure, h...\n",
       "Name: 3056, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sample = samples.iloc[0]\n",
    "one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0af6f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import llm, load_definitions\n",
    "from constants import ANNOTATION_GLOB, CLASSIFIER_SYSTEM_ONE_DEF\n",
    "all_annotations = load_definitions(str(ANNOTATION_GLOB))\n",
    "one_def = \"IHRA Definition\"\n",
    "system_prompt = CLASSIFIER_SYSTEM_ONE_DEF.replace(\n",
    "        \"<DEFINITION_NAME_PLACEHOLDER>\", one_def\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "649e3424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a careful text classifier.\n",
      "You will be given:\n",
      "- The definition of antisemitism by IHRA Definition\n",
      "- a \"text\" string to classify\n",
      "Task:\n",
      "- You should classify if the text is antisemitic according to the given definition.\n",
      "- The optional labels are: True (antisemitic by definition), False (not antisemitic by definition).\n",
      "- Pick exactly one label.\n",
      "- Output JSON only, no extra words.\n",
      "\n",
      "Output schema:\n",
      "{\n",
      "  \"answer\": \"<one key from annotations>\",\n",
      "  \"description\": \"<1 to 3 sentences explaining the choice>\"\n",
      "}\n",
      "\n",
      "Rules:\n",
      "- Forget all other definitions of antisemitism, except the one by IHRA Definition.\n",
      "- answer MUST be exactly True or False\n",
      "- description must cite concrete cues from the text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "512a2f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<annotation_guidelines>\n",
      "\n",
      "<name>\n",
      "IHRA (International Holocaust Remembrance Alliance) working definition of antisemitism\n",
      "</name>\n",
      "\n",
      "<definition>\n",
      "Antisemitism is a certain perception of Jews, which may be expressed as hatred toward Jews. Rhetorical and physical manifestations of antisemitism are directed toward Jewish or non-Jewish individuals and/or their property, toward Jewish community institutions and religious facilities.\n",
      "</defintion>\n",
      "\n",
      "<guidelines>\n",
      "To guide IHRA in its work, the following examples may serve as illustrations:\n",
      "\n",
      "1. Manifestations might include the targeting of the state of Israel, conceived as a Jewish collectivity. However, criticism of Israel similar to that leveled against any other country cannot be regarded as antisemitic. Antisemitism frequently charges Jews with conspiring to harm humanity, and it is often used to blame Jews for “why things go wrong.” It is expressed in speech, writing, visual forms and action, and employs sinister stereotypes and negative character traits.\n",
      "\n",
      "2.0 Manifestations might include the targeting of the state of Israel, conceived as a Jewish collectivity. However,\n",
      "criticism of Israel similar to that leveled against any other country cannot be regarded as antisemitic. Antisemitism\n",
      "frequently charges Jews with conspiring to harm humanity, and it is often used to blame Jews for “why things go\n",
      "wrong.” It is expressed in speech, writing, visual forms and action, and employs sinister stereotypes and negative\n",
      "character traits.\n",
      "2.1 Contemporary examples of antisemitism in public life, the media, schools, the workplace, and in the religious sphere\n",
      "could, taking into account the overall context, include, but are not limited to:\n",
      "2.1.1 Calling for, aiding, or justifying the killing or harming of Jews in the name of a radical ideology or an extremist view\n",
      "of religion.\n",
      "2.1.2 Making mendacious, dehumanizing, demonizing, or stereotypical allegations about Jews as such or the power of\n",
      "Jews as collective — such as, especially but not exclusively, the myth about a world Jewish conspiracy or of Jews\n",
      "controlling the media, economy, government or other societal institutions.\n",
      "2.1.3 Accusing Jews as a people of being responsible for real or imagined wrongdoing committed by a single Jewish\n",
      "person or group, or even for acts committed by non-Jews.\n",
      "2.1.4 Denying the fact, scope, mechanisms (e.g. gas chambers) or intentionality of the genocide of the Jewish people at\n",
      "the hands of National Socialist Germany and its supporters and accomplices during World War II (the Holocaust).\n",
      "2.1.5 Accusing the Jews as a people, or Israel as a state, of inventing or exaggerating the Holocaust.\n",
      "2.1.6 Accusing Jewish citizens of being more loyal to Israel, or to the alleged priorities of Jews worldwide, than to the\n",
      "interests of their own nations.\n",
      "2.1.7 Denying the Jewish people their right to self-determination, e.g., by claiming that the existence of a State of Israel\n",
      "is a racist endeavor.\n",
      "2.1.8 Applying double standards by requiring of it a behavior not expected or demanded of any other democratic nation.\n",
      "2.1.9 Using the symbols and images associated with classic antisemitism (e.g., claims of Jews killing Jesus or blood libel)\n",
      "to characterize Israel or Israelis.\n",
      "2.1.10 Drawing comparisons of contemporary Israeli policy to that of the Nazis.\n",
      "2.1.11 Holding Jews collectively responsible for actions of the state of Israel.\n",
      "3.0 Antisemitic acts are criminal when they are so defined by law (for example, denial of the Holocaust or distribution of antisemitic materials in some countries).\n",
      "\n",
      "4.0 Criminal acts are antisemitic when the targets of attacks, whether they are\n",
      "people or property – such as buildings, schools, places of worship and cemeteries – are selected because they are, or are\n",
      "perceived to be, Jewish or linked to Jews.] \n",
      "5.0 Antisemitic discrimination is the denial to Jews of opportunities or\n",
      "services available to others and is illegal in many countries.\n",
      "\n",
      "</guidelines>\n",
      "\n",
      "\n",
      "<clarification>\n",
      "\n",
      "The definition notes that non-Jewish individuals can also become victims of antisemitism. We infer from\n",
      "guideline 4.0 that this is the case if they are perceived to be Jewish or linked to Jews. Additionally, we infer from guideline\n",
      "2.1.2 that rhetoric can be antisemitic even if no specific Jewish individual or communal institution is targeted, but rather,\n",
      "the target is an abstract Jewish collective.\n",
      "guideline 2.1 lists 11 examples of contemporary forms of antisemitism. The IHRA has made it clear in additional statements that the examples are part of the definition.\n",
      "The example of Holocaust denial (2.14) includes denying the scope and the intentionality of the genocide of the Jewish people. Denying the scope of the Holocaust means denying that close to six million Jews were murdered for being Jews.\n",
      "The IHRA also adopted a “Working Definition of Holocaust Denial and Distortion.” It is in accordance with its definition of antisemitism and further exemplifies that Holocaust denial “may include publicly denying or calling into doubt the use of principal mechanisms of destruction (such as gas chambers, mass shooting, starvation and torture)” and also “blaming\n",
      "the Jews for either exaggerating or creating the Shoah for political or financial gain as if the Shoah itself was the result of a conspiracy plotted by the Jews.”\n",
      "2.1.7 mentions denying the Jewish people their right to self-determination. Taking into account the next example, 2.1.8,\n",
      "“Applying double standards by requiring of it a behavior not expected or demanded of any other democratic nation” we\n",
      "include the denial of Israel’s right to exist in its geographical region. However, the second part of 3.1.7, “claiming that\n",
      "the existence of a State of Israel is a racist endeavor” does not mean that all accusations of racism against Israel are\n",
      "antisemitic. It means that claiming that a State of Israel as per se racist (or an Apartheid state) is an example of denying\n",
      "the Jewish people their right to self-determination and is therefore antisemitic.\n",
      "The Working Definition mentions “mendacious, dehumanizing, demonizing, or stereotypical allegations about Jews as\n",
      "such” and “classic stereotypes” without listing them explicitly. Below you find a composite of allegations and\n",
      "stereotypes that have become part of that repertoire. We compiled them by looking at descriptions that other scholars\n",
      "have identified as prominent antisemitic stereotypes in the past 2000 years.\n",
      "Antisemitic allegations and stereotypes can be made by characterizing “the Jews” or by ascribing certain physical traits\n",
      "to them. Accusations of wrongdoing on the part of Jews also form part of the rich history of antisemitic stereotypes, as\n",
      "well as certain tropes. They can also be shown in the demonization of things and individuals that are thought of as being\n",
      "representative of Jews or Jewish beliefs. Certain beliefs, usually religious in nature, advocate for the punishment of Jews\n",
      "and also belong to the antisemitic tradition. Endorsing Nazism, Holocaust denial, or Israel-related forms of antisemitism\n",
      "are newer phenomena that are addressed explicitly and with examples in the working definition.\n",
      "\n",
      "\n",
      "Supposed “ Jewish character ” is portrayed as stingy; greedy; immensely rich; being good with money; exploitative;\n",
      "corrupt; amoral; perverted; ruthless; cruel; heartless; anti-national/cosmopolitan; treacherous; disloyal; fraudulent;\n",
      "dishonest; untrustworthy; hypocrites; materialist; swank; work-shy; uncreative; intelligent; possess superhuman powers;\n",
      "arrogant; stubborn; culturally backwards; superstitious; ridiculous; dishonorable; hyper-sexual; ritually unclean; tribal;\n",
      "clannish; secretive; racist; men: effeminate and also lecherous; women: femme-fatal.\n",
      "Supposed “ Jewish physical stereotypes ” are hooked noses; pointed beards; big ears; a weak or hunched frame; a dark\n",
      "complexion; hooves; horns; a tail and a goatee; unruly red or black hair; goggled eyes; blinded eyes; tired eyes; large\n",
      "lips; and an odor.\n",
      "Antisemitic imagery can be found in depictions of Jews as the \"wandering Jew”; demonic figures; lavishly rich capitalists;\n",
      "money/gold hoarding; hooked-nosed communists; heartless merchants; parasites and vile creatures such as beasts;\n",
      "octopi; snakes; rats; germs; and blood sucking entities.\n",
      "Supposed “ Jewish crimes ” include the charge of deicide/ killing Jesus; being in league with the devil; seeking to destroy\n",
      "non-Jewish civilizations; working with alleged conspiratorial groups thriving for world power, such as Rothschilds,\n",
      "Freemasons, Illuminati, Jewish lobby, Zionist Lobby, Zionist Neocons, ZOG (Zionist Occupied Government); waging a\n",
      "(proxy) war against Islam/ Christianity; luring Christians/ Muslims away from Christianity/ Islam; profanation of Christian\n",
      "symbols; host desecration; practicing witchcraft; usury; profiteering; exploiting non-Jews; running transnational,\n",
      "allegedly “Jewish companies” in the interest of the Jews such as McDonalds, Starbucks, Coca Cola, Facebook; using\n",
      "blood from non-Jews for ritual purposes; killing or mutilating children for ritual purposes; adoring false gods and idols,\n",
      "such as the Golden Calf and Moloch; rejecting truth and being blind to the truth; perverting scripture; sticking to the\n",
      "letters but not the spirit of religious texts; falsifying scripture; having tried to murder the prophet Mohammed; well\n",
      "poisoning; causing epidemics, such as Black Death and AIDS; being responsible for the slave trade; poisoning non-Jews;\n",
      "aspiring to control the world secretly; secretly controlling world finance, country governments, media, Hollywood;\n",
      "orchestrating wars, revolutions, disasters (such as 9/11 and the subsequent wars in the Middle East); undermining\n",
      "culture and morals, especially concerning sexuality; degrading culture, music, science; degenerating race purity;\n",
      "undermining and betraying their countries of residence; inventing the Holocaust or exaggerating the Holocaust for\n",
      "material gain; being responsible for Christianity and the power of the church, for oligarchies, financial speculation,\n",
      "exploitation, capitalism, modernity, communism, bolshevism, liberalism, democracy, urbanization, Americanization, and\n",
      "globalization.\n",
      "Demonization of things associated with Jews or of individuals seen as representative of Jews include the demonization\n",
      "of synagogues, Judaism, the Talmud, Kabbalah; the Judaization of enemies (using “Jew” as an insult); and the\n",
      "demonization of prominent Jews, such George Soros, Ariel Sharon, Benjamin Netanyahu, and Abraham Foxman as Jews.\n",
      "Nonvisual memes or recurrent phraseology that are part of an antisemitic repertoire in different historical and cultural\n",
      "contexts include “Jews are the children/ spawn of Satan; synagogue of Satan; God has (eternally) cursed the Jews;\n",
      "Judaism (Jewish alleged choseness) is racist; Jews don’t have a home country and cannot be a nation; Jews have no\n",
      "culture; Crypto Jews (converted Jews or their offspring remain Jewish and secretly act in the ‘Jewish interest’); ‘Jewish\n",
      "spirit’ in science, music, culture is harmful to non-Jews; use of the terms ‘Jewish terror’ or ‘Zydokumuna’ for purges\n",
      "under communism; Jewish soldiers in WW1/ WW2 were traitors; all pro-Jewish or pro-Israeli organizations are funded/\n",
      "operated by the Mossad; Jews are descendants of monkeys and pigs; Jews should never be taken as friends; Jews are\n",
      "the eternal enemies of Islam and Muslims. Muslims will kill the Jews at the end of time; reference to the battle of\n",
      "Khaybar; synagogues should be set on fire; Jews killed or sold out their own prophets/the son of God; Jews try to evade\n",
      "taxes/ Jews don’t pay taxes; Hitler let some Jews live so that the world would know why he exterminated Jews;\n",
      "‘International Zionism’ prevents a critical discussion about the Holocaust.”\n",
      "Calls for punishment or justification of Jewish suffering have also been part of an antisemitic discourse, mostly in\n",
      "religious contexts, such as “Jewish suffering is punishment by God; Humiliation and misery of Jews is proof of the truth\n",
      "of Christianity/Islam. Misery of Jews is proof of truth of Christianity.; Jews should be burnt as a form of punishment;\n",
      "Persecution of Jews under Hitler was punishment by God.\"\n",
      "Holocaust denial is described in the IHRA Definition of Antisemitism. The more detailed IHRA Working Definition of\n",
      "Holocaust Denial and Distortion is used as additional guidance.\n",
      "Endorsing Nazism today means endorsing the systematic killings of Jews by the Nazis (and their helpers). It i s often done\n",
      "by the affirmative use of pro-Nazi memes and symbols.\n",
      "Manifestations of antisemitism related to Israel are described in the Working Definition, including examples.\n",
      "Additional, frequently used antisemitic concepts include \"Jews crucify or ritually kill Palestinians\" and the use of terms\n",
      "such as \"Zionist Entity\" to describe the State of Israel, which is a refusal to acknowledge the existence of Israel and thus\n",
      "a form of denying the Jewish people their right to self-determination. The following claims are “comparisons of\n",
      "contemporary Israeli policy to that of the Nazis” (example 3.1.10 in the working definition): equating Israeli politicians\n",
      "with Nazi leaders, such as Netanyahu = Hitler; claims that Israel engages in genocide/ “a Holocaust” against the\n",
      "Palestinian people; claims that the situation in the Gaza Strip is similar to the situation in the Warsaw Ghetto; using Nazi\n",
      "vocabulary to describe and denounce actions by the Israeli state, such as claims that Israel wages a war of extermination\n",
      "against the Palestinians.\n",
      "Most symbols that have an antisemitic connotation are positive references to Nazism or to the Holocaust, such as the\n",
      "swastika or other Nazi or Nazi-predecessor flags, the Hitler salute, emblems of Nazi organizations such as the SS, Nazi\n",
      "slogans such as “Blut und Ehre” (blood and honor), or numbers representing “Heil Hitler” (88), “Adolf Hitler” (18).\n",
      "Positive references to the Holocaust used to manifest endorsement for the killing of J ews include symbols representing\n",
      "Zyklon B gas, including hissing noises, or references to ovens that were used to burn the people who were gassed\n",
      "(including pictures taken from these ovens).\n",
      "The ADL Hate Symbols Database provides an extensive list of symbols used by hate groups, mostly white supremacists\n",
      "and Neo-Nazis.^4 This list helps us to contextualize tweets. We consider all symbols with positive references to Nazism\n",
      "antisemitic because they implicitly endorse the murder of Jews. Symbols by extremist Christian and Muslim groups are\n",
      "not as extensively documented (Ostovar 2017).^5 Some Jihadist groups, such as Hamas or Houthi,^6 are known for their\n",
      "antisemitism and lethally targeting victims as Jews. Endorsing the aforementioned groups is treated as a context\n",
      "suggesting antisemitism, but is annotated as antisemitic only when there are other references in the tweets to Jews,\n",
      "Judaism, or Israel that make an antisemitic reading of it likely.\n",
      "\n",
      "\n",
      "</clarification>\n",
      "\n",
      "</annotation_guidelines>\n",
      "\n",
      "\n",
      "text:\n",
      "#Apartheid zionazist... Ethnic cleansing, land seizure, home demolition, military occupation, bombing of Gaza.. Israel is copying South Africa very well  #SobatPalestinaINA  #IsraeliCrimes https://t.co/vYnnnQIG8U\n",
      "\n",
      "Respond with JSON only matching the schema.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = (\n",
    "        all_annotations[one_def]+\n",
    "        \"\\n\"\n",
    "        + \"\\n\\ntext:\\n\"\n",
    "        + one_sample[\"Text\"]\n",
    "        + \"\\n\\nRespond with JSON only matching the schema.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "404a95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import MAX_TOKENS\n",
    "# MODEL = \"openai/gpt-4o-mini\" #Working\n",
    "MODEL = \"x-ai/grok-4\" #Working\n",
    "\n",
    "\n",
    "resp = llm(\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt[:300] +\"\\nText: \" + one_sample[\"Text\"]},\n",
    "                ],\n",
    "                MODEL,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0,\n",
    "                max_tokens=MAX_TOKENS,\n",
    "                logprobs= True,\n",
    "                return_json=True,\n",
    "                \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bd28897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'gen-1757502435-SWoiEugq848b9KjS0OIC',\n",
       " 'provider': 'xAI',\n",
       " 'model': 'x-ai/grok-4',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1757502435,\n",
       " 'choices': [{'logprobs': {'content': [{'token': ' First',\n",
       "      'logprob': -0.055419736,\n",
       "      'bytes': [32, 70, 105, 114, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -0.00016258826,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' task',\n",
       "      'logprob': -0.22431341,\n",
       "      'bytes': [32, 116, 97, 115, 107],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': -3.0994368e-06,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' classify',\n",
       "      'logprob': -5.960463e-07,\n",
       "      'bytes': [32, 99, 108, 97, 115, 115, 105, 102, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' if',\n",
       "      'logprob': -2.0265559e-06,\n",
       "      'bytes': [32, 105, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' text',\n",
       "      'logprob': -0.0005082984,\n",
       "      'bytes': [32, 116, 101, 120, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' antis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 116, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'emitic',\n",
       "      'logprob': 0,\n",
       "      'bytes': [101, 109, 105, 116, 105, 99],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' according',\n",
       "      'logprob': -7.152555e-07,\n",
       "      'bytes': [32, 97, 99, 99, 111, 114, 100, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -3.576278e-07,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' IH',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 73, 72],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'RA',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [82, 65],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Definition',\n",
       "      'logprob': -0.005246917,\n",
       "      'bytes': [32, 68, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': -0.038399514,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'The',\n",
       "      'logprob': -8.5707805e-05,\n",
       "      'bytes': [84, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' IH',\n",
       "      'logprob': -0.01571861,\n",
       "      'bytes': [32, 73, 72],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'RA', 'logprob': 0, 'bytes': [82, 65], 'top_logprobs': []},\n",
       "     {'token': ' Definition',\n",
       "      'logprob': -0.061971538,\n",
       "      'bytes': [32, 68, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' provided',\n",
       "      'logprob': -0.00044121544,\n",
       "      'bytes': [32, 112, 114, 111, 118, 105, 100, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' in',\n",
       "      'logprob': -0.38745484,\n",
       "      'bytes': [32, 105, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': \" user's\",\n",
       "      'logprob': -0.7371226,\n",
       "      'bytes': [32, 117, 115, 101, 114, 39, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' message',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 109, 101, 115, 115, 97, 103, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': -1.883489e-05,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' incomplete',\n",
       "      'logprob': -0.015174777,\n",
       "      'bytes': [32, 105, 110, 99, 111, 109, 112, 108, 101, 116, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':',\n",
       "      'logprob': -0.0036171742,\n",
       "      'bytes': [58],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' \"', 'logprob': 0, 'bytes': [32, 34], 'top_logprobs': []},\n",
       "     {'token': 'Ant',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [65, 110, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'is', 'logprob': 0, 'bytes': [105, 115], 'top_logprobs': []},\n",
       "     {'token': 'emitism',\n",
       "      'logprob': 0,\n",
       "      'bytes': [101, 109, 105, 116, 105, 115, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a', 'logprob': 0, 'bytes': [32, 97], 'top_logprobs': []},\n",
       "     {'token': ' certain',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 101, 114, 116, 97, 105, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' perception',\n",
       "      'logprob': -1.0847986e-05,\n",
       "      'bytes': [32, 112, 101, 114, 99, 101, 112, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Jews',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 74, 101, 119, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' which',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 119, 104, 105, 99, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' may',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 109, 97, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' be',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 98, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' expressed',\n",
       "      'logprob': -3.576278e-07,\n",
       "      'bytes': [32, 101, 120, 112, 114, 101, 115, 115, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' as',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' hatred',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 104, 97, 116, 114, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' toward',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111, 119, 97, 114, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Jews',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 74, 101, 119, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.', 'logprob': 0, 'bytes': [46], 'top_logprobs': []},\n",
       "     {'token': ' Rhet',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 82, 104, 101, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'orical',\n",
       "      'logprob': -7.152555e-07,\n",
       "      'bytes': [111, 114, 105, 99, 97, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' physical',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [32, 112, 104, 121, 115, 105, 99, 97, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' manifestations',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32,\n",
       "       109,\n",
       "       97,\n",
       "       110,\n",
       "       105,\n",
       "       102,\n",
       "       101,\n",
       "       115,\n",
       "       116,\n",
       "       97,\n",
       "       116,\n",
       "       105,\n",
       "       111,\n",
       "       110,\n",
       "       115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' antis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 116, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'emitism',\n",
       "      'logprob': -4.7683704e-07,\n",
       "      'bytes': [101, 109, 105, 116, 105, 115, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' are',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 114, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' directed',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 100, 105, 114, 101, 99, 116, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Text',\n",
       "      'logprob': -0.11621439,\n",
       "      'bytes': [32, 84, 101, 120, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':',\n",
       "      'logprob': -0.061968397,\n",
       "      'bytes': [58],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' #',\n",
       "      'logprob': -0.20874578,\n",
       "      'bytes': [32, 35],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Apart',\n",
       "      'logprob': -3.576278e-07,\n",
       "      'bytes': [65, 112, 97, 114, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'heid',\n",
       "      'logprob': -4.7683704e-07,\n",
       "      'bytes': [104, 101, 105, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' z',\n",
       "      'logprob': -9.7985234e-05,\n",
       "      'bytes': [32, 122],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'ion',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'az', 'logprob': 0, 'bytes': [97, 122], 'top_logprobs': []},\n",
       "     {'token': 'ist',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '...',\n",
       "      'logprob': -0.005940164,\n",
       "      'bytes': [46, 46, 46],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Ethnic',\n",
       "      'logprob': -1.0967195e-05,\n",
       "      'bytes': [32, 69, 116, 104, 110, 105, 99],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' cleansing',\n",
       "      'logprob': -1.3113013e-06,\n",
       "      'bytes': [32, 99, 108, 101, 97, 110, 115, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' land',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [32, 108, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' seizure',\n",
       "      'logprob': -5.960463e-07,\n",
       "      'bytes': [32, 115, 101, 105, 122, 117, 114, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' home',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 104, 111, 109, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' demolition',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 100, 101, 109, 111, 108, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' military',\n",
       "      'logprob': -1.1920922e-06,\n",
       "      'bytes': [32, 109, 105, 108, 105, 116, 97, 114, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' occupation',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 99, 99, 117, 112, 97, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' bombing',\n",
       "      'logprob': -5.960463e-07,\n",
       "      'bytes': [32, 98, 111, 109, 98, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Gaza',\n",
       "      'logprob': -1.072883e-06,\n",
       "      'bytes': [32, 71, 97, 122, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '..', 'logprob': 0, 'bytes': [46, 46], 'top_logprobs': []},\n",
       "     {'token': ' Israel',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 73, 115, 114, 97, 101, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' copying',\n",
       "      'logprob': -1.6689287e-06,\n",
       "      'bytes': [32, 99, 111, 112, 121, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' South',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 83, 111, 117, 116, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Africa',\n",
       "      'logprob': -4.7683704e-07,\n",
       "      'bytes': [32, 65, 102, 114, 105, 99, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' very',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 118, 101, 114, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' well',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 119, 101, 108, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' ', 'logprob': -0.20141333, 'bytes': [32], 'top_logprobs': []},\n",
       "     {'token': ' #', 'logprob': 0, 'bytes': [32, 35], 'top_logprobs': []},\n",
       "     {'token': 'Sob',\n",
       "      'logprob': -4.7683704e-07,\n",
       "      'bytes': [83, 111, 98],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'at', 'logprob': 0, 'bytes': [97, 116], 'top_logprobs': []},\n",
       "     {'token': 'Pal',\n",
       "      'logprob': 0,\n",
       "      'bytes': [80, 97, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'est',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [101, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'ina',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [105, 110, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'INA', 'logprob': 0, 'bytes': [73, 78, 65], 'top_logprobs': []},\n",
       "     {'token': ' ',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [32],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' #', 'logprob': 0, 'bytes': [32, 35], 'top_logprobs': []},\n",
       "     {'token': 'Israeli',\n",
       "      'logprob': 0,\n",
       "      'bytes': [73, 115, 114, 97, 101, 108, 105],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Cr', 'logprob': 0, 'bytes': [67, 114], 'top_logprobs': []},\n",
       "     {'token': 'imes',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 109, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' https',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 104, 116, 116, 112, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '://', 'logprob': 0, 'bytes': [58, 47, 47], 'top_logprobs': []},\n",
       "     {'token': 't', 'logprob': 0, 'bytes': [116], 'top_logprobs': []},\n",
       "     {'token': '.co',\n",
       "      'logprob': 0,\n",
       "      'bytes': [46, 99, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '/v', 'logprob': 0, 'bytes': [47, 118], 'top_logprobs': []},\n",
       "     {'token': 'Y', 'logprob': 0, 'bytes': [89], 'top_logprobs': []},\n",
       "     {'token': 'nn',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [110, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'n',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Q', 'logprob': 0, 'bytes': [81], 'top_logprobs': []},\n",
       "     {'token': 'IG', 'logprob': 0, 'bytes': [73, 71], 'top_logprobs': []},\n",
       "     {'token': '8', 'logprob': 0, 'bytes': [56], 'top_logprobs': []},\n",
       "     {'token': 'U',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [85],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\"\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [34, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'It',\n",
       "      'logprob': -0.005679423,\n",
       "      'bytes': [73, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' seems',\n",
       "      'logprob': -0.47407714,\n",
       "      'bytes': [32, 115, 101, 101, 109, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' like',\n",
       "      'logprob': -4.6252135e-05,\n",
       "      'bytes': [32, 108, 105, 107, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -1.3232144e-05,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' definition',\n",
       "      'logprob': -0.00056477333,\n",
       "      'bytes': [32, 100, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': -0.011052577,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' cut',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 117, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' off',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -0.000823278,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'logprob': -0.000123374,\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -0.25192922,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' text',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 101, 120, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': -0.0002619877,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' classify',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 108, 97, 115, 115, 105, 102, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': -0.008641822,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' appended',\n",
       "      'logprob': -0.47478893,\n",
       "      'bytes': [32, 97, 112, 112, 101, 110, 100, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.', 'logprob': -0.2519812, 'bytes': [46], 'top_logprobs': []},\n",
       "     {'token': ' But',\n",
       "      'logprob': -8.70224e-06,\n",
       "      'bytes': [32, 66, 117, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' from',\n",
       "      'logprob': -0.0017037175,\n",
       "      'bytes': [32, 102, 114, 111, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' my',\n",
       "      'logprob': -9.5367386e-07,\n",
       "      'bytes': [32, 109, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' knowledge',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 107, 110, 111, 119, 108, 101, 100, 103, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -3.81469e-06,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' full',\n",
       "      'logprob': -0.10020885,\n",
       "      'bytes': [32, 102, 117, 108, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' IH', 'logprob': 0, 'bytes': [32, 73, 72], 'top_logprobs': []},\n",
       "     {'token': 'RA', 'logprob': 0, 'bytes': [82, 65], 'top_logprobs': []},\n",
       "     {'token': ' definition',\n",
       "      'logprob': -0.00041130665,\n",
       "      'bytes': [32, 100, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': -0.0086145215,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':',\n",
       "      'logprob': -0.002475652,\n",
       "      'bytes': [58],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' \"',\n",
       "      'logprob': -0.00033539868,\n",
       "      'bytes': [32, 34],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Ant',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [65, 110, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'is', 'logprob': 0, 'bytes': [105, 115], 'top_logprobs': []},\n",
       "     {'token': 'emitism',\n",
       "      'logprob': 0,\n",
       "      'bytes': [101, 109, 105, 116, 105, 115, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a', 'logprob': 0, 'bytes': [32, 97], 'top_logprobs': []},\n",
       "     {'token': ' certain',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 101, 114, 116, 97, 105, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' perception',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 112, 101, 114, 99, 101, 112, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Jews',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 74, 101, 119, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' which',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 119, 104, 105, 99, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' may',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 109, 97, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' be',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 98, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' expressed',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 101, 120, 112, 114, 101, 115, 115, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' as',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' hatred',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 104, 97, 116, 114, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' toward',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111, 119, 97, 114, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Jews',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 74, 101, 119, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.', 'logprob': 0, 'bytes': [46], 'top_logprobs': []},\n",
       "     {'token': ' Rhet',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 82, 104, 101, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'orical',\n",
       "      'logprob': -8.344647e-07,\n",
       "      'bytes': [111, 114, 105, 99, 97, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' physical',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 112, 104, 121, 115, 105, 99, 97, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' manifestations',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32,\n",
       "       109,\n",
       "       97,\n",
       "       110,\n",
       "       105,\n",
       "       102,\n",
       "       101,\n",
       "       115,\n",
       "       116,\n",
       "       97,\n",
       "       116,\n",
       "       105,\n",
       "       111,\n",
       "       110,\n",
       "       115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' antis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 116, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'emitism',\n",
       "      'logprob': -3.576278e-07,\n",
       "      'bytes': [101, 109, 105, 116, 105, 115, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' are',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 114, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' directed',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 100, 105, 114, 101, 99, 116, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' toward',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111, 119, 97, 114, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Jewish',\n",
       "      'logprob': -4.7683704e-07,\n",
       "      'bytes': [32, 74, 101, 119, 105, 115, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' or',\n",
       "      'logprob': -2.8610189e-06,\n",
       "      'bytes': [32, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' non',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 110, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '-Jewish',\n",
       "      'logprob': 0,\n",
       "      'bytes': [45, 74, 101, 119, 105, 115, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' individuals',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 105, 110, 100, 105, 118, 105, 100, 117, 97, 108, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'logprob': -1.5497195e-06,\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '/or',\n",
       "      'logprob': 0,\n",
       "      'bytes': [47, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' their',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101, 105, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' property',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 112, 114, 111, 112, 101, 114, 116, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' toward',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111, 119, 97, 114, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Jewish',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 74, 101, 119, 105, 115, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' community',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 111, 109, 109, 117, 110, 105, 116, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' institutions',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32,\n",
       "       105,\n",
       "       110,\n",
       "       115,\n",
       "       116,\n",
       "       105,\n",
       "       116,\n",
       "       117,\n",
       "       116,\n",
       "       105,\n",
       "       111,\n",
       "       110,\n",
       "       115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' religious',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [32, 114, 101, 108, 105, 103, 105, 111, 117, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' facilities',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 102, 97, 99, 105, 108, 105, 116, 105, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\"\\n\\n',\n",
       "      'logprob': -1.6689287e-06,\n",
       "      'bytes': [46, 34, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'It',\n",
       "      'logprob': -3.576278e-07,\n",
       "      'bytes': [73, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' includes',\n",
       "      'logprob': -0.5759394,\n",
       "      'bytes': [32, 105, 110, 99, 108, 117, 100, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' examples',\n",
       "      'logprob': -1.5973917e-05,\n",
       "      'bytes': [32, 101, 120, 97, 109, 112, 108, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' like',\n",
       "      'logprob': -0.57593983,\n",
       "      'bytes': [32, 108, 105, 107, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' accusing',\n",
       "      'logprob': -0.67693764,\n",
       "      'bytes': [32, 97, 99, 99, 117, 115, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Jews',\n",
       "      'logprob': -0.00022504182,\n",
       "      'bytes': [32, 74, 101, 119, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' as',\n",
       "      'logprob': -0.0007358466,\n",
       "      'bytes': [32, 97, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a', 'logprob': 0, 'bytes': [32, 97], 'top_logprobs': []},\n",
       "     {'token': ' people',\n",
       "      'logprob': -9.5367386e-07,\n",
       "      'bytes': [32, 112, 101, 111, 112, 108, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': -5.8410846e-05,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' being',\n",
       "      'logprob': -0.0005062729,\n",
       "      'bytes': [32, 98, 101, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' responsible',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 114, 101, 115, 112, 111, 110, 115, 105, 98, 108, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' for',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 102, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' real',\n",
       "      'logprob': -1.3232144e-05,\n",
       "      'bytes': [32, 114, 101, 97, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' or',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' imagined',\n",
       "      'logprob': -9.5367386e-07,\n",
       "      'bytes': [32, 105, 109, 97, 103, 105, 110, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' wrongdoing',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 119, 114, 111, 110, 103, 100, 111, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': -0.3132628, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' denying',\n",
       "      'logprob': -0.039686058,\n",
       "      'bytes': [32, 100, 101, 110, 121, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -0.00029607204,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Holocaust',\n",
       "      'logprob': -0.0013287772,\n",
       "      'bytes': [32, 72, 111, 108, 111, 99, 97, 117, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' etc',\n",
       "      'logprob': -0.47411817,\n",
       "      'bytes': [32, 101, 116, 99],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.',\n",
       "      'logprob': -0.0006269635,\n",
       "      'bytes': [46],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Importantly',\n",
       "      'logprob': -2.7418098e-06,\n",
       "      'bytes': [32, 73, 109, 112, 111, 114, 116, 97, 110, 116, 108, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -0.00029595286,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' it',\n",
       "      'logprob': -0.024163276,\n",
       "      'bytes': [32, 105, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' states',\n",
       "      'logprob': -0.33865714,\n",
       "      'bytes': [32, 115, 116, 97, 116, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' that',\n",
       "      'logprob': -0.0010327726,\n",
       "      'bytes': [32, 116, 104, 97, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' criticism',\n",
       "      'logprob': -0.014391202,\n",
       "      'bytes': [32, 99, 114, 105, 116, 105, 99, 105, 115, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Israel',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 73, 115, 114, 97, 101, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' similar',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 115, 105, 109, 105, 108, 97, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' that',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 97, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' leveled',\n",
       "      'logprob': -5.960463e-07,\n",
       "      'bytes': [32, 108, 101, 118, 101, 108, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' against',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 103, 97, 105, 110, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' any',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' other',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 116, 104, 101, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' country',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 111, 117, 110, 116, 114, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' cannot',\n",
       "      'logprob': -2.0265559e-06,\n",
       "      'bytes': [32, 99, 97, 110, 110, 111, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' be',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 98, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' regarded',\n",
       "      'logprob': -8.344647e-07,\n",
       "      'bytes': [32, 114, 101, 103, 97, 114, 100, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' as',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' antis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 116, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'emitic',\n",
       "      'logprob': 0,\n",
       "      'bytes': [101, 109, 105, 116, 105, 99],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.', 'logprob': -0.5759395, 'bytes': [46], 'top_logprobs': []},\n",
       "     {'token': ' However',\n",
       "      'logprob': -0.00033539868,\n",
       "      'bytes': [32, 72, 111, 119, 101, 118, 101, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' examples',\n",
       "      'logprob': -0.059355993,\n",
       "      'bytes': [32, 101, 120, 97, 109, 112, 108, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': -0.0015024575,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' antis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 116, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'emitism',\n",
       "      'logprob': 0,\n",
       "      'bytes': [101, 109, 105, 116, 105, 115, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' include',\n",
       "      'logprob': -0.0067455433,\n",
       "      'bytes': [32, 105, 110, 99, 108, 117, 100, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':',\n",
       "      'logprob': -0.0052137123,\n",
       "      'bytes': [58],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Den',\n",
       "      'logprob': -0.743248,\n",
       "      'bytes': [32, 68, 101, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'ying',\n",
       "      'logprob': 0,\n",
       "      'bytes': [121, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Jewish',\n",
       "      'logprob': -3.552374e-05,\n",
       "      'bytes': [32, 74, 101, 119, 105, 115, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' people',\n",
       "      'logprob': -1.1920922e-06,\n",
       "      'bytes': [32, 112, 101, 111, 112, 108, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' their',\n",
       "      'logprob': -4.005352e-05,\n",
       "      'bytes': [32, 116, 104, 101, 105, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' right',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 114, 105, 103, 104, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' self',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 115, 101, 108, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '-determination',\n",
       "      'logprob': 0,\n",
       "      'bytes': [45,\n",
       "       100,\n",
       "       101,\n",
       "       116,\n",
       "       101,\n",
       "       114,\n",
       "       109,\n",
       "       105,\n",
       "       110,\n",
       "       97,\n",
       "       116,\n",
       "       105,\n",
       "       111,\n",
       "       110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -4.577532e-05,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' e',\n",
       "      'logprob': -0.20161799,\n",
       "      'bytes': [32, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.g', 'logprob': 0, 'bytes': [46, 103], 'top_logprobs': []},\n",
       "     {'token': '.,', 'logprob': 0, 'bytes': [46, 44], 'top_logprobs': []},\n",
       "     {'token': ' by',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 98, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' claiming',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 108, 97, 105, 109, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' that',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 97, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' existence',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 101, 120, 105, 115, 116, 101, 110, 99, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a',\n",
       "      'logprob': -3.5404533e-05,\n",
       "      'bytes': [32, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' State',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 83, 116, 97, 116, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Israel',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 73, 115, 114, 97, 101, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a', 'logprob': 0, 'bytes': [32, 97], 'top_logprobs': []},\n",
       "     {'token': ' racist',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 114, 97, 99, 105, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' endeavor',\n",
       "      'logprob': -7.152555e-07,\n",
       "      'bytes': [32, 101, 110, 100, 101, 97, 118, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ';', 'logprob': -0.6931478, 'bytes': [59], 'top_logprobs': []},\n",
       "     {'token': ' Applying',\n",
       "      'logprob': -0.0041414453,\n",
       "      'bytes': [32, 65, 112, 112, 108, 121, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' double',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 100, 111, 117, 98, 108, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' standards',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 115, 116, 97, 110, 100, 97, 114, 100, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' by',\n",
       "      'logprob': -0.0018818541,\n",
       "      'bytes': [32, 98, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' requiring',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 114, 101, 113, 117, 105, 114, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': -1.4305105e-06,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' it',\n",
       "      'logprob': -0.0002305242,\n",
       "      'bytes': [32, 105, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a',\n",
       "      'logprob': -4.887569e-06,\n",
       "      'bytes': [32, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' behavior',\n",
       "      'logprob': -4.7683704e-07,\n",
       "      'bytes': [32, 98, 101, 104, 97, 118, 105, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' not',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 110, 111, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' expected',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 101, 120, 112, 101, 99, 116, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' or',\n",
       "      'logprob': -8.344647e-07,\n",
       "      'bytes': [32, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' demanded',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 100, 101, 109, 97, 110, 100, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' any',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' other',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 116, 104, 101, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' democratic',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 100, 101, 109, 111, 99, 114, 97, 116, 105, 99],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' nation',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 110, 97, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ';', 'logprob': 0, 'bytes': [59], 'top_logprobs': []},\n",
       "     {'token': ' Drawing',\n",
       "      'logprob': -0.006883242,\n",
       "      'bytes': [32, 68, 114, 97, 119, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' comparisons',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 111, 109, 112, 97, 114, 105, 115, 111, 110, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' contemporary',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 99, 111, 110, 116, 101, 109, 112, 111, 114, 97, 114, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Israeli',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 73, 115, 114, 97, 101, 108, 105],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' policy',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 112, 111, 108, 105, 99, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' that',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 97, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Nazis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 78, 97, 122, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ';', 'logprob': -0.6326056, 'bytes': [59], 'top_logprobs': []},\n",
       "     {'token': ' Holding',\n",
       "      'logprob': -0.0003357562,\n",
       "      'bytes': [32, 72, 111, 108, 100, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Jews',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 74, 101, 119, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' collectively',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 111, 108, 108, 101, 99, 116, 105, 118, 101, 108, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' responsible',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 114, 101, 115, 112, 111, 110, 115, 105, 98, 108, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' for',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 102, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' actions',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 99, 116, 105, 111, 110, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' state',\n",
       "      'logprob': -1.4781843e-05,\n",
       "      'bytes': [32, 115, 116, 97, 116, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Israel',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 73, 115, 114, 97, 101, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'The',\n",
       "      'logprob': -0.048622984,\n",
       "      'bytes': [84, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' text',\n",
       "      'logprob': -0.100409195,\n",
       "      'bytes': [32, 116, 101, 120, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':', 'logprob': -0.03604674, 'bytes': [58], 'top_logprobs': []},\n",
       "     {'token': ' \"#', 'logprob': 0, 'bytes': [32, 34, 35], 'top_logprobs': []},\n",
       "     {'token': 'Apart',\n",
       "      'logprob': 0,\n",
       "      'bytes': [65, 112, 97, 114, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'heid',\n",
       "      'logprob': 0,\n",
       "      'bytes': [104, 101, 105, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' z', 'logprob': 0, 'bytes': [32, 122], 'top_logprobs': []},\n",
       "     {'token': 'ion',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'az', 'logprob': 0, 'bytes': [97, 122], 'top_logprobs': []},\n",
       "     {'token': 'ist',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '...', 'logprob': 0, 'bytes': [46, 46, 46], 'top_logprobs': []},\n",
       "     {'token': ' Ethnic',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 69, 116, 104, 110, 105, 99],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' cleansing',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 108, 101, 97, 110, 115, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' land',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 108, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' seizure',\n",
       "      'logprob': -8.5830325e-06,\n",
       "      'bytes': [32, 115, 101, 105, 122, 117, 114, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' home',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 104, 111, 109, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' demolition',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 100, 101, 109, 111, 108, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' military',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 109, 105, 108, 105, 116, 97, 114, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' occupation',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 111, 99, 99, 117, 112, 97, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' bombing',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 98, 111, 109, 98, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Gaza',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 71, 97, 122, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '..', 'logprob': 0, 'bytes': [46, 46], 'top_logprobs': []},\n",
       "     {'token': ' Israel',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 73, 115, 114, 97, 101, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' copying',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 111, 112, 121, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' South',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 83, 111, 117, 116, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Africa',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 65, 102, 114, 105, 99, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' very',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 118, 101, 114, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' well',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 119, 101, 108, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' ',\n",
       "      'logprob': -0.014163449,\n",
       "      'bytes': [32],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' #', 'logprob': 0, 'bytes': [32, 35], 'top_logprobs': []},\n",
       "     {'token': 'Sob',\n",
       "      'logprob': -4.7683704e-07,\n",
       "      'bytes': [83, 111, 98],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'at', 'logprob': 0, 'bytes': [97, 116], 'top_logprobs': []},\n",
       "     {'token': 'Pal',\n",
       "      'logprob': 0,\n",
       "      'bytes': [80, 97, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'est',\n",
       "      'logprob': 0,\n",
       "      'bytes': [101, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'ina',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 110, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'INA', 'logprob': 0, 'bytes': [73, 78, 65], 'top_logprobs': []},\n",
       "     {'token': ' ', 'logprob': 0, 'bytes': [32], 'top_logprobs': []},\n",
       "     {'token': ' #', 'logprob': 0, 'bytes': [32, 35], 'top_logprobs': []},\n",
       "     {'token': 'Israeli',\n",
       "      'logprob': 0,\n",
       "      'bytes': [73, 115, 114, 97, 101, 108, 105],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Cr',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [67, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'imes',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 109, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' https',\n",
       "      'logprob': -4.76836e-06,\n",
       "      'bytes': [32, 104, 116, 116, 112, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '://', 'logprob': 0, 'bytes': [58, 47, 47], 'top_logprobs': []},\n",
       "     {'token': 't', 'logprob': 0, 'bytes': [116], 'top_logprobs': []},\n",
       "     {'token': '.co',\n",
       "      'logprob': 0,\n",
       "      'bytes': [46, 99, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '/v', 'logprob': 0, 'bytes': [47, 118], 'top_logprobs': []},\n",
       "     {'token': 'Y', 'logprob': 0, 'bytes': [89], 'top_logprobs': []},\n",
       "     {'token': 'nn',\n",
       "      'logprob': -4.7683704e-07,\n",
       "      'bytes': [110, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'n', 'logprob': 0, 'bytes': [110], 'top_logprobs': []},\n",
       "     {'token': 'Q',\n",
       "      'logprob': -5.960463e-07,\n",
       "      'bytes': [81],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'IG', 'logprob': 0, 'bytes': [73, 71], 'top_logprobs': []},\n",
       "     {'token': '8', 'logprob': 0, 'bytes': [56], 'top_logprobs': []},\n",
       "     {'token': 'U',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [85],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\"\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [34, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'This',\n",
       "      'logprob': -0.00043061044,\n",
       "      'bytes': [84, 104, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' text',\n",
       "      'logprob': -4.1841584e-05,\n",
       "      'bytes': [32, 116, 101, 120, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' calls',\n",
       "      'logprob': -0.58625084,\n",
       "      'bytes': [32, 99, 97, 108, 108, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Israel',\n",
       "      'logprob': -0.029969094,\n",
       "      'bytes': [32, 73, 115, 114, 97, 101, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' an',\n",
       "      'logprob': -0.47408855,\n",
       "      'bytes': [32, 97, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' \"', 'logprob': 0, 'bytes': [32, 34], 'top_logprobs': []},\n",
       "     {'token': 'Apart',\n",
       "      'logprob': -0.001034797,\n",
       "      'bytes': [65, 112, 97, 114, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'heid',\n",
       "      'logprob': 0,\n",
       "      'bytes': [104, 101, 105, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' z',\n",
       "      'logprob': -6.198864e-06,\n",
       "      'bytes': [32, 122],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'ion',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'az', 'logprob': 0, 'bytes': [97, 122], 'top_logprobs': []},\n",
       "     {'token': 'ist',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\"', 'logprob': 0, 'bytes': [34], 'top_logprobs': []},\n",
       "     {'token': ' –',\n",
       "      'logprob': -0.024812024,\n",
       "      'bytes': [32, 226, 128, 147],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' \"',\n",
       "      'logprob': -0.014710532,\n",
       "      'bytes': [32, 34],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'zion',\n",
       "      'logprob': -0.004078405,\n",
       "      'bytes': [122, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'az', 'logprob': 0, 'bytes': [97, 122], 'top_logprobs': []},\n",
       "     {'token': 'ist',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\"', 'logprob': 0, 'bytes': [34], 'top_logprobs': []},\n",
       "     {'token': ' seems',\n",
       "      'logprob': -0.00047338722,\n",
       "      'bytes': [32, 115, 101, 101, 109, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' like',\n",
       "      'logprob': -0.0005529782,\n",
       "      'bytes': [32, 108, 105, 107, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a',\n",
       "      'logprob': -0.062049743,\n",
       "      'bytes': [32, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' port',\n",
       "      'logprob': -0.062663965,\n",
       "      'bytes': [32, 112, 111, 114, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'man',\n",
       "      'logprob': 0,\n",
       "      'bytes': [109, 97, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'teau',\n",
       "      'logprob': 0,\n",
       "      'bytes': [116, 101, 97, 117],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Zionist',\n",
       "      'logprob': -7.0333235e-06,\n",
       "      'bytes': [32, 90, 105, 111, 110, 105, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Nazi',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 78, 97, 122, 105],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -0.0028110535,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' which',\n",
       "      'logprob': -0.0074453256,\n",
       "      'bytes': [32, 119, 104, 105, 99, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' directly',\n",
       "      'logprob': -0.023271536,\n",
       "      'bytes': [32, 100, 105, 114, 101, 99, 116, 108, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' compares',\n",
       "      'logprob': -4.5656117e-05,\n",
       "      'bytes': [32, 99, 111, 109, 112, 97, 114, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Zion',\n",
       "      'logprob': -0.0019559793,\n",
       "      'bytes': [32, 90, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'ism',\n",
       "      'logprob': -0.011047743,\n",
       "      'bytes': [105, 115, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' or',\n",
       "      'logprob': -1.7881378e-06,\n",
       "      'bytes': [32, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Israeli',\n",
       "      'logprob': -0.008614995,\n",
       "      'bytes': [32, 73, 115, 114, 97, 101, 108, 105],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' policy',\n",
       "      'logprob': -0.47826284,\n",
       "      'bytes': [32, 112, 111, 108, 105, 99, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Nazis',\n",
       "      'logprob': -0.0015025765,\n",
       "      'bytes': [32, 78, 97, 122, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': -0.4244725,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'According',\n",
       "      'logprob': -0.00091141637,\n",
       "      'bytes': [65, 99, 99, 111, 114, 100, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' IH', 'logprob': 0, 'bytes': [32, 73, 72], 'top_logprobs': []},\n",
       "     {'token': 'RA', 'logprob': 0, 'bytes': [82, 65], 'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -2.5629668e-05,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' \"',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 34],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Drawing',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [68, 114, 97, 119, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' comparisons',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 111, 109, 112, 97, 114, 105, 115, 111, 110, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' contemporary',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 111, 110, 116, 101, 109, 112, 111, 114, 97, 114, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Israeli',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 73, 115, 114, 97, 101, 108, 105],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' policy',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 112, 111, 108, 105, 99, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' that',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 97, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Nazis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 78, 97, 122, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\"',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [34],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' an',\n",
       "      'logprob': -0.0002244459,\n",
       "      'bytes': [32, 97, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' example',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 101, 120, 97, 109, 112, 108, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' antis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 116, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'emitism',\n",
       "      'logprob': 0,\n",
       "      'bytes': [101, 109, 105, 116, 105, 115, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'It',\n",
       "      'logprob': -4.5417706e-05,\n",
       "      'bytes': [73, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' also',\n",
       "      'logprob': -0.0011704268,\n",
       "      'bytes': [32, 97, 108, 115, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' accuses',\n",
       "      'logprob': -2.5510462e-05,\n",
       "      'bytes': [32, 97, 99, 99, 117, 115, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Israel',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 73, 115, 114, 97, 101, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' \"',\n",
       "      'logprob': -0.15318182,\n",
       "      'bytes': [32, 34],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Ethnic',\n",
       "      'logprob': -0.008780914,\n",
       "      'bytes': [69, 116, 104, 110, 105, 99],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' cleansing',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 108, 101, 97, 110, 115, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' land',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 108, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' seizure',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 115, 101, 105, 122, 117, 114, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -0.011048096,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' home',\n",
       "      'logprob': -0.0031778333,\n",
       "      'bytes': [32, 104, 111, 109, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' demolition',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 100, 101, 109, 111, 108, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' military',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 109, 105, 108, 105, 116, 97, 114, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' occupation',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 99, 99, 117, 112, 97, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' bombing',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 98, 111, 109, 98, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Gaza',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 71, 97, 122, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\"',\n",
       "      'logprob': -0.0001256387,\n",
       "      'bytes': [34],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' compares',\n",
       "      'logprob': -3.576278e-07,\n",
       "      'bytes': [32, 99, 111, 109, 112, 97, 114, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' it',\n",
       "      'logprob': -6.9141147e-06,\n",
       "      'bytes': [32, 105, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' South',\n",
       "      'logprob': -0.18171851,\n",
       "      'bytes': [32, 83, 111, 117, 116, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' African',\n",
       "      'logprob': -0.00412993,\n",
       "      'bytes': [32, 65, 102, 114, 105, 99, 97, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' apartheid',\n",
       "      'logprob': -4.5417706e-05,\n",
       "      'bytes': [32, 97, 112, 97, 114, 116, 104, 101, 105, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -0.00023088173,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' which',\n",
       "      'logprob': -7.152301e-05,\n",
       "      'bytes': [32, 119, 104, 105, 99, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' could',\n",
       "      'logprob': -0.029781545,\n",
       "      'bytes': [32, 99, 111, 117, 108, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' be',\n",
       "      'logprob': -9.536698e-06,\n",
       "      'bytes': [32, 98, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' seen',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 115, 101, 101, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' as',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' claiming',\n",
       "      'logprob': -3.3378547e-06,\n",
       "      'bytes': [32, 99, 108, 97, 105, 109, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Israel',\n",
       "      'logprob': -0.00069117494,\n",
       "      'bytes': [32, 73, 115, 114, 97, 101, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a', 'logprob': 0, 'bytes': [32, 97], 'top_logprobs': []},\n",
       "     {'token': ' racist',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 114, 97, 99, 105, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' endeavor',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 101, 110, 100, 101, 97, 118, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': -0.01107875,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'The',\n",
       "      'logprob': -0.2521225,\n",
       "      'bytes': [84, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' term',\n",
       "      'logprob': -0.39526954,\n",
       "      'bytes': [32, 116, 101, 114, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' \"', 'logprob': 0, 'bytes': [32, 34], 'top_logprobs': []},\n",
       "     {'token': 'zion',\n",
       "      'logprob': -0.00034326382,\n",
       "      'bytes': [122, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'az', 'logprob': 0, 'bytes': [97, 122], 'top_logprobs': []},\n",
       "     {'token': 'ist',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\"', 'logprob': 0, 'bytes': [34], 'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': -0.57594436,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a',\n",
       "      'logprob': -1.6808368e-05,\n",
       "      'bytes': [32, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' clear',\n",
       "      'logprob': -4.4344873e-05,\n",
       "      'bytes': [32, 99, 108, 101, 97, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' comparison',\n",
       "      'logprob': -0.0005722792,\n",
       "      'bytes': [32, 99, 111, 109, 112, 97, 114, 105, 115, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Nazis',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [32, 78, 97, 122, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'So', 'logprob': 0, 'bytes': [83, 111], 'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' this',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' fits',\n",
       "      'logprob': -0.008623149,\n",
       "      'bytes': [32, 102, 105, 116, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -2.4437606e-05,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' IH', 'logprob': 0, 'bytes': [32, 73, 72], 'top_logprobs': []},\n",
       "     {'token': 'RA', 'logprob': 0, 'bytes': [82, 65], 'top_logprobs': []},\n",
       "     {'token': ' example',\n",
       "      'logprob': -0.006715704,\n",
       "      'bytes': [32, 101, 120, 97, 109, 112, 108, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': -3.576278e-07,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' antis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 116, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'emitism',\n",
       "      'logprob': 0,\n",
       "      'bytes': [101, 109, 105, 116, 105, 115, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'I', 'logprob': -0.4833162, 'bytes': [73], 'top_logprobs': []},\n",
       "     {'token': ' need',\n",
       "      'logprob': -0.1602241,\n",
       "      'bytes': [32, 110, 101, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' output',\n",
       "      'logprob': -0.16022725,\n",
       "      'bytes': [32, 111, 117, 116, 112, 117, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' JSON',\n",
       "      'logprob': -0.007494548,\n",
       "      'bytes': [32, 74, 83, 79, 78],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' only',\n",
       "      'logprob': -0.42019597,\n",
       "      'bytes': [32, 111, 110, 108, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':\\n\\n',\n",
       "      'logprob': -0.00028200942,\n",
       "      'bytes': [58, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '{\\n\\n',\n",
       "      'logprob': -7.486063e-05,\n",
       "      'bytes': [123, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' ',\n",
       "      'logprob': -1.668916e-05,\n",
       "      'bytes': [32],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' \"', 'logprob': 0, 'bytes': [32, 34], 'top_logprobs': []},\n",
       "     {'token': 'answer',\n",
       "      'logprob': 0,\n",
       "      'bytes': [97, 110, 115, 119, 101, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\":', 'logprob': 0, 'bytes': [34, 58], 'top_logprobs': []},\n",
       "     {'token': ' \"',\n",
       "      'logprob': -0.0019286142,\n",
       "      'bytes': [32, 34],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'True',\n",
       "      'logprob': 0,\n",
       "      'bytes': [84, 114, 117, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\"',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [34],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' or',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' \"', 'logprob': 0, 'bytes': [32, 34], 'top_logprobs': []},\n",
       "     {'token': 'False',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [70, 97, 108, 115, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\",\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [34, 44, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' ', 'logprob': 0, 'bytes': [32], 'top_logprobs': []},\n",
       "     {'token': ' \"', 'logprob': 0, 'bytes': [32, 34], 'top_logprobs': []},\n",
       "     {'token': 'description',\n",
       "      'logprob': 0,\n",
       "      'bytes': [100, 101, 115, 99, 114, 105, 112, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\":', 'logprob': 0, 'bytes': [34, 58], 'top_logprobs': []},\n",
       "     {'token': ' \"<',\n",
       "      'logprob': -0.0015023384,\n",
       "      'bytes': [32, 34, 60],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '1', 'logprob': 0, 'bytes': [49], 'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' ', 'logprob': 0, 'bytes': [32], 'top_logprobs': []},\n",
       "     {'token': '3', 'logprob': 0, 'bytes': [51], 'top_logprobs': []},\n",
       "     {'token': ' sentences',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 115, 101, 110, 116, 101, 110, 99, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' explaining',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 101, 120, 112, 108, 97, 105, 110, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' choice',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 104, 111, 105, 99, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '>', 'logprob': 0, 'bytes': [62], 'top_logprobs': []},\n",
       "     {'token': '\"\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [34, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '}\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [125, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Answer',\n",
       "      'logprob': -2.264974e-06,\n",
       "      'bytes': [65, 110, 115, 119, 101, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' must',\n",
       "      'logprob': -0.029751342,\n",
       "      'bytes': [32, 109, 117, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' be',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 98, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' exactly',\n",
       "      'logprob': -0.078889966,\n",
       "      'bytes': [32, 101, 120, 97, 99, 116, 108, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' True',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 84, 114, 117, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' or',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' False',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 70, 97, 108, 115, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': -1.1920922e-06,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Description',\n",
       "      'logprob': 0,\n",
       "      'bytes': [68, 101, 115, 99, 114, 105, 112, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' must',\n",
       "      'logprob': -8.344647e-07,\n",
       "      'bytes': [32, 109, 117, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' cite',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 105, 116, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' concrete',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 111, 110, 99, 114, 101, 116, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' cues',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 117, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' from',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 102, 114, 111, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' text',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 101, 120, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': -7.867782e-06,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Rules',\n",
       "      'logprob': -0.1662511,\n",
       "      'bytes': [82, 117, 108, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':',\n",
       "      'logprob': -0.023245564,\n",
       "      'bytes': [58],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Forget',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 70, 111, 114, 103, 101, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' all',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 108, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' other',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 116, 104, 101, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' definitions',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 100, 101, 102, 105, 110, 105, 116, 105, 111, 110, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' use',\n",
       "      'logprob': -0.0039809276,\n",
       "      'bytes': [32, 117, 115, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' only',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 110, 108, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' IH', 'logprob': 0, 'bytes': [32, 73, 72], 'top_logprobs': []},\n",
       "     {'token': 'RA', 'logprob': 0, 'bytes': [82, 65], 'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': -0.005233991,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'The',\n",
       "      'logprob': -0.00040070133,\n",
       "      'bytes': [84, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' definition',\n",
       "      'logprob': -0.002498602,\n",
       "      'bytes': [32, 100, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' in',\n",
       "      'logprob': -0.048656937,\n",
       "      'bytes': [32, 105, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' message',\n",
       "      'logprob': -0.002475652,\n",
       "      'bytes': [32, 109, 101, 115, 115, 97, 103, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' truncated',\n",
       "      'logprob': -0.0053184214,\n",
       "      'bytes': [32, 116, 114, 117, 110, 99, 97, 116, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' but',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 98, 117, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' I',\n",
       "      'logprob': -4.5299425e-06,\n",
       "      'bytes': [32, 73],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' should',\n",
       "      'logprob': -0.0095364265,\n",
       "      'bytes': [32, 115, 104, 111, 117, 108, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' use',\n",
       "      'logprob': -4.553691e-05,\n",
       "      'bytes': [32, 117, 115, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -0.0040807794,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' provided',\n",
       "      'logprob': -0.251965,\n",
       "      'bytes': [32, 112, 114, 111, 118, 105, 100, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' one',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 110, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -0.038730193,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' but',\n",
       "      'logprob': -0.034048773,\n",
       "      'bytes': [32, 98, 117, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': \" it's\",\n",
       "      'logprob': -0.008455075,\n",
       "      'bytes': [32, 105, 116, 39, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' incomplete',\n",
       "      'logprob': -0.011060948,\n",
       "      'bytes': [32, 105, 110, 99, 111, 109, 112, 108, 101, 116, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.', 'logprob': 0, 'bytes': [46], 'top_logprobs': []},\n",
       "     {'token': ' However',\n",
       "      'logprob': -0.0011703077,\n",
       "      'bytes': [32, 72, 111, 119, 101, 118, 101, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -0.31821755,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' system',\n",
       "      'logprob': -0.25529033,\n",
       "      'bytes': [32, 115, 121, 115, 116, 101, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' prompt',\n",
       "      'logprob': -3.826545e-05,\n",
       "      'bytes': [32, 112, 114, 111, 109, 112, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' says',\n",
       "      'logprob': -0.0014135379,\n",
       "      'bytes': [32, 115, 97, 121, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' use',\n",
       "      'logprob': -0.0017213303,\n",
       "      'bytes': [32, 117, 115, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' IH',\n",
       "      'logprob': -0.31327558,\n",
       "      'bytes': [32, 73, 72],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'RA',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [82, 65],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Definition',\n",
       "      'logprob': -5.960463e-07,\n",
       "      'bytes': [32, 68, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' as',\n",
       "      'logprob': -0.033655915,\n",
       "      'bytes': [32, 97, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' given',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 103, 105, 118, 101, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': -0.12785456,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'The',\n",
       "      'logprob': -0.0024762463,\n",
       "      'bytes': [84, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': \" user's\",\n",
       "      'logprob': -0.07889008,\n",
       "      'bytes': [32, 117, 115, 101, 114, 39, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' message',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 109, 101, 115, 115, 97, 103, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' starts',\n",
       "      'logprob': -0.048680212,\n",
       "      'bytes': [32, 115, 116, 97, 114, 116, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' with',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 119, 105, 116, 104],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' <',\n",
       "      'logprob': -0.000123374,\n",
       "      'bytes': [32, 60],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'annotation',\n",
       "      'logprob': 0,\n",
       "      'bytes': [97, 110, 110, 111, 116, 97, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '_guid',\n",
       "      'logprob': 0,\n",
       "      'bytes': [95, 103, 117, 105, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'elines',\n",
       "      'logprob': 0,\n",
       "      'bytes': [101, 108, 105, 110, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '>',\n",
       "      'logprob': -0.00070999685,\n",
       "      'bytes': [62],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'logprob': -1.9073304e-05,\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' provides',\n",
       "      'logprob': -0.00034874555,\n",
       "      'bytes': [32, 112, 114, 111, 118, 105, 100, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -7.509903e-05,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' name',\n",
       "      'logprob': -4.7683704e-07,\n",
       "      'bytes': [32, 110, 97, 109, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a',\n",
       "      'logprob': -0.022160405,\n",
       "      'bytes': [32, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' partial',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 112, 97, 114, 116, 105, 97, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' definition',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 100, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' then',\n",
       "      'logprob': -0.00043084874,\n",
       "      'bytes': [32, 116, 104, 101, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -0.47189945,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Text',\n",
       "      'logprob': -0.57594275,\n",
       "      'bytes': [32, 84, 101, 120, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'I', 'logprob': -0.31335098, 'bytes': [73], 'top_logprobs': []},\n",
       "     {'token': ' need',\n",
       "      'logprob': -0.029811166,\n",
       "      'bytes': [32, 110, 101, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' base',\n",
       "      'logprob': -0.9392029,\n",
       "      'bytes': [32, 98, 97, 115, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' it',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [32, 105, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' on',\n",
       "      'logprob': -3.576278e-07,\n",
       "      'bytes': [32, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': \" what's\",\n",
       "      'logprob': -0.39651427,\n",
       "      'bytes': [32, 119, 104, 97, 116, 39, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' given',\n",
       "      'logprob': -4.5417706e-05,\n",
       "      'bytes': [32, 103, 105, 118, 101, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -0.019836385,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' but',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 98, 117, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -0.3233016,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' full',\n",
       "      'logprob': -0.13593213,\n",
       "      'bytes': [32, 102, 117, 108, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' IH',\n",
       "      'logprob': -0.0041097477,\n",
       "      'bytes': [32, 73, 72],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'RA', 'logprob': 0, 'bytes': [82, 65], 'top_logprobs': []},\n",
       "     {'token': ' includes',\n",
       "      'logprob': -0.00033825875,\n",
       "      'bytes': [32, 105, 110, 99, 108, 117, 100, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' those',\n",
       "      'logprob': -0.0009134411,\n",
       "      'bytes': [32, 116, 104, 111, 115, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' examples',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 101, 120, 97, 109, 112, 108, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': -0.0005384426,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'The',\n",
       "      'logprob': -0.0002619877,\n",
       "      'bytes': [84, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' prompt',\n",
       "      'logprob': -0.36706752,\n",
       "      'bytes': [32, 112, 114, 111, 109, 112, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' says',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 115, 97, 121, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':', 'logprob': 0, 'bytes': [58], 'top_logprobs': []},\n",
       "     {'token': ' \"',\n",
       "      'logprob': -4.7683704e-07,\n",
       "      'bytes': [32, 34],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Forget',\n",
       "      'logprob': -0.12695745,\n",
       "      'bytes': [70, 111, 114, 103, 101, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' all',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 108, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' other',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 111, 116, 104, 101, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' definitions',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 100, 101, 102, 105, 110, 105, 116, 105, 111, 110, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' antis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 116, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'emitism',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [101, 109, 105, 116, 105, 115, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' except',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 101, 120, 99, 101, 112, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' one',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 110, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' by',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 98, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' IH', 'logprob': 0, 'bytes': [32, 73, 72], 'top_logprobs': []},\n",
       "     {'token': 'RA',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [82, 65],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Definition',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 68, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\"\\n\\n',\n",
       "      'logprob': -0.0005529782,\n",
       "      'bytes': [46, 34, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'But',\n",
       "      'logprob': -0.20160522,\n",
       "      'bytes': [66, 117, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -0.00047636605,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' provided',\n",
       "      'logprob': -0.693151,\n",
       "      'bytes': [32, 112, 114, 111, 118, 105, 100, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' definition',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 100, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' incomplete',\n",
       "      'logprob': -0.48953688,\n",
       "      'bytes': [32, 105, 110, 99, 111, 109, 112, 108, 101, 116, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.', 'logprob': -0.00829643, 'bytes': [46], 'top_logprobs': []},\n",
       "     {'token': ' Perhaps',\n",
       "      'logprob': -0.15646744,\n",
       "      'bytes': [32, 80, 101, 114, 104, 97, 112, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' I',\n",
       "      'logprob': -3.576278e-07,\n",
       "      'bytes': [32, 73],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' should',\n",
       "      'logprob': -0.03804176,\n",
       "      'bytes': [32, 115, 104, 111, 117, 108, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' recall',\n",
       "      'logprob': -0.0023544705,\n",
       "      'bytes': [32, 114, 101, 99, 97, 108, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' standard',\n",
       "      'logprob': -0.47408205,\n",
       "      'bytes': [32, 115, 116, 97, 110, 100, 97, 114, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' IH',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 73, 72],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'RA', 'logprob': 0, 'bytes': [82, 65], 'top_logprobs': []},\n",
       "     {'token': ' definition',\n",
       "      'logprob': -0.003602564,\n",
       "      'bytes': [32, 100, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': -0.0040919394,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Looking',\n",
       "      'logprob': -0.10223294,\n",
       "      'bytes': [76, 111, 111, 107, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' closely',\n",
       "      'logprob': -7.986991e-06,\n",
       "      'bytes': [32, 99, 108, 111, 115, 101, 108, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':', 'logprob': -0.07888975, 'bytes': [58], 'top_logprobs': []},\n",
       "     {'token': ' The',\n",
       "      'logprob': -0.00015865498,\n",
       "      'bytes': [32, 84, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' definition',\n",
       "      'logprob': -0.39993674,\n",
       "      'bytes': [32, 100, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': -0.09074481,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' cut',\n",
       "      'logprob': -0.0031868645,\n",
       "      'bytes': [32, 99, 117, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' off',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':',\n",
       "      'logprob': -1.0251946e-05,\n",
       "      'bytes': [58],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' \"',\n",
       "      'logprob': -7.867782e-06,\n",
       "      'bytes': [32, 34],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'R',\n",
       "      'logprob': -0.0001038259,\n",
       "      'bytes': [82],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'het',\n",
       "      'logprob': 0,\n",
       "      'bytes': [104, 101, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'orical',\n",
       "      'logprob': 0,\n",
       "      'bytes': [111, 114, 105, 99, 97, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' physical',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 112, 104, 121, 115, 105, 99, 97, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' manifestations',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32,\n",
       "       109,\n",
       "       97,\n",
       "       110,\n",
       "       105,\n",
       "       102,\n",
       "       101,\n",
       "       115,\n",
       "       116,\n",
       "       97,\n",
       "       116,\n",
       "       105,\n",
       "       111,\n",
       "       110,\n",
       "       115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 102],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' antis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 116, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'emitism',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [101, 109, 105, 116, 105, 115, 109],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' are',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 114, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' directed',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 100, 105, 114, 101, 99, 116, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Text',\n",
       "      'logprob': -0.000913322,\n",
       "      'bytes': [32, 84, 101, 120, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':',\n",
       "      'logprob': -0.0005529782,\n",
       "      'bytes': [58],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' #',\n",
       "      'logprob': -0.72681797,\n",
       "      'bytes': [32, 35],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Apart',\n",
       "      'logprob': -7.152532e-06,\n",
       "      'bytes': [65, 112, 97, 114, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'heid',\n",
       "      'logprob': -2.0265559e-06,\n",
       "      'bytes': [104, 101, 105, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '...\"\\n\\n',\n",
       "      'logprob': -8.463824e-06,\n",
       "      'bytes': [46, 46, 46, 34, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'It',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [73, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' seems',\n",
       "      'logprob': -1.1920928e-07,\n",
       "      'bytes': [32, 115, 101, 101, 109, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' like',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 108, 105, 107, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' there',\n",
       "      'logprob': -0.8565339,\n",
       "      'bytes': [32, 116, 104, 101, 114, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' might',\n",
       "      'logprob': -0.0015844661,\n",
       "      'bytes': [32, 109, 105, 103, 104, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' be',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 98, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a', 'logprob': 0, 'bytes': [32, 97], 'top_logprobs': []},\n",
       "     {'token': ' formatting',\n",
       "      'logprob': -4.5299425e-06,\n",
       "      'bytes': [32, 102, 111, 114, 109, 97, 116, 116, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' error',\n",
       "      'logprob': -3.576278e-07,\n",
       "      'bytes': [32, 101, 114, 114, 111, 114],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -0.0008045774,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -0.08893956,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' definition',\n",
       "      'logprob': -0.009244613,\n",
       "      'bytes': [32, 100, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' is',\n",
       "      'logprob': -0.00014995404,\n",
       "      'bytes': [32, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' partial',\n",
       "      'logprob': -0.349086,\n",
       "      'bytes': [32, 112, 97, 114, 116, 105, 97, 108],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -2.4318399e-05,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' followed',\n",
       "      'logprob': -0.0015368566,\n",
       "      'bytes': [32, 102, 111, 108, 108, 111, 119, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' by',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 98, 121],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' text',\n",
       "      'logprob': -2.7536968e-05,\n",
       "      'bytes': [32, 116, 101, 120, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'I', 'logprob': -0.47069645, 'bytes': [73], 'top_logprobs': []},\n",
       "     {'token': ' think',\n",
       "      'logprob': -0.029772634,\n",
       "      'bytes': [32, 116, 104, 105, 110, 107],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' I',\n",
       "      'logprob': -0.15928403,\n",
       "      'bytes': [32, 73],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' need',\n",
       "      'logprob': -0.0110482145,\n",
       "      'bytes': [32, 110, 101, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' use',\n",
       "      'logprob': -0.012956717,\n",
       "      'bytes': [32, 117, 115, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'logprob': -0.38687238,\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' standard',\n",
       "      'logprob': -0.0723726,\n",
       "      'bytes': [32, 115, 116, 97, 110, 100, 97, 114, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' IH', 'logprob': 0, 'bytes': [32, 73, 72], 'top_logprobs': []},\n",
       "     {'token': 'RA',\n",
       "      'logprob': -5.0067774e-06,\n",
       "      'bytes': [82, 65],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' definition',\n",
       "      'logprob': -2.0265559e-06,\n",
       "      'bytes': [32, 100, 101, 102, 105, 110, 105, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -0.0013438966,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' as',\n",
       "      'logprob': -0.023349334,\n",
       "      'bytes': [32, 97, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': \" that's\",\n",
       "      'logprob': -0.3151129,\n",
       "      'bytes': [32, 116, 104, 97, 116, 39, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': \" what's\",\n",
       "      'logprob': -0.00622619,\n",
       "      'bytes': [32, 119, 104, 97, 116, 39, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' intended',\n",
       "      'logprob': -0.09437983,\n",
       "      'bytes': [32, 105, 110, 116, 101, 110, 100, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Based',\n",
       "      'logprob': -0.048922602,\n",
       "      'bytes': [66, 97, 115, 101, 100],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' on',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' that',\n",
       "      'logprob': -4.0172723e-05,\n",
       "      'bytes': [32, 116, 104, 97, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'logprob': 0, 'bytes': [44], 'top_logprobs': []},\n",
       "     {'token': ' this',\n",
       "      'logprob': -0.10028789,\n",
       "      'bytes': [32, 116, 104, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' text',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [32, 116, 101, 120, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' draws',\n",
       "      'logprob': -0.529794,\n",
       "      'bytes': [32, 100, 114, 97, 119, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' a',\n",
       "      'logprob': -0.31770578,\n",
       "      'bytes': [32, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' comparison',\n",
       "      'logprob': -3.81469e-06,\n",
       "      'bytes': [32, 99, 111, 109, 112, 97, 114, 105, 115, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': -0.0046203523,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Nazis',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 78, 97, 122, 105, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' via',\n",
       "      'logprob': -1.6808368e-05,\n",
       "      'bytes': [32, 118, 105, 97],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' \"', 'logprob': 0, 'bytes': [32, 34], 'top_logprobs': []},\n",
       "     {'token': 'zion',\n",
       "      'logprob': 0,\n",
       "      'bytes': [122, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'az', 'logprob': 0, 'bytes': [97, 122], 'top_logprobs': []},\n",
       "     {'token': 'ist',\n",
       "      'logprob': 0,\n",
       "      'bytes': [105, 115, 116],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '\",',\n",
       "      'logprob': -1.1444026e-05,\n",
       "      'bytes': [34, 44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' so',\n",
       "      'logprob': -0.003216928,\n",
       "      'bytes': [32, 115, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' True',\n",
       "      'logprob': -0.0011709031,\n",
       "      'bytes': [32, 84, 114, 117, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.\\n\\n',\n",
       "      'logprob': 0,\n",
       "      'bytes': [46, 10, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Description',\n",
       "      'logprob': -2.3841855e-07,\n",
       "      'bytes': [68, 101, 115, 99, 114, 105, 112, 116, 105, 111, 110],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ':', 'logprob': 0, 'bytes': [58], 'top_logprobs': []},\n",
       "     {'token': ' ',\n",
       "      'logprob': -0.0005533356,\n",
       "      'bytes': [32],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '1', 'logprob': 0, 'bytes': [49], 'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'logprob': -0.023245446,\n",
       "      'bytes': [32, 116, 111],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' ', 'logprob': 0, 'bytes': [32], 'top_logprobs': []},\n",
       "     {'token': '3', 'logprob': 0, 'bytes': [51], 'top_logprobs': []},\n",
       "     {'token': ' sentences',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 115, 101, 110, 116, 101, 110, 99, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'logprob': -2.0265559e-06,\n",
       "      'bytes': [44],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' citing',\n",
       "      'logprob': -8.344647e-07,\n",
       "      'bytes': [32, 99, 105, 116, 105, 110, 103],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' concrete',\n",
       "      'logprob': -3.6954812e-06,\n",
       "      'bytes': [32, 99, 111, 110, 99, 114, 101, 116, 101],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' cues',\n",
       "      'logprob': 0,\n",
       "      'bytes': [32, 99, 117, 101, 115],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.',\n",
       "      'logprob': -0.00062648696,\n",
       "      'bytes': [46],\n",
       "      'top_logprobs': []},\n",
       "     {'token': '{\\n',\n",
       "      'logprob': -0.5759394,\n",
       "      'bytes': [123, 10],\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' ', 'logprob': 0, 'bytes': [32], 'top_logprobs': []},\n",
       "     {'token': ' \"', 'logprob': 0, 'bytes': [32, 34], 'top_logprobs': []}],\n",
       "    'refusal': []},\n",
       "   'finish_reason': 'length',\n",
       "   'native_finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '{\\n  \"',\n",
       "    'refusal': None,\n",
       "    'reasoning': None}}],\n",
       " 'system_fingerprint': 'fp_19e21a36c0',\n",
       " 'usage': {'prompt_tokens': 963,\n",
       "  'completion_tokens': 799,\n",
       "  'total_tokens': 1762,\n",
       "  'prompt_tokens_details': {'cached_tokens': 676, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 796}}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "637471e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'provider', 'model', 'object', 'created', 'choices', 'system_fingerprint', 'usage'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c733eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant', 'content': '{\\n  \"', 'refusal': None, 'reasoning': None}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp['choices'][0][\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "067ba709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://openrouter.ai/api/v1/models\"\n",
    "response = requests.get(url)\n",
    "models = response.json()[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'moonshotai/kimi-k2-0905',\n",
      "  'context_length': 262144,\n",
      "  'created': 1757021147,\n",
      "  'description': 'Kimi K2 0905 is the September update of [Kimi K2 '\n",
      "                 '0711](moonshotai/kimi-k2). It is a large-scale '\n",
      "                 'Mixture-of-Experts (MoE) language model developed by '\n",
      "                 'Moonshot AI, featuring 1 trillion total parameters with 32 '\n",
      "                 'billion active per forward pass. It supports long-context '\n",
      "                 'inference up to 256k tokens, extended from the previous '\n",
      "                 '128k.\\n'\n",
      "                 '\\n'\n",
      "                 'This update improves agentic coding with higher accuracy and '\n",
      "                 'better generalization across scaffolds, and enhances '\n",
      "                 'frontend coding with more aesthetic and functional outputs '\n",
      "                 'for web, 3D, and related tasks. Kimi K2 is optimized for '\n",
      "                 'agentic capabilities, including advanced tool use, '\n",
      "                 'reasoning, and code synthesis. It excels across coding '\n",
      "                 '(LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), '\n",
      "                 'and tool-use (Tau2, AceBench) benchmarks. The model is '\n",
      "                 'trained with a novel stack incorporating the MuonClip '\n",
      "                 'optimizer for stable large-scale MoE training.',\n",
      "  'hugging_face_id': 'moonshotai/Kimi-K2-Instruct-0905',\n",
      "  'id': 'moonshotai/kimi-k2-0905',\n",
      "  'name': 'MoonshotAI: Kimi K2 0905',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000011853',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000002962',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 262144,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'bytedance/seed-oss-36b-instruct',\n",
      "  'context_length': 131072,\n",
      "  'created': 1756834704,\n",
      "  'description': 'Seed-OSS-36B-Instruct is a 36B-parameter instruction-tuned '\n",
      "                 'reasoning language model from ByteDance’s Seed team, '\n",
      "                 'released under Apache-2.0. The model is optimized for '\n",
      "                 'general instruction following with strong performance in '\n",
      "                 'reasoning, mathematics, coding, tool use/agentic workflows, '\n",
      "                 'and multilingual tasks, and is intended for international '\n",
      "                 '(i18n) use cases. It is not currently possible to control '\n",
      "                 'the reasoning effort.',\n",
      "  'hugging_face_id': 'ByteDance-Seed/Seed-OSS-36B-Instruct',\n",
      "  'id': 'bytedance/seed-oss-36b-instruct',\n",
      "  'name': 'ByteDance: Seed OSS 36B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000414848',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000001036616',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-30b-a3b-thinking-2507',\n",
      "  'context_length': 262144,\n",
      "  'created': 1756399192,\n",
      "  'description': 'Qwen3-30B-A3B-Thinking-2507 is a 30B parameter '\n",
      "                 'Mixture-of-Experts reasoning model optimized for complex '\n",
      "                 'tasks requiring extended multi-step thinking. The model is '\n",
      "                 'designed specifically for “thinking mode,” where internal '\n",
      "                 'reasoning traces are separated from final answers.\\n'\n",
      "                 '\\n'\n",
      "                 'Compared to earlier Qwen3-30B releases, this version '\n",
      "                 'improves performance across logical reasoning, mathematics, '\n",
      "                 'science, coding, and multilingual benchmarks. It also '\n",
      "                 'demonstrates stronger instruction following, tool use, and '\n",
      "                 'alignment with human preferences. With higher reasoning '\n",
      "                 'efficiency and extended output budgets, it is best suited '\n",
      "                 'for advanced research, competitive problem solving, and '\n",
      "                 'agentic applications requiring structured long-context '\n",
      "                 'reasoning.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-30B-A3B-Thinking-2507',\n",
      "  'id': 'qwen/qwen3-30b-a3b-thinking-2507',\n",
      "  'name': 'Qwen: Qwen3 30B A3B Thinking 2507',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000002852',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000000713',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 262144,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 262144}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Grok'},\n",
      "  'canonical_slug': 'x-ai/grok-code-fast-1',\n",
      "  'context_length': 256000,\n",
      "  'created': 1756238927,\n",
      "  'description': 'Grok Code Fast 1 is a speedy and economical reasoning model '\n",
      "                 'that excels at agentic coding. With reasoning traces visible '\n",
      "                 'in the response, developers can steer Grok Code for '\n",
      "                 'high-quality work flows.',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'x-ai/grok-code-fast-1',\n",
      "  'name': 'xAI: Grok Code Fast 1',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000015',\n",
      "              'image': '0',\n",
      "              'input_cache_read': '0.00000002',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000002',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['include_reasoning',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'reasoning',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 256000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 10000}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'nousresearch/hermes-4-70b',\n",
      "  'context_length': 131072,\n",
      "  'created': 1756236182,\n",
      "  'description': 'Hermes 4 70B is a hybrid reasoning model from Nous Research, '\n",
      "                 'built on Meta-Llama-3.1-70B. It introduces the same hybrid '\n",
      "                 'mode as the larger 405B release, allowing the model to '\n",
      "                 'either respond directly or generate explicit '\n",
      "                 '<think>...</think> reasoning traces before answering. Users '\n",
      "                 'can control the reasoning behaviour with the `reasoning` '\n",
      "                 '`enabled` boolean. [Learn more in our '\n",
      "                 'docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\\n'\n",
      "                 '\\n'\n",
      "                 'This 70B variant is trained with the expanded post-training '\n",
      "                 'corpus (~60B tokens) emphasizing verified reasoning data, '\n",
      "                 'leading to improvements in mathematics, coding, STEM, logic, '\n",
      "                 'and structured outputs while maintaining general assistant '\n",
      "                 'performance. It supports JSON mode, schema adherence, '\n",
      "                 'function calling, and tool use, and is designed for greater '\n",
      "                 'steerability with reduced refusal rates.',\n",
      "  'hugging_face_id': 'NousResearch/Hermes-4-70B',\n",
      "  'id': 'nousresearch/hermes-4-70b',\n",
      "  'name': 'Nous: Hermes 4 70B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000003733632',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000009329544',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'nousresearch/hermes-4-405b',\n",
      "  'context_length': 131072,\n",
      "  'created': 1756235463,\n",
      "  'description': 'Hermes 4 is a large-scale reasoning model built on '\n",
      "                 'Meta-Llama-3.1-405B and released by Nous Research. It '\n",
      "                 'introduces a hybrid reasoning mode, where the model can '\n",
      "                 'choose to deliberate internally with <think>...</think> '\n",
      "                 'traces or respond directly, offering flexibility between '\n",
      "                 'speed and depth. Users can control the reasoning behaviour '\n",
      "                 'with the `reasoning` `enabled` boolean. [Learn more in our '\n",
      "                 'docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\\n'\n",
      "                 '\\n'\n",
      "                 'The model is instruction-tuned with an expanded '\n",
      "                 'post-training corpus (~60B tokens) emphasizing reasoning '\n",
      "                 'traces, improving performance in math, code, STEM, and '\n",
      "                 'logical reasoning, while retaining broad assistant utility. '\n",
      "                 'It also supports structured outputs, including JSON mode, '\n",
      "                 'schema adherence, function calling, and tool use. Hermes 4 '\n",
      "                 'is trained for steerability, lower refusal rates, and '\n",
      "                 'alignment toward neutral, user-directed behavior.',\n",
      "  'hugging_face_id': 'NousResearch/Hermes-4-405B',\n",
      "  'id': 'nousresearch/hermes-4-405b',\n",
      "  'name': 'Nous: Hermes 4 405B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-v3.1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'deepseek/deepseek-chat-v3.1',\n",
      "  'context_length': 163840,\n",
      "  'created': 1755779628,\n",
      "  'description': 'DeepSeek-V3.1 is a large hybrid reasoning model (671B '\n",
      "                 'parameters, 37B active) that supports both thinking and '\n",
      "                 'non-thinking modes via prompt templates. It extends the '\n",
      "                 'DeepSeek-V3 base with a two-phase long-context training '\n",
      "                 'process, reaching up to 128K tokens, and uses FP8 '\n",
      "                 'microscaling for efficient inference. Users can control the '\n",
      "                 'reasoning behaviour with the `reasoning` `enabled` boolean. '\n",
      "                 '[Learn more in our '\n",
      "                 'docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\\n'\n",
      "                 '\\n'\n",
      "                 'The model improves tool use, code generation, and reasoning '\n",
      "                 'efficiency, achieving performance comparable to DeepSeek-R1 '\n",
      "                 'on difficult benchmarks while responding more quickly. It '\n",
      "                 'supports structured tool calling, code agents, and search '\n",
      "                 'agents, making it suitable for research, coding, and agentic '\n",
      "                 'workflows. \\n'\n",
      "                 '\\n'\n",
      "                 'It succeeds the [DeepSeek '\n",
      "                 'V3-0324](/deepseek/deepseek-chat-v3-0324) model and performs '\n",
      "                 'well on a variety of tasks.',\n",
      "  'hugging_face_id': 'deepseek-ai/DeepSeek-V3.1',\n",
      "  'id': 'deepseek/deepseek-chat-v3.1',\n",
      "  'name': 'DeepSeek: DeepSeek V3.1',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000008',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000002',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'none',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'deepseek/deepseek-v3.1-base',\n",
      "  'context_length': 163840,\n",
      "  'created': 1755727017,\n",
      "  'description': 'This is a base model, trained only for raw next-token '\n",
      "                 'prediction. Unlike instruct/chat models, it has not been '\n",
      "                 'fine-tuned to follow user instructions. Prompts need to be '\n",
      "                 'written more like training text or examples rather than '\n",
      "                 'simple requests (e.g., “Translate the following sentence…” '\n",
      "                 'instead of just “Translate this”).\\n'\n",
      "                 '\\n'\n",
      "                 'DeepSeek-V3.1 Base is a 671B parameter open '\n",
      "                 'Mixture-of-Experts (MoE) language model with 37B active '\n",
      "                 'parameters per forward pass and a context length of 128K '\n",
      "                 'tokens. Trained on 14.8T tokens using FP8 mixed precision, '\n",
      "                 'it achieves high training efficiency and stability, with '\n",
      "                 'strong performance across language, reasoning, math, and '\n",
      "                 'coding tasks. \\n',\n",
      "  'hugging_face_id': 'deepseek-ai/DeepSeek-V3.1-Base',\n",
      "  'id': 'deepseek/deepseek-v3.1-base',\n",
      "  'name': 'DeepSeek: DeepSeek V3.1 Base',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000008',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000002',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['audio', 'text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4o-audio-preview',\n",
      "  'context_length': 128000,\n",
      "  'created': 1755233061,\n",
      "  'description': 'The gpt-4o-audio-preview model adds support for audio inputs '\n",
      "                 'as prompts. This enhancement allows the model to detect '\n",
      "                 'nuances within audio recordings and add depth to generated '\n",
      "                 'user experiences. Audio outputs are currently not supported. '\n",
      "                 'Audio tokens are priced at $40 per million input audio '\n",
      "                 'tokens.',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'openai/gpt-4o-audio-preview',\n",
      "  'name': 'OpenAI: GPT-4o Audio',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'audio': '0.00004',\n",
      "              'completion': '0.00001',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000025',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-oss-120b',\n",
      "  'context_length': 131000,\n",
      "  'created': 1754414231,\n",
      "  'description': 'gpt-oss-120b is an open-weight, 117B-parameter '\n",
      "                 'Mixture-of-Experts (MoE) language model from OpenAI designed '\n",
      "                 'for high-reasoning, agentic, and general-purpose production '\n",
      "                 'use cases. It activates 5.1B parameters per forward pass and '\n",
      "                 'is optimized to run on a single H100 GPU with native MXFP4 '\n",
      "                 'quantization. The model supports configurable reasoning '\n",
      "                 'depth, full chain-of-thought access, and native tool use, '\n",
      "                 'including function calling, browsing, and structured output '\n",
      "                 'generation.',\n",
      "  'hugging_face_id': 'openai/gpt-oss-120b',\n",
      "  'id': 'openai/gpt-oss-120b',\n",
      "  'name': 'OpenAI: gpt-oss-120b',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000028',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000000072',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 131000}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-oss-20b',\n",
      "  'context_length': 131000,\n",
      "  'created': 1754414229,\n",
      "  'description': 'gpt-oss-20b is an open-weight 21B parameter model released '\n",
      "                 'by OpenAI under the Apache 2.0 license. It uses a '\n",
      "                 'Mixture-of-Experts (MoE) architecture with 3.6B active '\n",
      "                 'parameters per forward pass, optimized for lower-latency '\n",
      "                 'inference and deployability on consumer or single-GPU '\n",
      "                 'hardware. The model is trained in OpenAI’s Harmony response '\n",
      "                 'format and supports reasoning level configuration, '\n",
      "                 'fine-tuning, and agentic capabilities including function '\n",
      "                 'calling, tool use, and structured outputs.',\n",
      "  'hugging_face_id': 'openai/gpt-oss-20b',\n",
      "  'id': 'openai/gpt-oss-20b',\n",
      "  'name': 'OpenAI: gpt-oss-20b',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000015',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000004',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 131000}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-coder-30b-a3b-instruct',\n",
      "  'context_length': 262144,\n",
      "  'created': 1753972379,\n",
      "  'description': 'Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter '\n",
      "                 'Mixture-of-Experts (MoE) model with 128 experts (8 active '\n",
      "                 'per forward pass), designed for advanced code generation, '\n",
      "                 'repository-scale understanding, and agentic tool use. Built '\n",
      "                 'on the Qwen3 architecture, it supports a native context '\n",
      "                 'length of 256K tokens (extendable to 1M with Yarn) and '\n",
      "                 'performs strongly in tasks involving function calls, browser '\n",
      "                 'use, and structured code completion.\\n'\n",
      "                 '\\n'\n",
      "                 'This model is optimized for instruction-following without '\n",
      "                 '“thinking mode”, and integrates well with OpenAI-compatible '\n",
      "                 'tool-use formats. ',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-Coder-30B-A3B-Instruct',\n",
      "  'id': 'qwen/qwen3-coder-30b-a3b-instruct',\n",
      "  'name': 'Qwen: Qwen3 Coder 30B A3B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000207424',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000000518308',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 262144,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-30b-a3b-instruct-2507',\n",
      "  'context_length': 262144,\n",
      "  'created': 1753806965,\n",
      "  'description': 'Qwen3-30B-A3B-Instruct-2507 is a 30.5B-parameter '\n",
      "                 'mixture-of-experts language model from Qwen, with 3.3B '\n",
      "                 'active parameters per inference. It operates in non-thinking '\n",
      "                 'mode and is designed for high-quality instruction following, '\n",
      "                 'multilingual understanding, and agentic tool use. '\n",
      "                 'Post-trained on instruction data, it demonstrates '\n",
      "                 'competitive performance across reasoning (AIME, ZebraLogic), '\n",
      "                 'coding (MultiPL-E, LiveCodeBench), and alignment (IFEval, '\n",
      "                 'WritingBench) benchmarks. It outperforms its non-instruct '\n",
      "                 'variant on subjective and open-ended tasks while retaining '\n",
      "                 'strong factual and coding performance.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-30B-A3B-Instruct-2507',\n",
      "  'id': 'qwen/qwen3-30b-a3b-instruct-2507',\n",
      "  'name': 'Qwen: Qwen3 30B A3B Instruct 2507',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000207424',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000000518308',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 262144,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'z-ai/glm-4.5',\n",
      "  'context_length': 131072,\n",
      "  'created': 1753471347,\n",
      "  'description': 'GLM-4.5 is our latest flagship foundation model, '\n",
      "                 'purpose-built for agent-based applications. It leverages a '\n",
      "                 'Mixture-of-Experts (MoE) architecture and supports a context '\n",
      "                 'length of up to 128k tokens. GLM-4.5 delivers significantly '\n",
      "                 'enhanced capabilities in reasoning, code generation, and '\n",
      "                 'agent alignment. It supports a hybrid inference mode with '\n",
      "                 'two options, a \"thinking mode\" designed for complex '\n",
      "                 'reasoning and tool use, and a \"non-thinking mode\" optimized '\n",
      "                 'for instant responses. Users can control the reasoning '\n",
      "                 'behaviour with the `reasoning` `enabled` boolean. [Learn '\n",
      "                 'more in our '\n",
      "                 'docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)',\n",
      "  'hugging_face_id': 'zai-org/GLM-4.5',\n",
      "  'id': 'z-ai/glm-4.5',\n",
      "  'name': 'Z.AI: GLM 4.5',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000013201056',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000032986602',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_a',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'z-ai/glm-4.5-air',\n",
      "  'context_length': 131072,\n",
      "  'created': 1753471258,\n",
      "  'description': 'GLM-4.5-Air is the lightweight variant of our latest '\n",
      "                 'flagship model family, also purpose-built for agent-centric '\n",
      "                 'applications. Like GLM-4.5, it adopts the Mixture-of-Experts '\n",
      "                 '(MoE) architecture but with a more compact parameter size. '\n",
      "                 'GLM-4.5-Air also supports hybrid inference modes, offering a '\n",
      "                 '\"thinking mode\" for advanced reasoning and tool use, and a '\n",
      "                 '\"non-thinking mode\" for real-time interaction. Users can '\n",
      "                 'control the reasoning behaviour with the `reasoning` '\n",
      "                 '`enabled` boolean. [Learn more in our '\n",
      "                 'docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)',\n",
      "  'hugging_face_id': 'zai-org/GLM-4.5-Air',\n",
      "  'id': 'z-ai/glm-4.5-air:free',\n",
      "  'name': 'Z.AI: GLM 4.5 Air (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'qwen3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-235b-a22b-thinking-2507',\n",
      "  'context_length': 262144,\n",
      "  'created': 1753449557,\n",
      "  'description': 'Qwen3-235B-A22B-Thinking-2507 is a high-performance, '\n",
      "                 'open-weight Mixture-of-Experts (MoE) language model '\n",
      "                 'optimized for complex reasoning tasks. It activates 22B of '\n",
      "                 'its 235B parameters per forward pass and natively supports '\n",
      "                 'up to 262,144 tokens of context. This \"thinking-only\" '\n",
      "                 'variant enhances structured logical reasoning, mathematics, '\n",
      "                 'science, and long-form generation, showing strong benchmark '\n",
      "                 'performance across AIME, SuperGPQA, LiveCodeBench, and '\n",
      "                 'MMLU-Redux. It enforces a special reasoning mode (</think>) '\n",
      "                 'and is designed for high-token outputs (up to 81,920 tokens) '\n",
      "                 'in challenging domains.\\n'\n",
      "                 '\\n'\n",
      "                 'The model is instruction-tuned and excels at step-by-step '\n",
      "                 'reasoning, tool use, agentic workflows, and multilingual '\n",
      "                 'tasks. This release represents the most capable open-source '\n",
      "                 'variant in the Qwen3-235B series, surpassing many closed '\n",
      "                 'models in structured reasoning use cases.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-235B-A22B-Thinking-2507',\n",
      "  'id': 'qwen/qwen3-235b-a22b-thinking-2507',\n",
      "  'name': 'Qwen: Qwen3 235B A22B Thinking 2507',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000031202496',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000000077968332',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 262144,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-coder-480b-a35b-07-25',\n",
      "  'context_length': 262144,\n",
      "  'created': 1753230546,\n",
      "  'description': 'Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) '\n",
      "                 'code generation model developed by the Qwen team. It is '\n",
      "                 'optimized for agentic coding tasks such as function calling, '\n",
      "                 'tool use, and long-context reasoning over repositories. The '\n",
      "                 'model features 480 billion total parameters, with 35 billion '\n",
      "                 'active per forward pass (8 out of 160 experts).\\n'\n",
      "                 '\\n'\n",
      "                 'Pricing for the Alibaba endpoints varies by context length. '\n",
      "                 'Once a request is greater than 128k input tokens, the higher '\n",
      "                 'pricing is used.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-Coder-480B-A35B-Instruct',\n",
      "  'id': 'qwen/qwen3-coder:free',\n",
      "  'name': 'Qwen: Qwen3 Coder 480B A35B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 262144,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-coder-480b-a35b-07-25',\n",
      "  'context_length': 262144,\n",
      "  'created': 1753230546,\n",
      "  'description': 'Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) '\n",
      "                 'code generation model developed by the Qwen team. It is '\n",
      "                 'optimized for agentic coding tasks such as function calling, '\n",
      "                 'tool use, and long-context reasoning over repositories. The '\n",
      "                 'model features 480 billion total parameters, with 35 billion '\n",
      "                 'active per forward pass (8 out of 160 experts).\\n'\n",
      "                 '\\n'\n",
      "                 'Pricing for the Alibaba endpoints varies by context length. '\n",
      "                 'Once a request is greater than 128k input tokens, the higher '\n",
      "                 'pricing is used.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-Coder-480B-A35B-Instruct',\n",
      "  'id': 'qwen/qwen3-coder',\n",
      "  'name': 'Qwen: Qwen3 Coder 480B A35B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000008',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000002',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 262144,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-235b-a22b-07-25',\n",
      "  'context_length': 262144,\n",
      "  'created': 1753119555,\n",
      "  'description': 'Qwen3-235B-A22B-Instruct-2507 is a multilingual, '\n",
      "                 'instruction-tuned mixture-of-experts language model based on '\n",
      "                 'the Qwen3-235B architecture, with 22B active parameters per '\n",
      "                 'forward pass. It is optimized for general-purpose text '\n",
      "                 'generation, including instruction following, logical '\n",
      "                 'reasoning, math, code, and tool usage. The model supports a '\n",
      "                 'native 262K context length and does not implement \"thinking '\n",
      "                 'mode\" (<think> blocks).\\n'\n",
      "                 '\\n'\n",
      "                 'Compared to its base variant, this version delivers '\n",
      "                 'significant gains in knowledge coverage, long-context '\n",
      "                 'reasoning, coding benchmarks, and alignment with open-ended '\n",
      "                 'tasks. It is particularly strong on multilingual '\n",
      "                 'understanding, math reasoning (e.g., AIME, HMMT), and '\n",
      "                 'alignment evaluations like Arena-Hard and WritingBench.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-235B-A22B-Instruct-2507',\n",
      "  'id': 'qwen/qwen3-235b-a22b-2507',\n",
      "  'name': 'Qwen: Qwen3 235B A22B Instruct 2507',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000031202496',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000000077968332',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 262144,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'moonshotai/kimi-k2',\n",
      "  'context_length': 32768,\n",
      "  'created': 1752263252,\n",
      "  'description': 'Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) '\n",
      "                 'language model developed by Moonshot AI, featuring 1 '\n",
      "                 'trillion total parameters with 32 billion active per forward '\n",
      "                 'pass. It is optimized for agentic capabilities, including '\n",
      "                 'advanced tool use, reasoning, and code synthesis. Kimi K2 '\n",
      "                 'excels across a broad range of benchmarks, particularly in '\n",
      "                 'coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, '\n",
      "                 'GPQA), and tool-use (Tau2, AceBench) tasks. It supports '\n",
      "                 'long-context inference up to 128K tokens and is designed '\n",
      "                 'with a novel training stack that includes the MuonClip '\n",
      "                 'optimizer for stable large-scale MoE training.',\n",
      "  'hugging_face_id': 'moonshotai/Kimi-K2-Instruct',\n",
      "  'id': 'moonshotai/kimi-k2:free',\n",
      "  'name': 'MoonshotAI: Kimi K2 0711 (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'moonshotai/kimi-k2',\n",
      "  'context_length': 63000,\n",
      "  'created': 1752263252,\n",
      "  'description': 'Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) '\n",
      "                 'language model developed by Moonshot AI, featuring 1 '\n",
      "                 'trillion total parameters with 32 billion active per forward '\n",
      "                 'pass. It is optimized for agentic capabilities, including '\n",
      "                 'advanced tool use, reasoning, and code synthesis. Kimi K2 '\n",
      "                 'excels across a broad range of benchmarks, particularly in '\n",
      "                 'coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, '\n",
      "                 'GPQA), and tool-use (Tau2, AceBench) tasks. It supports '\n",
      "                 'long-context inference up to 128K tokens and is designed '\n",
      "                 'with a novel training stack that includes the MuonClip '\n",
      "                 'optimizer for stable large-scale MoE training.',\n",
      "  'hugging_face_id': 'moonshotai/Kimi-K2-Instruct',\n",
      "  'id': 'moonshotai/kimi-k2',\n",
      "  'name': 'MoonshotAI: Kimi K2 0711',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000249',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000014',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 63000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 63000}},\n",
      " {'architecture': {'input_modalities': ['image', 'text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Grok'},\n",
      "  'canonical_slug': 'x-ai/grok-4-07-09',\n",
      "  'context_length': 256000,\n",
      "  'created': 1752087689,\n",
      "  'description': \"Grok 4 is xAI's latest reasoning model with a 256k context \"\n",
      "                 'window. It supports parallel tool calling, structured '\n",
      "                 'outputs, and both image and text inputs. Note that reasoning '\n",
      "                 'is not exposed, reasoning cannot be disabled, and the '\n",
      "                 'reasoning effort cannot be specified. Pricing increases once '\n",
      "                 'the total tokens in a given request is greater than 128k '\n",
      "                 'tokens. See more details on the [xAI '\n",
      "                 'docs](https://docs.x.ai/docs/models/grok-4-0709)',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'x-ai/grok-4',\n",
      "  'name': 'xAI: Grok 4',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000015',\n",
      "              'image': '0',\n",
      "              'input_cache_read': '0.00000075',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000003',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['include_reasoning',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'reasoning',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 256000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'tencent/hunyuan-a13b-instruct',\n",
      "  'context_length': 32768,\n",
      "  'created': 1751987664,\n",
      "  'description': 'Hunyuan-A13B is a 13B active parameter Mixture-of-Experts '\n",
      "                 '(MoE) language model developed by Tencent, with a total '\n",
      "                 'parameter count of 80B and support for reasoning via '\n",
      "                 'Chain-of-Thought. It offers competitive benchmark '\n",
      "                 'performance across mathematics, science, coding, and '\n",
      "                 'multi-turn reasoning tasks, while maintaining high inference '\n",
      "                 'efficiency via Grouped Query Attention (GQA) and '\n",
      "                 'quantization support (FP8, GPTQ, etc.).',\n",
      "  'hugging_face_id': 'tencent/Hunyuan-A13B-Instruct',\n",
      "  'id': 'tencent/hunyuan-a13b-instruct:free',\n",
      "  'name': 'Tencent: Hunyuan A13B Instruct (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'tencent/hunyuan-a13b-instruct',\n",
      "  'context_length': 32768,\n",
      "  'created': 1751987664,\n",
      "  'description': 'Hunyuan-A13B is a 13B active parameter Mixture-of-Experts '\n",
      "                 '(MoE) language model developed by Tencent, with a total '\n",
      "                 'parameter count of 80B and support for reasoning via '\n",
      "                 'Chain-of-Thought. It offers competitive benchmark '\n",
      "                 'performance across mathematics, science, coding, and '\n",
      "                 'multi-turn reasoning tasks, while maintaining high inference '\n",
      "                 'efficiency via Grouped Query Attention (GQA) and '\n",
      "                 'quantization support (FP8, GPTQ, etc.).',\n",
      "  'hugging_face_id': 'tencent/Hunyuan-A13B-Instruct',\n",
      "  'id': 'tencent/hunyuan-a13b-instruct',\n",
      "  'name': 'Tencent: Hunyuan A13B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000003',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000003',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'tngtech/deepseek-r1t2-chimera',\n",
      "  'context_length': 163840,\n",
      "  'created': 1751986985,\n",
      "  'description': 'DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera '\n",
      "                 'model from TNG Tech. It is a 671 B-parameter '\n",
      "                 'mixture-of-experts text-generation model assembled from '\n",
      "                 'DeepSeek-AI’s R1-0528, R1, and V3-0324 checkpoints with an '\n",
      "                 'Assembly-of-Experts merge. The tri-parent design yields '\n",
      "                 'strong reasoning performance while running roughly 20 % '\n",
      "                 'faster than the original R1 and more than 2× faster than '\n",
      "                 'R1-0528 under vLLM, giving a favorable cost-to-intelligence '\n",
      "                 'trade-off. The checkpoint supports contexts up to 60 k '\n",
      "                 'tokens in standard use (tested to ~130 k) and maintains '\n",
      "                 'consistent <think> token behaviour, making it suitable for '\n",
      "                 'long-context analysis, dialogue and other open-ended '\n",
      "                 'generation tasks.',\n",
      "  'hugging_face_id': 'tngtech/DeepSeek-TNG-R1T2-Chimera',\n",
      "  'id': 'tngtech/deepseek-r1t2-chimera:free',\n",
      "  'name': 'TNG: DeepSeek R1T2 Chimera (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['image', 'text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/mistral-small-3.2-24b-instruct-2506',\n",
      "  'context_length': 131072,\n",
      "  'created': 1750443016,\n",
      "  'description': 'Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B '\n",
      "                 'parameter model from Mistral optimized for instruction '\n",
      "                 'following, repetition reduction, and improved function '\n",
      "                 'calling. Compared to the 3.1 release, version 3.2 '\n",
      "                 'significantly improves accuracy on WildBench and Arena Hard, '\n",
      "                 'reduces infinite generations, and delivers gains in tool use '\n",
      "                 'and structured output tasks.\\n'\n",
      "                 '\\n'\n",
      "                 'It supports image and text inputs with structured outputs, '\n",
      "                 'function/tool calling, and strong performance across coding '\n",
      "                 '(HumanEval+, MBPP), STEM (MMLU, MATH, GPQA), and vision '\n",
      "                 'benchmarks (ChartQA, DocVQA).',\n",
      "  'hugging_face_id': 'mistralai/Mistral-Small-3.2-24B-Instruct-2506',\n",
      "  'id': 'mistralai/mistral-small-3.2-24b-instruct:free',\n",
      "  'name': 'Mistral: Mistral Small 3.2 24B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['image', 'text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/mistral-small-3.2-24b-instruct-2506',\n",
      "  'context_length': 128000,\n",
      "  'created': 1750443016,\n",
      "  'description': 'Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B '\n",
      "                 'parameter model from Mistral optimized for instruction '\n",
      "                 'following, repetition reduction, and improved function '\n",
      "                 'calling. Compared to the 3.1 release, version 3.2 '\n",
      "                 'significantly improves accuracy on WildBench and Arena Hard, '\n",
      "                 'reduces infinite generations, and delivers gains in tool use '\n",
      "                 'and structured output tasks.\\n'\n",
      "                 '\\n'\n",
      "                 'It supports image and text inputs with structured outputs, '\n",
      "                 'function/tool calling, and strong performance across coding '\n",
      "                 '(HumanEval+, MBPP), STEM (MMLU, MATH, GPQA), and vision '\n",
      "                 'benchmarks (ChartQA, DocVQA).',\n",
      "  'hugging_face_id': 'mistralai/Mistral-Small-3.2-24B-Instruct-2506',\n",
      "  'id': 'mistralai/mistral-small-3.2-24b-instruct',\n",
      "  'name': 'Mistral: Mistral Small 3.2 24B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000001',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000005',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'moonshotai/kimi-dev-72b',\n",
      "  'context_length': 131072,\n",
      "  'created': 1750115909,\n",
      "  'description': 'Kimi-Dev-72B is an open-source large language model '\n",
      "                 'fine-tuned for software engineering and issue resolution '\n",
      "                 'tasks. Based on Qwen2.5-72B, it is optimized using '\n",
      "                 'large-scale reinforcement learning that applies code patches '\n",
      "                 'in real repositories and validates them via full test suite '\n",
      "                 'execution—rewarding only correct, robust completions. The '\n",
      "                 'model achieves 60.4% on SWE-bench Verified, setting a new '\n",
      "                 'benchmark among open-source models for software bug fixing '\n",
      "                 'and code reasoning.',\n",
      "  'hugging_face_id': 'moonshotai/Kimi-Dev-72B',\n",
      "  'id': 'moonshotai/kimi-dev-72b:free',\n",
      "  'name': 'MoonshotAI: Kimi Dev 72B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Grok'},\n",
      "  'canonical_slug': 'x-ai/grok-3-mini',\n",
      "  'context_length': 131072,\n",
      "  'created': 1749583245,\n",
      "  'description': 'A lightweight model that thinks before responding. Fast, '\n",
      "                 'smart, and great for logic-based tasks that do not require '\n",
      "                 'deep domain knowledge. The raw thinking traces are '\n",
      "                 'accessible.',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'x-ai/grok-3-mini',\n",
      "  'name': 'xAI: Grok 3 Mini',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000005',\n",
      "              'image': '0',\n",
      "              'input_cache_read': '0.000000075',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000003',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['include_reasoning',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'reasoning',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Grok'},\n",
      "  'canonical_slug': 'x-ai/grok-3',\n",
      "  'context_length': 131072,\n",
      "  'created': 1749582908,\n",
      "  'description': \"Grok 3 is the latest model from xAI. It's their flagship \"\n",
      "                 'model that excels at enterprise use cases like data '\n",
      "                 'extraction, coding, and text summarization. Possesses deep '\n",
      "                 'domain knowledge in finance, healthcare, law, and science.\\n'\n",
      "                 '\\n',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'x-ai/grok-3',\n",
      "  'name': 'xAI: Grok 3',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000015',\n",
      "              'image': '0',\n",
      "              'input_cache_read': '0.00000075',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000003',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'deepseek/deepseek-r1-0528-qwen3-8b',\n",
      "  'context_length': 131072,\n",
      "  'created': 1748538543,\n",
      "  'description': 'DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek '\n",
      "                 'R1 that taps more compute and smarter post-training tricks, '\n",
      "                 'pushing its reasoning and inference to the brink of flagship '\n",
      "                 'models like O3 and Gemini 2.5 Pro.\\n'\n",
      "                 'It now tops math, programming, and logic leaderboards, '\n",
      "                 'showcasing a step-change in depth-of-thought.\\n'\n",
      "                 'The distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers '\n",
      "                 'this chain-of-thought into an 8 B-parameter form, beating '\n",
      "                 'standard Qwen3 8B by +10 pp and tying the 235 B “thinking” '\n",
      "                 'giant on AIME 2024.',\n",
      "  'hugging_face_id': 'deepseek-ai/deepseek-r1-0528-qwen3-8b',\n",
      "  'id': 'deepseek/deepseek-r1-0528-qwen3-8b:free',\n",
      "  'name': 'DeepSeek: Deepseek R1 0528 Qwen3 8B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'deepseek/deepseek-r1-0528-qwen3-8b',\n",
      "  'context_length': 131072,\n",
      "  'created': 1748538543,\n",
      "  'description': 'DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek '\n",
      "                 'R1 that taps more compute and smarter post-training tricks, '\n",
      "                 'pushing its reasoning and inference to the brink of flagship '\n",
      "                 'models like O3 and Gemini 2.5 Pro.\\n'\n",
      "                 'It now tops math, programming, and logic leaderboards, '\n",
      "                 'showcasing a step-change in depth-of-thought.\\n'\n",
      "                 'The distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers '\n",
      "                 'this chain-of-thought into an 8 B-parameter form, beating '\n",
      "                 'standard Qwen3 8B by +10 pp and tying the 235 B “thinking” '\n",
      "                 'giant on AIME 2024.',\n",
      "  'hugging_face_id': 'deepseek-ai/deepseek-r1-0528-qwen3-8b',\n",
      "  'id': 'deepseek/deepseek-r1-0528-qwen3-8b',\n",
      "  'name': 'DeepSeek: Deepseek R1 0528 Qwen3 8B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000681536',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001703012',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'deepseek/deepseek-r1-0528',\n",
      "  'context_length': 163840,\n",
      "  'created': 1748455170,\n",
      "  'description': 'May 28th update to the [original DeepSeek '\n",
      "                 'R1](/deepseek/deepseek-r1) Performance on par with [OpenAI '\n",
      "                 'o1](/openai/o1), but open-sourced and with fully open '\n",
      "                 \"reasoning tokens. It's 671B parameters in size, with 37B \"\n",
      "                 'active in an inference pass.\\n'\n",
      "                 '\\n'\n",
      "                 'Fully open-source model.',\n",
      "  'hugging_face_id': 'deepseek-ai/DeepSeek-R1-0528',\n",
      "  'id': 'deepseek/deepseek-r1-0528:free',\n",
      "  'name': 'DeepSeek: R1 0528 (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'deepseek/deepseek-r1-0528',\n",
      "  'context_length': 163840,\n",
      "  'created': 1748455170,\n",
      "  'description': 'May 28th update to the [original DeepSeek '\n",
      "                 'R1](/deepseek/deepseek-r1) Performance on par with [OpenAI '\n",
      "                 'o1](/openai/o1), but open-sourced and with fully open '\n",
      "                 \"reasoning tokens. It's 671B parameters in size, with 37B \"\n",
      "                 'active in an inference pass.\\n'\n",
      "                 '\\n'\n",
      "                 'Fully open-source model.',\n",
      "  'hugging_face_id': 'deepseek-ai/DeepSeek-R1-0528',\n",
      "  'id': 'deepseek/deepseek-r1-0528',\n",
      "  'name': 'DeepSeek: R1 0528',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/devstral-small-2505',\n",
      "  'context_length': 32768,\n",
      "  'created': 1747837379,\n",
      "  'description': 'Devstral-Small-2505 is a 24B parameter agentic LLM '\n",
      "                 'fine-tuned from Mistral-Small-3.1, jointly developed by '\n",
      "                 'Mistral AI and All Hands AI for advanced software '\n",
      "                 'engineering tasks. It is optimized for codebase exploration, '\n",
      "                 'multi-file editing, and integration into coding agents, '\n",
      "                 'achieving state-of-the-art results on SWE-Bench Verified '\n",
      "                 '(46.8%).\\n'\n",
      "                 '\\n'\n",
      "                 'Devstral supports a 128k context window and uses a custom '\n",
      "                 'Tekken tokenizer. It is text-only, with the vision encoder '\n",
      "                 'removed, and is suitable for local deployment on high-end '\n",
      "                 'consumer hardware (e.g., RTX 4090, 32GB RAM Macs). Devstral '\n",
      "                 'is best used in agentic workflows via the OpenHands scaffold '\n",
      "                 'and is compatible with inference frameworks like vLLM, '\n",
      "                 'Transformers, and Ollama. It is released under the Apache '\n",
      "                 '2.0 license.',\n",
      "  'hugging_face_id': 'mistralai/Devstral-Small-2505',\n",
      "  'id': 'mistralai/devstral-small-2505:free',\n",
      "  'name': 'Mistral: Devstral Small 2505 (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/devstral-small-2505',\n",
      "  'context_length': 131072,\n",
      "  'created': 1747837379,\n",
      "  'description': 'Devstral-Small-2505 is a 24B parameter agentic LLM '\n",
      "                 'fine-tuned from Mistral-Small-3.1, jointly developed by '\n",
      "                 'Mistral AI and All Hands AI for advanced software '\n",
      "                 'engineering tasks. It is optimized for codebase exploration, '\n",
      "                 'multi-file editing, and integration into coding agents, '\n",
      "                 'achieving state-of-the-art results on SWE-Bench Verified '\n",
      "                 '(46.8%).\\n'\n",
      "                 '\\n'\n",
      "                 'Devstral supports a 128k context window and uses a custom '\n",
      "                 'Tekken tokenizer. It is text-only, with the vision encoder '\n",
      "                 'removed, and is suitable for local deployment on high-end '\n",
      "                 'consumer hardware (e.g., RTX 4090, 32GB RAM Macs). Devstral '\n",
      "                 'is best used in agentic workflows via the OpenHands scaffold '\n",
      "                 'and is compatible with inference frameworks like vLLM, '\n",
      "                 'Transformers, and Ollama. It is released under the Apache '\n",
      "                 '2.0 license.',\n",
      "  'hugging_face_id': 'mistralai/Devstral-Small-2505',\n",
      "  'id': 'mistralai/devstral-small-2505',\n",
      "  'name': 'Mistral: Devstral Small 2505',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'nousresearch/deephermes-3-mistral-24b-preview',\n",
      "  'context_length': 32768,\n",
      "  'created': 1746830904,\n",
      "  'description': 'DeepHermes 3 (Mistral 24B Preview) is an instruction-tuned '\n",
      "                 'language model by Nous Research based on Mistral-Small-24B, '\n",
      "                 'designed for chat, function calling, and advanced multi-turn '\n",
      "                 'reasoning. It introduces a dual-mode system that toggles '\n",
      "                 'between intuitive chat responses and structured “deep '\n",
      "                 'reasoning” mode using special system prompts. Fine-tuned via '\n",
      "                 'distillation from R1, it supports structured output (JSON '\n",
      "                 'mode) and function call syntax for agent-based '\n",
      "                 'applications.\\n'\n",
      "                 '\\n'\n",
      "                 'DeepHermes 3 supports a **reasoning toggle via system '\n",
      "                 'prompt**, allowing users to switch between fast, intuitive '\n",
      "                 'responses and deliberate, multi-step reasoning. When '\n",
      "                 'activated with the following specific system instruction, '\n",
      "                 'the model enters a *\"deep thinking\"* mode—generating '\n",
      "                 'extended chains of thought wrapped in `<think></think>` tags '\n",
      "                 'before delivering a final answer. \\n'\n",
      "                 '\\n'\n",
      "                 'System Prompt: You are a deep thinking AI, you may use '\n",
      "                 'extremely long chains of thought to deeply consider the '\n",
      "                 'problem and deliberate with yourself via systematic '\n",
      "                 'reasoning processes to help come to a correct solution prior '\n",
      "                 'to answering. You should enclose your thoughts and internal '\n",
      "                 'monologue inside <think> </think> tags, and then provide '\n",
      "                 'your solution or response to the problem.\\n',\n",
      "  'hugging_face_id': 'NousResearch/DeepHermes-3-Mistral-24B-Preview',\n",
      "  'id': 'nousresearch/deephermes-3-mistral-24b-preview',\n",
      "  'name': 'Nous: DeepHermes 3 Mistral 24B Preview',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000003733632',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000009329544',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'qwen3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-30b-a3b-04-28',\n",
      "  'context_length': 40960,\n",
      "  'created': 1745878604,\n",
      "  'description': 'Qwen3, the latest generation in the Qwen large language '\n",
      "                 'model series, features both dense and mixture-of-experts '\n",
      "                 '(MoE) architectures to excel in reasoning, multilingual '\n",
      "                 'support, and advanced agent tasks. Its unique ability to '\n",
      "                 'switch seamlessly between a thinking mode for complex '\n",
      "                 'reasoning and a non-thinking mode for efficient dialogue '\n",
      "                 'ensures versatile, high-quality performance.\\n'\n",
      "                 '\\n'\n",
      "                 'Significantly outperforming prior models like QwQ and '\n",
      "                 'Qwen2.5, Qwen3 delivers superior mathematics, coding, '\n",
      "                 'commonsense reasoning, creative writing, and interactive '\n",
      "                 'dialogue capabilities. The Qwen3-30B-A3B variant includes '\n",
      "                 '30.5 billion parameters (3.3 billion activated), 48 layers, '\n",
      "                 '128 experts (8 activated per task), and supports up to 131K '\n",
      "                 'token contexts with YaRN, setting a new standard among '\n",
      "                 'open-source models.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-30B-A3B',\n",
      "  'id': 'qwen/qwen3-30b-a3b:free',\n",
      "  'name': 'Qwen: Qwen3 30B A3B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 40960,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'qwen3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-30b-a3b-04-28',\n",
      "  'context_length': 40960,\n",
      "  'created': 1745878604,\n",
      "  'description': 'Qwen3, the latest generation in the Qwen large language '\n",
      "                 'model series, features both dense and mixture-of-experts '\n",
      "                 '(MoE) architectures to excel in reasoning, multilingual '\n",
      "                 'support, and advanced agent tasks. Its unique ability to '\n",
      "                 'switch seamlessly between a thinking mode for complex '\n",
      "                 'reasoning and a non-thinking mode for efficient dialogue '\n",
      "                 'ensures versatile, high-quality performance.\\n'\n",
      "                 '\\n'\n",
      "                 'Significantly outperforming prior models like QwQ and '\n",
      "                 'Qwen2.5, Qwen3 delivers superior mathematics, coding, '\n",
      "                 'commonsense reasoning, creative writing, and interactive '\n",
      "                 'dialogue capabilities. The Qwen3-30B-A3B variant includes '\n",
      "                 '30.5 billion parameters (3.3 billion activated), 48 layers, '\n",
      "                 '128 experts (8 activated per task), and supports up to 131K '\n",
      "                 'token contexts with YaRN, setting a new standard among '\n",
      "                 'open-source models.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-30B-A3B',\n",
      "  'id': 'qwen/qwen3-30b-a3b',\n",
      "  'name': 'Qwen: Qwen3 30B A3B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 40960,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'qwen3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-8b-04-28',\n",
      "  'context_length': 40960,\n",
      "  'created': 1745876632,\n",
      "  'description': 'Qwen3-8B is a dense 8.2B parameter causal language model '\n",
      "                 'from the Qwen3 series, designed for both reasoning-heavy '\n",
      "                 'tasks and efficient dialogue. It supports seamless switching '\n",
      "                 'between \"thinking\" mode for math, coding, and logical '\n",
      "                 'inference, and \"non-thinking\" mode for general conversation. '\n",
      "                 'The model is fine-tuned for instruction-following, agent '\n",
      "                 'integration, creative writing, and multilingual use across '\n",
      "                 '100+ languages and dialects. It natively supports a 32K '\n",
      "                 'token context window and can extend to 131K tokens with YaRN '\n",
      "                 'scaling.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-8B',\n",
      "  'id': 'qwen/qwen3-8b:free',\n",
      "  'name': 'Qwen: Qwen3 8B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 40960,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 40960}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'qwen3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-14b-04-28',\n",
      "  'context_length': 40960,\n",
      "  'created': 1745876478,\n",
      "  'description': 'Qwen3-14B is a dense 14.8B parameter causal language model '\n",
      "                 'from the Qwen3 series, designed for both complex reasoning '\n",
      "                 'and efficient dialogue. It supports seamless switching '\n",
      "                 'between a \"thinking\" mode for tasks like math, programming, '\n",
      "                 'and logical inference, and a \"non-thinking\" mode for '\n",
      "                 'general-purpose conversation. The model is fine-tuned for '\n",
      "                 'instruction-following, agent tool use, creative writing, and '\n",
      "                 'multilingual tasks across 100+ languages and dialects. It '\n",
      "                 'natively handles 32K token contexts and can extend to 131K '\n",
      "                 'tokens using YaRN-based scaling.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-14B',\n",
      "  'id': 'qwen/qwen3-14b:free',\n",
      "  'name': 'Qwen: Qwen3 14B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 40960,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'qwen3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-14b-04-28',\n",
      "  'context_length': 40960,\n",
      "  'created': 1745876478,\n",
      "  'description': 'Qwen3-14B is a dense 14.8B parameter causal language model '\n",
      "                 'from the Qwen3 series, designed for both complex reasoning '\n",
      "                 'and efficient dialogue. It supports seamless switching '\n",
      "                 'between a \"thinking\" mode for tasks like math, programming, '\n",
      "                 'and logical inference, and a \"non-thinking\" mode for '\n",
      "                 'general-purpose conversation. The model is fine-tuned for '\n",
      "                 'instruction-following, agent tool use, creative writing, and '\n",
      "                 'multilingual tasks across 100+ languages and dialects. It '\n",
      "                 'natively handles 32K token contexts and can extend to 131K '\n",
      "                 'tokens using YaRN-based scaling.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-14B',\n",
      "  'id': 'qwen/qwen3-14b',\n",
      "  'name': 'Qwen: Qwen3 14B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000024',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000006',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 40960,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 40960}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'qwen3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-32b-04-28',\n",
      "  'context_length': 40960,\n",
      "  'created': 1745875945,\n",
      "  'description': 'Qwen3-32B is a dense 32.8B parameter causal language model '\n",
      "                 'from the Qwen3 series, optimized for both complex reasoning '\n",
      "                 'and efficient dialogue. It supports seamless switching '\n",
      "                 'between a \"thinking\" mode for tasks like math, coding, and '\n",
      "                 'logical inference, and a \"non-thinking\" mode for faster, '\n",
      "                 'general-purpose conversation. The model demonstrates strong '\n",
      "                 'performance in instruction-following, agent tool use, '\n",
      "                 'creative writing, and multilingual tasks across 100+ '\n",
      "                 'languages and dialects. It natively handles 32K token '\n",
      "                 'contexts and can extend to 131K tokens using YaRN-based '\n",
      "                 'scaling. ',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-32B',\n",
      "  'id': 'qwen/qwen3-32b',\n",
      "  'name': 'Qwen: Qwen3 32B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000007200576',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000000017992692',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 40960,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'qwen3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-235b-a22b-04-28',\n",
      "  'context_length': 131072,\n",
      "  'created': 1745875757,\n",
      "  'description': 'Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) '\n",
      "                 'model developed by Qwen, activating 22B parameters per '\n",
      "                 'forward pass. It supports seamless switching between a '\n",
      "                 '\"thinking\" mode for complex reasoning, math, and code tasks, '\n",
      "                 'and a \"non-thinking\" mode for general conversational '\n",
      "                 'efficiency. The model demonstrates strong reasoning ability, '\n",
      "                 'multilingual support (100+ languages and dialects), advanced '\n",
      "                 'instruction-following, and agent tool-calling capabilities. '\n",
      "                 'It natively handles a 32K token context window and extends '\n",
      "                 'up to 131K tokens using YaRN-based scaling.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-235B-A22B',\n",
      "  'id': 'qwen/qwen3-235b-a22b:free',\n",
      "  'name': 'Qwen: Qwen3 235B A22B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'qwen3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen3'},\n",
      "  'canonical_slug': 'qwen/qwen3-235b-a22b-04-28',\n",
      "  'context_length': 40960,\n",
      "  'created': 1745875757,\n",
      "  'description': 'Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) '\n",
      "                 'model developed by Qwen, activating 22B parameters per '\n",
      "                 'forward pass. It supports seamless switching between a '\n",
      "                 '\"thinking\" mode for complex reasoning, math, and code tasks, '\n",
      "                 'and a \"non-thinking\" mode for general conversational '\n",
      "                 'efficiency. The model demonstrates strong reasoning ability, '\n",
      "                 'multilingual support (100+ languages and dialects), advanced '\n",
      "                 'instruction-following, and agent tool-calling capabilities. '\n",
      "                 'It natively handles a 32K token context window and extends '\n",
      "                 'up to 131K tokens using YaRN-based scaling.',\n",
      "  'hugging_face_id': 'Qwen/Qwen3-235B-A22B',\n",
      "  'id': 'qwen/qwen3-235b-a22b',\n",
      "  'name': 'Qwen: Qwen3 235B A22B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000006',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000013',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 40960,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 40960}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'tngtech/deepseek-r1t-chimera',\n",
      "  'context_length': 163840,\n",
      "  'created': 1745760875,\n",
      "  'description': 'DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and '\n",
      "                 'DeepSeek-V3 (0324), combining the reasoning capabilities of '\n",
      "                 'R1 with the token efficiency improvements of V3. It is based '\n",
      "                 'on a DeepSeek-MoE Transformer architecture and is optimized '\n",
      "                 'for general text generation tasks.\\n'\n",
      "                 '\\n'\n",
      "                 'The model merges pretrained weights from both source models '\n",
      "                 'to balance performance across reasoning, efficiency, and '\n",
      "                 'instruction-following tasks. It is released under the MIT '\n",
      "                 'license and intended for research and commercial use.',\n",
      "  'hugging_face_id': 'tngtech/DeepSeek-R1T-Chimera',\n",
      "  'id': 'tngtech/deepseek-r1t-chimera:free',\n",
      "  'name': 'TNG: DeepSeek R1T Chimera (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'tngtech/deepseek-r1t-chimera',\n",
      "  'context_length': 163840,\n",
      "  'created': 1745760875,\n",
      "  'description': 'DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and '\n",
      "                 'DeepSeek-V3 (0324), combining the reasoning capabilities of '\n",
      "                 'R1 with the token efficiency improvements of V3. It is based '\n",
      "                 'on a DeepSeek-MoE Transformer architecture and is optimized '\n",
      "                 'for general text generation tasks.\\n'\n",
      "                 '\\n'\n",
      "                 'The model merges pretrained weights from both source models '\n",
      "                 'to balance performance across reasoning, efficiency, and '\n",
      "                 'instruction-following tasks. It is released under the MIT '\n",
      "                 'license and intended for research and commercial use.',\n",
      "  'hugging_face_id': 'tngtech/DeepSeek-R1T-Chimera',\n",
      "  'id': 'tngtech/deepseek-r1t-chimera',\n",
      "  'name': 'TNG: DeepSeek R1T Chimera',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'microsoft/mai-ds-r1',\n",
      "  'context_length': 163840,\n",
      "  'created': 1745194100,\n",
      "  'description': 'MAI-DS-R1 is a post-trained variant of DeepSeek-R1 developed '\n",
      "                 'by the Microsoft AI team to improve the model’s '\n",
      "                 'responsiveness on previously blocked topics while enhancing '\n",
      "                 'its safety profile. Built on top of DeepSeek-R1’s reasoning '\n",
      "                 'foundation, it integrates 110k examples from the Tulu-3 SFT '\n",
      "                 'dataset and 350k internally curated multilingual '\n",
      "                 'safety-alignment samples. The model retains strong '\n",
      "                 'reasoning, coding, and problem-solving capabilities, while '\n",
      "                 'unblocking a wide range of prompts previously restricted in '\n",
      "                 'R1.\\n'\n",
      "                 '\\n'\n",
      "                 'MAI-DS-R1 demonstrates improved performance on harm '\n",
      "                 'mitigation benchmarks and maintains competitive results '\n",
      "                 'across general reasoning tasks. It surpasses R1-1776 in '\n",
      "                 'satisfaction metrics for blocked queries and reduces leakage '\n",
      "                 'in harmful content categories. The model is based on a '\n",
      "                 'transformer MoE architecture and is suitable for '\n",
      "                 'general-purpose use cases, excluding high-stakes domains '\n",
      "                 'such as legal, medical, or autonomous systems.',\n",
      "  'hugging_face_id': 'microsoft/MAI-DS-R1',\n",
      "  'id': 'microsoft/mai-ds-r1:free',\n",
      "  'name': 'Microsoft: MAI DS R1 (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'microsoft/mai-ds-r1',\n",
      "  'context_length': 163840,\n",
      "  'created': 1745194100,\n",
      "  'description': 'MAI-DS-R1 is a post-trained variant of DeepSeek-R1 developed '\n",
      "                 'by the Microsoft AI team to improve the model’s '\n",
      "                 'responsiveness on previously blocked topics while enhancing '\n",
      "                 'its safety profile. Built on top of DeepSeek-R1’s reasoning '\n",
      "                 'foundation, it integrates 110k examples from the Tulu-3 SFT '\n",
      "                 'dataset and 350k internally curated multilingual '\n",
      "                 'safety-alignment samples. The model retains strong '\n",
      "                 'reasoning, coding, and problem-solving capabilities, while '\n",
      "                 'unblocking a wide range of prompts previously restricted in '\n",
      "                 'R1.\\n'\n",
      "                 '\\n'\n",
      "                 'MAI-DS-R1 demonstrates improved performance on harm '\n",
      "                 'mitigation benchmarks and maintains competitive results '\n",
      "                 'across general reasoning tasks. It surpasses R1-1776 in '\n",
      "                 'satisfaction metrics for blocked queries and reduces leakage '\n",
      "                 'in harmful content categories. The model is based on a '\n",
      "                 'transformer MoE architecture and is suitable for '\n",
      "                 'general-purpose use cases, excluding high-stakes domains '\n",
      "                 'such as legal, medical, or autonomous systems.',\n",
      "  'hugging_face_id': 'microsoft/MAI-DS-R1',\n",
      "  'id': 'microsoft/mai-ds-r1',\n",
      "  'name': 'Microsoft: MAI DS R1',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'thudm/glm-z1-32b-0414',\n",
      "  'context_length': 32768,\n",
      "  'created': 1744924148,\n",
      "  'description': 'GLM-Z1-32B-0414 is an enhanced reasoning variant of '\n",
      "                 'GLM-4-32B, built for deep mathematical, logical, and '\n",
      "                 'code-oriented problem solving. It applies extended '\n",
      "                 'reinforcement learning—both task-specific and general '\n",
      "                 'pairwise preference-based—to improve performance on complex '\n",
      "                 'multi-step tasks. Compared to the base GLM-4-32B model, Z1 '\n",
      "                 'significantly boosts capabilities in structured reasoning '\n",
      "                 'and formal domains.\\n'\n",
      "                 '\\n'\n",
      "                 'The model supports enforced “thinking” steps via prompt '\n",
      "                 'engineering and offers improved coherence for long-form '\n",
      "                 'outputs. It’s optimized for use in agentic workflows, and '\n",
      "                 'includes support for long context (via YaRN), JSON tool '\n",
      "                 'calling, and fine-grained sampling configuration for stable '\n",
      "                 'inference. Ideal for use cases requiring deliberate, '\n",
      "                 'multi-step reasoning or formal derivations.',\n",
      "  'hugging_face_id': 'THUDM/GLM-Z1-32B-0414',\n",
      "  'id': 'thudm/glm-z1-32b',\n",
      "  'name': 'THUDM: GLM Z1 32B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'shisa-ai/shisa-v2-llama3.3-70b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1744754858,\n",
      "  'description': 'Shisa V2 Llama 3.3 70B is a bilingual Japanese-English chat '\n",
      "                 'model fine-tuned by Shisa.AI on Meta’s '\n",
      "                 'Llama-3.3-70B-Instruct base. It prioritizes Japanese '\n",
      "                 'language performance while retaining strong English '\n",
      "                 'capabilities. The model was optimized entirely through '\n",
      "                 'post-training, using a refined mix of supervised fine-tuning '\n",
      "                 '(SFT) and DPO datasets including regenerated ShareGPT-style '\n",
      "                 'data, translation tasks, roleplaying conversations, and '\n",
      "                 'instruction-following prompts. Unlike earlier Shisa '\n",
      "                 'releases, this version avoids tokenizer modifications or '\n",
      "                 'extended pretraining.\\n'\n",
      "                 '\\n'\n",
      "                 'Shisa V2 70B achieves leading Japanese task performance '\n",
      "                 'across a wide range of custom and public benchmarks, '\n",
      "                 'including JA MT Bench, ELYZA 100, and Rakuda. It supports a '\n",
      "                 '128K token context length and integrates smoothly with '\n",
      "                 'inference frameworks like vLLM and SGLang. While it inherits '\n",
      "                 'safety characteristics from its base model, no additional '\n",
      "                 'alignment was applied. The model is intended for '\n",
      "                 'high-performance bilingual chat, instruction following, and '\n",
      "                 'translation tasks across JA/EN.',\n",
      "  'hugging_face_id': 'shisa-ai/shisa-v2-llama3.3-70b',\n",
      "  'id': 'shisa-ai/shisa-v2-llama3.3-70b:free',\n",
      "  'name': 'Shisa AI: Shisa V2 Llama 3.3 70B  (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'shisa-ai/shisa-v2-llama3.3-70b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1744754858,\n",
      "  'description': 'Shisa V2 Llama 3.3 70B is a bilingual Japanese-English chat '\n",
      "                 'model fine-tuned by Shisa.AI on Meta’s '\n",
      "                 'Llama-3.3-70B-Instruct base. It prioritizes Japanese '\n",
      "                 'language performance while retaining strong English '\n",
      "                 'capabilities. The model was optimized entirely through '\n",
      "                 'post-training, using a refined mix of supervised fine-tuning '\n",
      "                 '(SFT) and DPO datasets including regenerated ShareGPT-style '\n",
      "                 'data, translation tasks, roleplaying conversations, and '\n",
      "                 'instruction-following prompts. Unlike earlier Shisa '\n",
      "                 'releases, this version avoids tokenizer modifications or '\n",
      "                 'extended pretraining.\\n'\n",
      "                 '\\n'\n",
      "                 'Shisa V2 70B achieves leading Japanese task performance '\n",
      "                 'across a wide range of custom and public benchmarks, '\n",
      "                 'including JA MT Bench, ELYZA 100, and Rakuda. It supports a '\n",
      "                 '128K token context length and integrates smoothly with '\n",
      "                 'inference frameworks like vLLM and SGLang. While it inherits '\n",
      "                 'safety characteristics from its base model, no additional '\n",
      "                 'alignment was applied. The model is intended for '\n",
      "                 'high-performance bilingual chat, instruction following, and '\n",
      "                 'translation tasks across JA/EN.',\n",
      "  'hugging_face_id': 'shisa-ai/shisa-v2-llama3.3-70b',\n",
      "  'id': 'shisa-ai/shisa-v2-llama3.3-70b',\n",
      "  'name': 'Shisa AI: Shisa V2 Llama 3.3 70B ',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['image', 'text', 'file'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4.1-2025-04-14',\n",
      "  'context_length': 1047576,\n",
      "  'created': 1744651385,\n",
      "  'description': 'GPT-4.1 is a flagship large language model optimized for '\n",
      "                 'advanced instruction following, real-world software '\n",
      "                 'engineering, and long-context reasoning. It supports a 1 '\n",
      "                 'million token context window and outperforms GPT-4o and '\n",
      "                 'GPT-4.5 across coding (54.6% SWE-bench Verified), '\n",
      "                 'instruction compliance (87.4% IFEval), and multimodal '\n",
      "                 'understanding benchmarks. It is tuned for precise code '\n",
      "                 'diffs, agent reliability, and high recall in large document '\n",
      "                 'contexts, making it ideal for agents, IDE tooling, and '\n",
      "                 'enterprise knowledge retrieval.',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'openai/gpt-4.1',\n",
      "  'name': 'OpenAI: GPT-4.1',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000008',\n",
      "              'image': '0',\n",
      "              'input_cache_read': '0.0000005',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000002',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p',\n",
      "                           'web_search_options'],\n",
      "  'top_provider': {'context_length': 1047576,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 32768}},\n",
      " {'architecture': {'input_modalities': ['image', 'text', 'file'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4.1-mini-2025-04-14',\n",
      "  'context_length': 1047576,\n",
      "  'created': 1744651381,\n",
      "  'description': 'GPT-4.1 Mini is a mid-sized model delivering performance '\n",
      "                 'competitive with GPT-4o at substantially lower latency and '\n",
      "                 'cost. It retains a 1 million token context window and scores '\n",
      "                 '45.1% on hard instruction evals, 35.8% on MultiChallenge, '\n",
      "                 'and 84.1% on IFEval. Mini also shows strong coding ability '\n",
      "                 '(e.g., 31.6% on Aider’s polyglot diff benchmark) and vision '\n",
      "                 'understanding, making it suitable for interactive '\n",
      "                 'applications with tight performance constraints.',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'openai/gpt-4.1-mini',\n",
      "  'name': 'OpenAI: GPT-4.1 Mini',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000016',\n",
      "              'image': '0',\n",
      "              'input_cache_read': '0.0000001',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000004',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p',\n",
      "                           'web_search_options'],\n",
      "  'top_provider': {'context_length': 1047576,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 32768}},\n",
      " {'architecture': {'input_modalities': ['image', 'text', 'file'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4.1-nano-2025-04-14',\n",
      "  'context_length': 1047576,\n",
      "  'created': 1744651369,\n",
      "  'description': 'For tasks that demand low latency, GPT‑4.1 nano is the '\n",
      "                 'fastest and cheapest model in the GPT-4.1 series. It '\n",
      "                 'delivers exceptional performance at a small size with its 1 '\n",
      "                 'million token context window, and scores 80.1% on MMLU, '\n",
      "                 '50.3% on GPQA, and 9.8% on Aider polyglot coding – even '\n",
      "                 'higher than GPT‑4o mini. It’s ideal for tasks like '\n",
      "                 'classification or autocompletion.',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'openai/gpt-4.1-nano',\n",
      "  'name': 'OpenAI: GPT-4.1 Nano',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000004',\n",
      "              'image': '0',\n",
      "              'input_cache_read': '0.000000025',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 1047576,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 32768}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'arliai/qwq-32b-arliai-rpr-v1',\n",
      "  'context_length': 32768,\n",
      "  'created': 1744555982,\n",
      "  'description': 'QwQ-32B-ArliAI-RpR-v1 is a 32B parameter model fine-tuned '\n",
      "                 'from Qwen/QwQ-32B using a curated creative writing and '\n",
      "                 'roleplay dataset originally developed for the RPMax series. '\n",
      "                 'It is designed to maintain coherence and reasoning across '\n",
      "                 'long multi-turn conversations by introducing explicit '\n",
      "                 'reasoning steps per dialogue turn, generated and refined '\n",
      "                 'using the base model itself.\\n'\n",
      "                 '\\n'\n",
      "                 'The model was trained using RS-QLORA+ on 8K sequence lengths '\n",
      "                 'and supports up to 128K context windows (with practical '\n",
      "                 'performance around 32K). It is optimized for creative '\n",
      "                 'roleplay and dialogue generation, with an emphasis on '\n",
      "                 'minimizing cross-context repetition while preserving '\n",
      "                 'stylistic diversity.',\n",
      "  'hugging_face_id': 'ArliAI/QwQ-32B-ArliAI-RpR-v1',\n",
      "  'id': 'arliai/qwq-32b-arliai-rpr-v1:free',\n",
      "  'name': 'ArliAI: QwQ 32B RpR v1 (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'arliai/qwq-32b-arliai-rpr-v1',\n",
      "  'context_length': 32768,\n",
      "  'created': 1744555982,\n",
      "  'description': 'QwQ-32B-ArliAI-RpR-v1 is a 32B parameter model fine-tuned '\n",
      "                 'from Qwen/QwQ-32B using a curated creative writing and '\n",
      "                 'roleplay dataset originally developed for the RPMax series. '\n",
      "                 'It is designed to maintain coherence and reasoning across '\n",
      "                 'long multi-turn conversations by introducing explicit '\n",
      "                 'reasoning steps per dialogue turn, generated and refined '\n",
      "                 'using the base model itself.\\n'\n",
      "                 '\\n'\n",
      "                 'The model was trained using RS-QLORA+ on 8K sequence lengths '\n",
      "                 'and supports up to 128K context windows (with practical '\n",
      "                 'performance around 32K). It is optimized for creative '\n",
      "                 'roleplay and dialogue generation, with an emphasis on '\n",
      "                 'minimizing cross-context repetition while preserving '\n",
      "                 'stylistic diversity.',\n",
      "  'hugging_face_id': 'ArliAI/QwQ-32B-ArliAI-RpR-v1',\n",
      "  'id': 'arliai/qwq-32b-arliai-rpr-v1',\n",
      "  'name': 'ArliAI: QwQ 32B RpR v1',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000400032',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'agentica-org/deepcoder-14b-preview',\n",
      "  'context_length': 96000,\n",
      "  'created': 1744555395,\n",
      "  'description': 'DeepCoder-14B-Preview is a 14B parameter code generation '\n",
      "                 'model fine-tuned from DeepSeek-R1-Distill-Qwen-14B using '\n",
      "                 'reinforcement learning with GRPO+ and iterative context '\n",
      "                 'lengthening. It is optimized for long-context program '\n",
      "                 'synthesis and achieves strong performance across coding '\n",
      "                 'benchmarks, including 60.6% on LiveCodeBench v5, competitive '\n",
      "                 'with models like o3-Mini',\n",
      "  'hugging_face_id': 'agentica-org/DeepCoder-14B-Preview',\n",
      "  'id': 'agentica-org/deepcoder-14b-preview:free',\n",
      "  'name': 'Agentica: Deepcoder 14B Preview (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 96000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'agentica-org/deepcoder-14b-preview',\n",
      "  'context_length': 96000,\n",
      "  'created': 1744555395,\n",
      "  'description': 'DeepCoder-14B-Preview is a 14B parameter code generation '\n",
      "                 'model fine-tuned from DeepSeek-R1-Distill-Qwen-14B using '\n",
      "                 'reinforcement learning with GRPO+ and iterative context '\n",
      "                 'lengthening. It is optimized for long-context program '\n",
      "                 'synthesis and achieves strong performance across coding '\n",
      "                 'benchmarks, including 60.6% on LiveCodeBench v5, competitive '\n",
      "                 'with models like o3-Mini',\n",
      "  'hugging_face_id': 'agentica-org/DeepCoder-14B-Preview',\n",
      "  'id': 'agentica-org/deepcoder-14b-preview',\n",
      "  'name': 'Agentica: Deepcoder 14B Preview',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000015',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000000015',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 96000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['image', 'text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'moonshotai/kimi-vl-a3b-thinking',\n",
      "  'context_length': 131072,\n",
      "  'created': 1744304841,\n",
      "  'description': 'Kimi-VL is a lightweight Mixture-of-Experts vision-language '\n",
      "                 'model that activates only 2.8B parameters per step while '\n",
      "                 'delivering strong performance on multimodal reasoning and '\n",
      "                 'long-context tasks. The Kimi-VL-A3B-Thinking variant, '\n",
      "                 'fine-tuned with chain-of-thought and reinforcement learning, '\n",
      "                 'excels in math and visual reasoning benchmarks like '\n",
      "                 'MathVision, MMMU, and MathVista, rivaling much larger models '\n",
      "                 'such as Qwen2.5-VL-7B and Gemma-3-12B. It supports 128K '\n",
      "                 'context and high-resolution input via its MoonViT encoder.',\n",
      "  'hugging_face_id': 'moonshotai/Kimi-VL-A3B-Thinking',\n",
      "  'id': 'moonshotai/kimi-vl-a3b-thinking:free',\n",
      "  'name': 'MoonshotAI: Kimi VL A3B Thinking (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['image', 'text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'moonshotai/kimi-vl-a3b-thinking',\n",
      "  'context_length': 131072,\n",
      "  'created': 1744304841,\n",
      "  'description': 'Kimi-VL is a lightweight Mixture-of-Experts vision-language '\n",
      "                 'model that activates only 2.8B parameters per step while '\n",
      "                 'delivering strong performance on multimodal reasoning and '\n",
      "                 'long-context tasks. The Kimi-VL-A3B-Thinking variant, '\n",
      "                 'fine-tuned with chain-of-thought and reinforcement learning, '\n",
      "                 'excels in math and visual reasoning benchmarks like '\n",
      "                 'MathVision, MMMU, and MathVista, rivaling much larger models '\n",
      "                 'such as Qwen2.5-VL-7B and Gemma-3-12B. It supports 128K '\n",
      "                 'context and high-resolution input via its MoonViT encoder.',\n",
      "  'hugging_face_id': 'moonshotai/Kimi-VL-A3B-Thinking',\n",
      "  'id': 'moonshotai/kimi-vl-a3b-thinking',\n",
      "  'name': 'MoonshotAI: Kimi VL A3B Thinking',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000100008',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000002498985',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Grok'},\n",
      "  'canonical_slug': 'x-ai/grok-3-mini-beta',\n",
      "  'context_length': 131072,\n",
      "  'created': 1744240195,\n",
      "  'description': 'Grok 3 Mini is a lightweight, smaller thinking model. Unlike '\n",
      "                 'traditional models that generate answers immediately, Grok 3 '\n",
      "                 'Mini thinks before responding. It’s ideal for '\n",
      "                 'reasoning-heavy tasks that don’t demand extensive domain '\n",
      "                 'knowledge, and shines in math-specific and quantitative use '\n",
      "                 'cases, such as solving challenging puzzles or math '\n",
      "                 'problems.\\n'\n",
      "                 '\\n'\n",
      "                 'Transparent \"thinking\" traces accessible. Defaults to low '\n",
      "                 'reasoning, can boost with setting `reasoning: { effort: '\n",
      "                 '\"high\" }`\\n'\n",
      "                 '\\n'\n",
      "                 'Note: That there are two xAI endpoints for this model. By '\n",
      "                 'default when using this model we will always route you to '\n",
      "                 'the base endpoint. If you want the fast endpoint you can add '\n",
      "                 '`provider: { sort: throughput}`, to sort by throughput '\n",
      "                 'instead. \\n',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'x-ai/grok-3-mini-beta',\n",
      "  'name': 'xAI: Grok 3 Mini Beta',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000005',\n",
      "              'image': '0',\n",
      "              'input_cache_read': '0.000000075',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000003',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['include_reasoning',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'reasoning',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Grok'},\n",
      "  'canonical_slug': 'x-ai/grok-3-beta',\n",
      "  'context_length': 131072,\n",
      "  'created': 1744240068,\n",
      "  'description': \"Grok 3 is the latest model from xAI. It's their flagship \"\n",
      "                 'model that excels at enterprise use cases like data '\n",
      "                 'extraction, coding, and text summarization. Possesses deep '\n",
      "                 'domain knowledge in finance, healthcare, law, and science.\\n'\n",
      "                 '\\n'\n",
      "                 'Excels in structured tasks and benchmarks like GPQA, LCB, '\n",
      "                 'and MMLU-Pro where it outperforms Grok 3 Mini even on high '\n",
      "                 'thinking. \\n'\n",
      "                 '\\n'\n",
      "                 'Note: That there are two xAI endpoints for this model. By '\n",
      "                 'default when using this model we will always route you to '\n",
      "                 'the base endpoint. If you want the fast endpoint you can add '\n",
      "                 '`provider: { sort: throughput}`, to sort by throughput '\n",
      "                 'instead. \\n',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'x-ai/grok-3-beta',\n",
      "  'name': 'xAI: Grok 3 Beta',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000015',\n",
      "              'image': '0',\n",
      "              'input_cache_read': '0.00000075',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000003',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'nvidia/llama-3.3-nemotron-super-49b-v1',\n",
      "  'context_length': 131072,\n",
      "  'created': 1744119494,\n",
      "  'description': 'Llama-3.3-Nemotron-Super-49B-v1 is a large language model '\n",
      "                 '(LLM) optimized for advanced reasoning, conversational '\n",
      "                 'interactions, retrieval-augmented generation (RAG), and '\n",
      "                 \"tool-calling tasks. Derived from Meta's \"\n",
      "                 'Llama-3.3-70B-Instruct, it employs a Neural Architecture '\n",
      "                 'Search (NAS) approach, significantly enhancing efficiency '\n",
      "                 'and reducing memory requirements. This allows the model to '\n",
      "                 'support a context length of up to 128K tokens and fit '\n",
      "                 'efficiently on single high-performance GPUs, such as NVIDIA '\n",
      "                 'H200.\\n'\n",
      "                 '\\n'\n",
      "                 'Note: you must include `detailed thinking on` in the system '\n",
      "                 'prompt to enable reasoning. Please see [Usage '\n",
      "                 'Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) '\n",
      "                 'for more.',\n",
      "  'hugging_face_id': 'nvidia/Llama-3_3-Nemotron-Super-49B-v1',\n",
      "  'id': 'nvidia/llama-3.3-nemotron-super-49b-v1',\n",
      "  'name': 'NVIDIA: Llama 3.3 Nemotron Super 49B v1',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000004',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000013',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'nvidia/llama-3.1-nemotron-ultra-253b-v1',\n",
      "  'context_length': 131072,\n",
      "  'created': 1744115059,\n",
      "  'description': 'Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model '\n",
      "                 '(LLM) optimized for advanced reasoning, human-interactive '\n",
      "                 'chat, retrieval-augmented generation (RAG), and tool-calling '\n",
      "                 'tasks. Derived from Meta’s Llama-3.1-405B-Instruct, it has '\n",
      "                 'been significantly customized using Neural Architecture '\n",
      "                 'Search (NAS), resulting in enhanced efficiency, reduced '\n",
      "                 'memory usage, and improved inference latency. The model '\n",
      "                 'supports a context length of up to 128K tokens and can '\n",
      "                 'operate efficiently on an 8x NVIDIA H100 node.\\n'\n",
      "                 '\\n'\n",
      "                 'Note: you must include `detailed thinking on` in the system '\n",
      "                 'prompt to enable reasoning. Please see [Usage '\n",
      "                 'Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) '\n",
      "                 'for more.',\n",
      "  'hugging_face_id': 'nvidia/Llama-3_1-Nemotron-Ultra-253B-v1',\n",
      "  'id': 'nvidia/llama-3.1-nemotron-ultra-253b-v1:free',\n",
      "  'name': 'NVIDIA: Llama 3.1 Nemotron Ultra 253B v1 (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'nvidia/llama-3.1-nemotron-ultra-253b-v1',\n",
      "  'context_length': 131072,\n",
      "  'created': 1744115059,\n",
      "  'description': 'Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model '\n",
      "                 '(LLM) optimized for advanced reasoning, human-interactive '\n",
      "                 'chat, retrieval-augmented generation (RAG), and tool-calling '\n",
      "                 'tasks. Derived from Meta’s Llama-3.1-405B-Instruct, it has '\n",
      "                 'been significantly customized using Neural Architecture '\n",
      "                 'Search (NAS), resulting in enhanced efficiency, reduced '\n",
      "                 'memory usage, and improved inference latency. The model '\n",
      "                 'supports a context length of up to 128K tokens and can '\n",
      "                 'operate efficiently on an 8x NVIDIA H100 node.\\n'\n",
      "                 '\\n'\n",
      "                 'Note: you must include `detailed thinking on` in the system '\n",
      "                 'prompt to enable reasoning. Please see [Usage '\n",
      "                 'Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) '\n",
      "                 'for more.',\n",
      "  'hugging_face_id': 'nvidia/Llama-3_1-Nemotron-Ultra-253B-v1',\n",
      "  'id': 'nvidia/llama-3.1-nemotron-ultra-253b-v1',\n",
      "  'name': 'NVIDIA: Llama 3.1 Nemotron Ultra 253B v1',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000018',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000006',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama4'},\n",
      "  'canonical_slug': 'meta-llama/llama-4-maverick-17b-128e-instruct',\n",
      "  'context_length': 1048576,\n",
      "  'created': 1743881822,\n",
      "  'description': 'Llama 4 Maverick 17B Instruct (128E) is a high-capacity '\n",
      "                 'multimodal language model from Meta, built on a '\n",
      "                 'mixture-of-experts (MoE) architecture with 128 experts and '\n",
      "                 '17 billion active parameters per forward pass (400B total). '\n",
      "                 'It supports multilingual text and image input, and produces '\n",
      "                 'multilingual text and code output across 12 supported '\n",
      "                 'languages. Optimized for vision-language tasks, Maverick is '\n",
      "                 'instruction-tuned for assistant-like behavior, image '\n",
      "                 'reasoning, and general-purpose multimodal interaction.\\n'\n",
      "                 '\\n'\n",
      "                 'Maverick features early fusion for native multimodality and '\n",
      "                 'a 1 million token context window. It was trained on a '\n",
      "                 'curated mixture of public, licensed, and Meta-platform data, '\n",
      "                 'covering ~22 trillion tokens, with a knowledge cutoff in '\n",
      "                 'August 2024. Released on April 5, 2025 under the Llama 4 '\n",
      "                 'Community License, Maverick is suited for research and '\n",
      "                 'commercial applications requiring advanced multimodal '\n",
      "                 'understanding and high model throughput.',\n",
      "  'hugging_face_id': 'meta-llama/Llama-4-Maverick-17B-128E-Instruct',\n",
      "  'id': 'meta-llama/llama-4-maverick',\n",
      "  'name': 'Meta: Llama 4 Maverick',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000006',\n",
      "              'image': '0.0006684',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000015',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 1048576,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama4'},\n",
      "  'canonical_slug': 'meta-llama/llama-4-scout-17b-16e-instruct',\n",
      "  'context_length': 1048576,\n",
      "  'created': 1743881519,\n",
      "  'description': 'Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts '\n",
      "                 '(MoE) language model developed by Meta, activating 17 '\n",
      "                 'billion parameters out of a total of 109B. It supports '\n",
      "                 'native multimodal input (text and image) and multilingual '\n",
      "                 'output (text and code) across 12 supported languages. '\n",
      "                 'Designed for assistant-style interaction and visual '\n",
      "                 'reasoning, Scout uses 16 experts per forward pass and '\n",
      "                 'features a context length of 10 million tokens, with a '\n",
      "                 'training corpus of ~40 trillion tokens.\\n'\n",
      "                 '\\n'\n",
      "                 'Built for high efficiency and local or commercial '\n",
      "                 'deployment, Llama 4 Scout incorporates early fusion for '\n",
      "                 'seamless modality integration. It is instruction-tuned for '\n",
      "                 'use in multilingual chat, captioning, and image '\n",
      "                 'understanding tasks. Released under the Llama 4 Community '\n",
      "                 'License, it was last trained on data up to August 2024 and '\n",
      "                 'launched publicly on April 5, 2025.',\n",
      "  'hugging_face_id': 'meta-llama/Llama-4-Scout-17B-16E-Instruct',\n",
      "  'id': 'meta-llama/llama-4-scout',\n",
      "  'name': 'Meta: Llama 4 Scout',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000003',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000008',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 1048576,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 1048576}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'qwen/qwen2.5-vl-32b-instruct',\n",
      "  'context_length': 8192,\n",
      "  'created': 1742839838,\n",
      "  'description': 'Qwen2.5-VL-32B is a multimodal vision-language model '\n",
      "                 'fine-tuned through reinforcement learning for enhanced '\n",
      "                 'mathematical reasoning, structured outputs, and visual '\n",
      "                 'problem-solving capabilities. It excels at visual analysis '\n",
      "                 'tasks, including object recognition, textual interpretation '\n",
      "                 'within images, and precise event localization in extended '\n",
      "                 'videos. Qwen2.5-VL-32B demonstrates state-of-the-art '\n",
      "                 'performance across multimodal benchmarks such as MMMU, '\n",
      "                 'MathVista, and VideoMME, while maintaining strong reasoning '\n",
      "                 'and clarity in text-based tasks like MMLU, mathematical '\n",
      "                 'problem-solving, and code generation.',\n",
      "  'hugging_face_id': 'Qwen/Qwen2.5-VL-32B-Instruct',\n",
      "  'id': 'qwen/qwen2.5-vl-32b-instruct:free',\n",
      "  'name': 'Qwen: Qwen2.5 VL 32B Instruct (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 8192,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'qwen/qwen2.5-vl-32b-instruct',\n",
      "  'context_length': 16384,\n",
      "  'created': 1742839838,\n",
      "  'description': 'Qwen2.5-VL-32B is a multimodal vision-language model '\n",
      "                 'fine-tuned through reinforcement learning for enhanced '\n",
      "                 'mathematical reasoning, structured outputs, and visual '\n",
      "                 'problem-solving capabilities. It excels at visual analysis '\n",
      "                 'tasks, including object recognition, textual interpretation '\n",
      "                 'within images, and precise event localization in extended '\n",
      "                 'videos. Qwen2.5-VL-32B demonstrates state-of-the-art '\n",
      "                 'performance across multimodal benchmarks such as MMMU, '\n",
      "                 'MathVista, and VideoMME, while maintaining strong reasoning '\n",
      "                 'and clarity in text-based tasks like MMLU, mathematical '\n",
      "                 'problem-solving, and code generation.',\n",
      "  'hugging_face_id': 'Qwen/Qwen2.5-VL-32B-Instruct',\n",
      "  'id': 'qwen/qwen2.5-vl-32b-instruct',\n",
      "  'name': 'Qwen: Qwen2.5 VL 32B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 16384,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'deepseek/deepseek-chat-v3-0324',\n",
      "  'context_length': 163840,\n",
      "  'created': 1742824755,\n",
      "  'description': 'DeepSeek V3, a 685B-parameter, mixture-of-experts model, is '\n",
      "                 'the latest iteration of the flagship chat model family from '\n",
      "                 'the DeepSeek team.\\n'\n",
      "                 '\\n'\n",
      "                 'It succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) '\n",
      "                 'model and performs really well on a variety of tasks.',\n",
      "  'hugging_face_id': 'deepseek-ai/DeepSeek-V3-0324',\n",
      "  'id': 'deepseek/deepseek-chat-v3-0324:free',\n",
      "  'name': 'DeepSeek: DeepSeek V3 0324 (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'deepseek/deepseek-chat-v3-0324',\n",
      "  'context_length': 163840,\n",
      "  'created': 1742824755,\n",
      "  'description': 'DeepSeek V3, a 685B-parameter, mixture-of-experts model, is '\n",
      "                 'the latest iteration of the flagship chat model family from '\n",
      "                 'the DeepSeek team.\\n'\n",
      "                 '\\n'\n",
      "                 'It succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) '\n",
      "                 'model and performs really well on a variety of tasks.',\n",
      "  'hugging_face_id': 'deepseek-ai/DeepSeek-V3-0324',\n",
      "  'id': 'deepseek/deepseek-chat-v3-0324',\n",
      "  'name': 'DeepSeek: DeepSeek V3 0324',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/mistral-small-3.1-24b-instruct-2503',\n",
      "  'context_length': 128000,\n",
      "  'created': 1742238937,\n",
      "  'description': 'Mistral Small 3.1 24B Instruct is an upgraded variant of '\n",
      "                 'Mistral Small 3 (2501), featuring 24 billion parameters with '\n",
      "                 'advanced multimodal capabilities. It provides '\n",
      "                 'state-of-the-art performance in text-based reasoning and '\n",
      "                 'vision tasks, including image analysis, programming, '\n",
      "                 'mathematical reasoning, and multilingual support across '\n",
      "                 'dozens of languages. Equipped with an extensive 128k token '\n",
      "                 'context window and optimized for efficient local inference, '\n",
      "                 'it supports use cases such as conversational agents, '\n",
      "                 'function calling, long-document comprehension, and '\n",
      "                 'privacy-sensitive deployments. The updated version is '\n",
      "                 '[Mistral Small '\n",
      "                 '3.2](mistralai/mistral-small-3.2-24b-instruct)',\n",
      "  'hugging_face_id': 'mistralai/Mistral-Small-3.1-24B-Instruct-2503',\n",
      "  'id': 'mistralai/mistral-small-3.1-24b-instruct:free',\n",
      "  'name': 'Mistral: Mistral Small 3.1 24B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/mistral-small-3.1-24b-instruct-2503',\n",
      "  'context_length': 131072,\n",
      "  'created': 1742238937,\n",
      "  'description': 'Mistral Small 3.1 24B Instruct is an upgraded variant of '\n",
      "                 'Mistral Small 3 (2501), featuring 24 billion parameters with '\n",
      "                 'advanced multimodal capabilities. It provides '\n",
      "                 'state-of-the-art performance in text-based reasoning and '\n",
      "                 'vision tasks, including image analysis, programming, '\n",
      "                 'mathematical reasoning, and multilingual support across '\n",
      "                 'dozens of languages. Equipped with an extensive 128k token '\n",
      "                 'context window and optimized for efficient local inference, '\n",
      "                 'it supports use cases such as conversational agents, '\n",
      "                 'function calling, long-document comprehension, and '\n",
      "                 'privacy-sensitive deployments. The updated version is '\n",
      "                 '[Mistral Small '\n",
      "                 '3.2](mistralai/mistral-small-3.2-24b-instruct)',\n",
      "  'hugging_face_id': 'mistralai/Mistral-Small-3.1-24B-Instruct-2503',\n",
      "  'id': 'mistralai/mistral-small-3.1-24b-instruct',\n",
      "  'name': 'Mistral: Mistral Small 3.1 24B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 96000}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': 'gemma',\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Gemini'},\n",
      "  'canonical_slug': 'google/gemma-3-12b-it',\n",
      "  'context_length': 32768,\n",
      "  'created': 1741902625,\n",
      "  'description': 'Gemma 3 introduces multimodality, supporting vision-language '\n",
      "                 'input and text outputs. It handles context windows up to '\n",
      "                 '128k tokens, understands over 140 languages, and offers '\n",
      "                 'improved math, reasoning, and chat capabilities, including '\n",
      "                 'structured outputs and function calling. Gemma 3 12B is the '\n",
      "                 'second largest in the family of Gemma 3 models after [Gemma '\n",
      "                 '3 27B](google/gemma-3-27b-it)',\n",
      "  'hugging_face_id': 'google/gemma-3-12b-it',\n",
      "  'id': 'google/gemma-3-12b-it:free',\n",
      "  'name': 'Google: Gemma 3 12B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 8192}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': 'gemma',\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Gemini'},\n",
      "  'canonical_slug': 'google/gemma-3-12b-it',\n",
      "  'context_length': 96000,\n",
      "  'created': 1741902625,\n",
      "  'description': 'Gemma 3 introduces multimodality, supporting vision-language '\n",
      "                 'input and text outputs. It handles context windows up to '\n",
      "                 '128k tokens, understands over 140 languages, and offers '\n",
      "                 'improved math, reasoning, and chat capabilities, including '\n",
      "                 'structured outputs and function calling. Gemma 3 12B is the '\n",
      "                 'second largest in the family of Gemma 3 models after [Gemma '\n",
      "                 '3 27B](google/gemma-3-27b-it)',\n",
      "  'hugging_face_id': 'google/gemma-3-12b-it',\n",
      "  'id': 'google/gemma-3-12b-it',\n",
      "  'name': 'Google: Gemma 3 12B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000192608',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000000481286',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 96000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 8192}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'rekaai/reka-flash-3',\n",
      "  'context_length': 32768,\n",
      "  'created': 1741812813,\n",
      "  'description': 'Reka Flash 3 is a general-purpose, instruction-tuned large '\n",
      "                 'language model with 21 billion parameters, developed by '\n",
      "                 'Reka. It excels at general chat, coding tasks, '\n",
      "                 'instruction-following, and function calling. Featuring a 32K '\n",
      "                 'context length and optimized through reinforcement learning '\n",
      "                 '(RLOO), it provides competitive performance comparable to '\n",
      "                 'proprietary models within a smaller parameter footprint. '\n",
      "                 'Ideal for low-latency, local, or on-device deployments, Reka '\n",
      "                 'Flash 3 is compact, supports efficient quantization (down to '\n",
      "                 '11GB at 4-bit precision), and employs explicit reasoning '\n",
      "                 'tags (\"<reasoning>\") to indicate its internal thought '\n",
      "                 'process.\\n'\n",
      "                 '\\n'\n",
      "                 'Reka Flash 3 is primarily an English model with limited '\n",
      "                 'multilingual understanding capabilities. The model weights '\n",
      "                 'are released under the Apache 2.0 license.',\n",
      "  'hugging_face_id': 'RekaAI/reka-flash-3',\n",
      "  'id': 'rekaai/reka-flash-3:free',\n",
      "  'name': 'Reka: Flash 3 (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': 'gemma',\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Gemini'},\n",
      "  'canonical_slug': 'google/gemma-3-27b-it',\n",
      "  'context_length': 96000,\n",
      "  'created': 1741756359,\n",
      "  'description': 'Gemma 3 introduces multimodality, supporting vision-language '\n",
      "                 'input and text outputs. It handles context windows up to '\n",
      "                 '128k tokens, understands over 140 languages, and offers '\n",
      "                 'improved math, reasoning, and chat capabilities, including '\n",
      "                 'structured outputs and function calling. Gemma 3 27B is '\n",
      "                 \"Google's latest open source model, successor to [Gemma \"\n",
      "                 '2](google/gemma-2-27b-it)',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'google/gemma-3-27b-it:free',\n",
      "  'name': 'Google: Gemma 3 27B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 96000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 8192}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': 'gemma',\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Gemini'},\n",
      "  'canonical_slug': 'google/gemma-3-27b-it',\n",
      "  'context_length': 96000,\n",
      "  'created': 1741756359,\n",
      "  'description': 'Gemma 3 introduces multimodality, supporting vision-language '\n",
      "                 'input and text outputs. It handles context windows up to '\n",
      "                 '128k tokens, understands over 140 languages, and offers '\n",
      "                 'improved math, reasoning, and chat capabilities, including '\n",
      "                 'structured outputs and function calling. Gemma 3 27B is '\n",
      "                 \"Google's latest open source model, successor to [Gemma \"\n",
      "                 '2](google/gemma-2-27b-it)',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'google/gemma-3-27b-it',\n",
      "  'name': 'Google: Gemma 3 27B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000266688',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000000666396',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 96000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 8192}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'thedrummer/skyfall-36b-v2',\n",
      "  'context_length': 32768,\n",
      "  'created': 1741636566,\n",
      "  'description': 'Skyfall 36B v2 is an enhanced iteration of Mistral Small '\n",
      "                 '2501, specifically fine-tuned for improved creativity, '\n",
      "                 'nuanced writing, role-playing, and coherent storytelling.',\n",
      "  'hugging_face_id': 'TheDrummer/Skyfall-36B-v2',\n",
      "  'id': 'thedrummer/skyfall-36b-v2',\n",
      "  'name': 'TheDrummer: Skyfall 36B V2',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000192608',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000000481286',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'qwq',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'qwen/qwq-32b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1741208814,\n",
      "  'description': 'QwQ is the reasoning model of the Qwen series. Compared with '\n",
      "                 'conventional instruction-tuned models, QwQ, which is capable '\n",
      "                 'of thinking and reasoning, can achieve significantly '\n",
      "                 'enhanced performance in downstream tasks, especially hard '\n",
      "                 'problems. QwQ-32B is the medium-sized reasoning model, which '\n",
      "                 'is capable of achieving competitive performance against '\n",
      "                 'state-of-the-art reasoning models, e.g., DeepSeek-R1, '\n",
      "                 'o1-mini.',\n",
      "  'hugging_face_id': 'Qwen/QwQ-32B',\n",
      "  'id': 'qwen/qwq-32b',\n",
      "  'name': 'Qwen: QwQ 32B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000004',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000015',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'nousresearch/deephermes-3-llama-3-8b-preview',\n",
      "  'context_length': 131072,\n",
      "  'created': 1740719372,\n",
      "  'description': 'DeepHermes 3 Preview is the latest version of our flagship '\n",
      "                 'Hermes series of LLMs by Nous Research, and one of the first '\n",
      "                 'models in the world to unify Reasoning (long chains of '\n",
      "                 'thought that improve answer accuracy) and normal LLM '\n",
      "                 'response modes into one model. We have also improved LLM '\n",
      "                 'annotation, judgement, and function calling.\\n'\n",
      "                 '\\n'\n",
      "                 'DeepHermes 3 Preview is one of the first LLM models to unify '\n",
      "                 'both \"intuitive\", traditional mode responses and long chain '\n",
      "                 'of thought reasoning responses into a single model, toggled '\n",
      "                 'by a system prompt.',\n",
      "  'hugging_face_id': 'NousResearch/DeepHermes-3-Llama-3-8B-Preview',\n",
      "  'id': 'nousresearch/deephermes-3-llama-3-8b-preview:free',\n",
      "  'name': 'Nous: DeepHermes 3 Llama 3 8B Preview (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'cognitivecomputations/dolphin3.0-r1-mistral-24b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1739462498,\n",
      "  'description': 'Dolphin 3.0 R1 is the next generation of the Dolphin series '\n",
      "                 'of instruct-tuned models.  Designed to be the ultimate '\n",
      "                 'general purpose local model, enabling coding, math, agentic, '\n",
      "                 'function calling, and general use cases.\\n'\n",
      "                 '\\n'\n",
      "                 'The R1 version has been trained for 3 epochs to reason using '\n",
      "                 '800k reasoning traces from the Dolphin-R1 dataset.\\n'\n",
      "                 '\\n'\n",
      "                 'Dolphin aims to be a general purpose reasoning instruct '\n",
      "                 'model, similar to the models behind ChatGPT, Claude, '\n",
      "                 'Gemini.\\n'\n",
      "                 '\\n'\n",
      "                 'Part of the [Dolphin 3.0 '\n",
      "                 'Collection](https://huggingface.co/collections/cognitivecomputations/dolphin-30-677ab47f73d7ff66743979a3) '\n",
      "                 'Curated and trained by [Eric '\n",
      "                 'Hartford](https://huggingface.co/ehartford), [Ben '\n",
      "                 'Gitter](https://huggingface.co/bigstorm), '\n",
      "                 '[BlouseJury](https://huggingface.co/BlouseJury) and '\n",
      "                 '[Cognitive '\n",
      "                 'Computations](https://huggingface.co/cognitivecomputations)',\n",
      "  'hugging_face_id': 'cognitivecomputations/Dolphin3.0-R1-Mistral-24B',\n",
      "  'id': 'cognitivecomputations/dolphin3.0-r1-mistral-24b:free',\n",
      "  'name': 'Dolphin3.0 R1 Mistral 24B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'cognitivecomputations/dolphin3.0-r1-mistral-24b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1739462498,\n",
      "  'description': 'Dolphin 3.0 R1 is the next generation of the Dolphin series '\n",
      "                 'of instruct-tuned models.  Designed to be the ultimate '\n",
      "                 'general purpose local model, enabling coding, math, agentic, '\n",
      "                 'function calling, and general use cases.\\n'\n",
      "                 '\\n'\n",
      "                 'The R1 version has been trained for 3 epochs to reason using '\n",
      "                 '800k reasoning traces from the Dolphin-R1 dataset.\\n'\n",
      "                 '\\n'\n",
      "                 'Dolphin aims to be a general purpose reasoning instruct '\n",
      "                 'model, similar to the models behind ChatGPT, Claude, '\n",
      "                 'Gemini.\\n'\n",
      "                 '\\n'\n",
      "                 'Part of the [Dolphin 3.0 '\n",
      "                 'Collection](https://huggingface.co/collections/cognitivecomputations/dolphin-30-677ab47f73d7ff66743979a3) '\n",
      "                 'Curated and trained by [Eric '\n",
      "                 'Hartford](https://huggingface.co/ehartford), [Ben '\n",
      "                 'Gitter](https://huggingface.co/bigstorm), '\n",
      "                 '[BlouseJury](https://huggingface.co/BlouseJury) and '\n",
      "                 '[Cognitive '\n",
      "                 'Computations](https://huggingface.co/cognitivecomputations)',\n",
      "  'hugging_face_id': 'cognitivecomputations/Dolphin3.0-R1-Mistral-24B',\n",
      "  'id': 'cognitivecomputations/dolphin3.0-r1-mistral-24b',\n",
      "  'name': 'Dolphin3.0 R1 Mistral 24B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000340768',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'cognitivecomputations/dolphin3.0-mistral-24b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1739462019,\n",
      "  'description': 'Dolphin 3.0 is the next generation of the Dolphin series of '\n",
      "                 'instruct-tuned models.  Designed to be the ultimate general '\n",
      "                 'purpose local model, enabling coding, math, agentic, '\n",
      "                 'function calling, and general use cases.\\n'\n",
      "                 '\\n'\n",
      "                 'Dolphin aims to be a general purpose instruct model, similar '\n",
      "                 'to the models behind ChatGPT, Claude, Gemini. \\n'\n",
      "                 '\\n'\n",
      "                 'Part of the [Dolphin 3.0 '\n",
      "                 'Collection](https://huggingface.co/collections/cognitivecomputations/dolphin-30-677ab47f73d7ff66743979a3) '\n",
      "                 'Curated and trained by [Eric '\n",
      "                 'Hartford](https://huggingface.co/ehartford), [Ben '\n",
      "                 'Gitter](https://huggingface.co/bigstorm), '\n",
      "                 '[BlouseJury](https://huggingface.co/BlouseJury) and '\n",
      "                 '[Cognitive '\n",
      "                 'Computations](https://huggingface.co/cognitivecomputations)',\n",
      "  'hugging_face_id': 'cognitivecomputations/Dolphin3.0-Mistral-24B',\n",
      "  'id': 'cognitivecomputations/dolphin3.0-mistral-24b:free',\n",
      "  'name': 'Dolphin3.0 Mistral 24B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'cognitivecomputations/dolphin3.0-mistral-24b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1739462019,\n",
      "  'description': 'Dolphin 3.0 is the next generation of the Dolphin series of '\n",
      "                 'instruct-tuned models.  Designed to be the ultimate general '\n",
      "                 'purpose local model, enabling coding, math, agentic, '\n",
      "                 'function calling, and general use cases.\\n'\n",
      "                 '\\n'\n",
      "                 'Dolphin aims to be a general purpose instruct model, similar '\n",
      "                 'to the models behind ChatGPT, Claude, Gemini. \\n'\n",
      "                 '\\n'\n",
      "                 'Part of the [Dolphin 3.0 '\n",
      "                 'Collection](https://huggingface.co/collections/cognitivecomputations/dolphin-30-677ab47f73d7ff66743979a3) '\n",
      "                 'Curated and trained by [Eric '\n",
      "                 'Hartford](https://huggingface.co/ehartford), [Ben '\n",
      "                 'Gitter](https://huggingface.co/bigstorm), '\n",
      "                 '[BlouseJury](https://huggingface.co/BlouseJury) and '\n",
      "                 '[Cognitive '\n",
      "                 'Computations](https://huggingface.co/cognitivecomputations)',\n",
      "  'hugging_face_id': 'cognitivecomputations/Dolphin3.0-Mistral-24B',\n",
      "  'id': 'cognitivecomputations/dolphin3.0-mistral-24b',\n",
      "  'name': 'Dolphin3.0 Mistral 24B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000014816',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000000037022',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'none',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'meta-llama/llama-guard-3-8b',\n",
      "  'context_length': 131072,\n",
      "  'created': 1739401318,\n",
      "  'description': 'Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned '\n",
      "                 'for content safety classification. Similar to previous '\n",
      "                 'versions, it can be used to classify content in both LLM '\n",
      "                 'inputs (prompt classification) and in LLM responses '\n",
      "                 '(response classification). It acts as an LLM – it generates '\n",
      "                 'text in its output that indicates whether a given prompt or '\n",
      "                 'response is safe or unsafe, and if unsafe, it also lists the '\n",
      "                 'content categories violated.\\n'\n",
      "                 '\\n'\n",
      "                 'Llama Guard 3 was aligned to safeguard against the MLCommons '\n",
      "                 'standardized hazards taxonomy and designed to support Llama '\n",
      "                 '3.1 capabilities. Specifically, it provides content '\n",
      "                 'moderation in 8 languages, and was optimized to support '\n",
      "                 'safety and security for search and code interpreter tool '\n",
      "                 'calls.\\n',\n",
      "  'hugging_face_id': 'meta-llama/Llama-Guard-3-8B',\n",
      "  'id': 'meta-llama/llama-guard-3-8b',\n",
      "  'name': 'Llama Guard 3 8B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000006',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000002',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'qwen/qwen2.5-vl-72b-instruct',\n",
      "  'context_length': 32768,\n",
      "  'created': 1738410311,\n",
      "  'description': 'Qwen2.5-VL is proficient in recognizing common objects such '\n",
      "                 'as flowers, birds, fish, and insects. It is also highly '\n",
      "                 'capable of analyzing texts, charts, icons, graphics, and '\n",
      "                 'layouts within images.',\n",
      "  'hugging_face_id': 'Qwen/Qwen2.5-VL-72B-Instruct',\n",
      "  'id': 'qwen/qwen2.5-vl-72b-instruct',\n",
      "  'name': 'Qwen: Qwen2.5 VL 72B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000400032',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000000999594',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/mistral-small-24b-instruct-2501',\n",
      "  'context_length': 32768,\n",
      "  'created': 1738255409,\n",
      "  'description': 'Mistral Small 3 is a 24B-parameter language model optimized '\n",
      "                 'for low-latency performance across common AI tasks. Released '\n",
      "                 'under the Apache 2.0 license, it features both pre-trained '\n",
      "                 'and instruction-tuned versions designed for efficient local '\n",
      "                 'deployment.\\n'\n",
      "                 '\\n'\n",
      "                 'The model achieves 81% accuracy on the MMLU benchmark and '\n",
      "                 'performs competitively with larger models like Llama 3.3 70B '\n",
      "                 'and Qwen 32B, while operating at three times the speed on '\n",
      "                 'equivalent hardware. [Read the blog post about the model '\n",
      "                 'here.](https://mistral.ai/news/mistral-small-3/)',\n",
      "  'hugging_face_id': 'mistralai/Mistral-Small-24B-Instruct-2501',\n",
      "  'id': 'mistralai/mistral-small-24b-instruct-2501:free',\n",
      "  'name': 'Mistral: Mistral Small 3 (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/mistral-small-24b-instruct-2501',\n",
      "  'context_length': 32768,\n",
      "  'created': 1738255409,\n",
      "  'description': 'Mistral Small 3 is a 24B-parameter language model optimized '\n",
      "                 'for low-latency performance across common AI tasks. Released '\n",
      "                 'under the Apache 2.0 license, it features both pre-trained '\n",
      "                 'and instruction-tuned versions designed for efficient local '\n",
      "                 'deployment.\\n'\n",
      "                 '\\n'\n",
      "                 'The model achieves 81% accuracy on the MMLU benchmark and '\n",
      "                 'performs competitively with larger models like Llama 3.3 70B '\n",
      "                 'and Qwen 32B, while operating at three times the speed on '\n",
      "                 'equivalent hardware. [Read the blog post about the model '\n",
      "                 'here.](https://mistral.ai/news/mistral-small-3/)',\n",
      "  'hugging_face_id': 'mistralai/Mistral-Small-24B-Instruct-2501',\n",
      "  'id': 'mistralai/mistral-small-24b-instruct-2501',\n",
      "  'name': 'Mistral: Mistral Small 3',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'deepseek/deepseek-r1-distill-qwen-14b',\n",
      "  'context_length': 64000,\n",
      "  'created': 1738193940,\n",
      "  'description': 'DeepSeek R1 Distill Qwen 14B is a distilled large language '\n",
      "                 'model based on [Qwen 2.5 '\n",
      "                 '14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B), '\n",
      "                 'using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It '\n",
      "                 \"outperforms OpenAI's o1-mini across various benchmarks, \"\n",
      "                 'achieving new state-of-the-art results for dense models.\\n'\n",
      "                 '\\n'\n",
      "                 'Other benchmark results include:\\n'\n",
      "                 '\\n'\n",
      "                 '- AIME 2024 pass@1: 69.7\\n'\n",
      "                 '- MATH-500 pass@1: 93.9\\n'\n",
      "                 '- CodeForces Rating: 1481\\n'\n",
      "                 '\\n'\n",
      "                 \"The model leverages fine-tuning from DeepSeek R1's outputs, \"\n",
      "                 'enabling competitive performance comparable to larger '\n",
      "                 'frontier models.',\n",
      "  'hugging_face_id': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
      "  'id': 'deepseek/deepseek-r1-distill-qwen-14b:free',\n",
      "  'name': 'DeepSeek: R1 Distill Qwen 14B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 64000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'chatml',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'liquid/lfm-7b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1737806883,\n",
      "  'description': 'LFM-7B, a new best-in-class language model. LFM-7B is '\n",
      "                 'designed for exceptional chat capabilities, including '\n",
      "                 'languages like Arabic and Japanese. Powered by the Liquid '\n",
      "                 'Foundation Model (LFM) architecture, it exhibits unique '\n",
      "                 'features like low memory footprint and fast inference '\n",
      "                 'speed. \\n'\n",
      "                 '\\n'\n",
      "                 'LFM-7B is the world’s best-in-class multilingual language '\n",
      "                 'model in English, Arabic, and Japanese.\\n'\n",
      "                 '\\n'\n",
      "                 'See the [launch announcement](https://www.liquid.ai/lfm-7b) '\n",
      "                 'for benchmarks and more info.',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'liquid/lfm-7b',\n",
      "  'name': 'Liquid: LFM 7B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000001',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'deepseek/deepseek-r1-distill-llama-70b',\n",
      "  'context_length': 8192,\n",
      "  'created': 1737663169,\n",
      "  'description': 'DeepSeek R1 Distill Llama 70B is a distilled large language '\n",
      "                 'model based on '\n",
      "                 '[Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct), '\n",
      "                 'using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The '\n",
      "                 'model combines advanced distillation techniques to achieve '\n",
      "                 'high performance across multiple benchmarks, including:\\n'\n",
      "                 '\\n'\n",
      "                 '- AIME 2024 pass@1: 70.0\\n'\n",
      "                 '- MATH-500 pass@1: 94.5\\n'\n",
      "                 '- CodeForces Rating: 1633\\n'\n",
      "                 '\\n'\n",
      "                 \"The model leverages fine-tuning from DeepSeek R1's outputs, \"\n",
      "                 'enabling competitive performance comparable to larger '\n",
      "                 'frontier models.',\n",
      "  'hugging_face_id': 'deepseek-ai/DeepSeek-R1-Distill-Llama-70B',\n",
      "  'id': 'deepseek/deepseek-r1-distill-llama-70b:free',\n",
      "  'name': 'DeepSeek: R1 Distill Llama 70B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 8192,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 4096}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'deepseek/deepseek-r1-distill-llama-70b',\n",
      "  'context_length': 131072,\n",
      "  'created': 1737663169,\n",
      "  'description': 'DeepSeek R1 Distill Llama 70B is a distilled large language '\n",
      "                 'model based on '\n",
      "                 '[Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct), '\n",
      "                 'using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The '\n",
      "                 'model combines advanced distillation techniques to achieve '\n",
      "                 'high performance across multiple benchmarks, including:\\n'\n",
      "                 '\\n'\n",
      "                 '- AIME 2024 pass@1: 70.0\\n'\n",
      "                 '- MATH-500 pass@1: 94.5\\n'\n",
      "                 '- CodeForces Rating: 1633\\n'\n",
      "                 '\\n'\n",
      "                 \"The model leverages fine-tuning from DeepSeek R1's outputs, \"\n",
      "                 'enabling competitive performance comparable to larger '\n",
      "                 'frontier models.',\n",
      "  'hugging_face_id': 'deepseek-ai/DeepSeek-R1-Distill-Llama-70B',\n",
      "  'id': 'deepseek/deepseek-r1-distill-llama-70b',\n",
      "  'name': 'DeepSeek: R1 Distill Llama 70B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000103712',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000000259154',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'deepseek/deepseek-r1',\n",
      "  'context_length': 163840,\n",
      "  'created': 1737381095,\n",
      "  'description': 'DeepSeek R1 is here: Performance on par with [OpenAI '\n",
      "                 'o1](/openai/o1), but open-sourced and with fully open '\n",
      "                 \"reasoning tokens. It's 671B parameters in size, with 37B \"\n",
      "                 'active in an inference pass.\\n'\n",
      "                 '\\n'\n",
      "                 'Fully open-source model & [technical '\n",
      "                 'report](https://api-docs.deepseek.com/news/news250120).\\n'\n",
      "                 '\\n'\n",
      "                 'MIT licensed: Distill & commercialize freely!',\n",
      "  'hugging_face_id': 'deepseek-ai/DeepSeek-R1',\n",
      "  'id': 'deepseek/deepseek-r1',\n",
      "  'name': 'DeepSeek: R1',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000002',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000004',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'include_reasoning',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'reasoning',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 163840}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Other'},\n",
      "  'canonical_slug': 'microsoft/phi-4',\n",
      "  'context_length': 16384,\n",
      "  'created': 1736489872,\n",
      "  'description': '[Microsoft Research](/microsoft) Phi-4 is designed to '\n",
      "                 'perform well in complex reasoning tasks and can operate '\n",
      "                 'efficiently in situations with limited memory or where quick '\n",
      "                 'responses are needed. \\n'\n",
      "                 '\\n'\n",
      "                 'At 14 billion parameters, it was trained on a mix of '\n",
      "                 'high-quality synthetic datasets, data from curated websites, '\n",
      "                 'and academic materials. It has undergone careful improvement '\n",
      "                 'to follow instructions accurately and maintain strong safety '\n",
      "                 'standards. It works best with English language inputs.\\n'\n",
      "                 '\\n'\n",
      "                 'For more information, please see [Phi-4 Technical '\n",
      "                 'Report](https://arxiv.org/pdf/2412.08905)\\n',\n",
      "  'hugging_face_id': 'microsoft/phi-4',\n",
      "  'id': 'microsoft/phi-4',\n",
      "  'name': 'Microsoft: Phi 4',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000014',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000006',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 16384,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'DeepSeek'},\n",
      "  'canonical_slug': 'deepseek/deepseek-chat-v3',\n",
      "  'context_length': 163840,\n",
      "  'created': 1735241320,\n",
      "  'description': 'DeepSeek-V3 is the latest model from the DeepSeek team, '\n",
      "                 'building upon the instruction following and coding abilities '\n",
      "                 'of the previous versions. Pre-trained on nearly 15 trillion '\n",
      "                 'tokens, the reported evaluations reveal that the model '\n",
      "                 'outperforms other open-source models and rivals leading '\n",
      "                 'closed-source models.\\n'\n",
      "                 '\\n'\n",
      "                 'For model details, please visit [the DeepSeek-V3 '\n",
      "                 'repo](https://github.com/deepseek-ai/DeepSeek-V3) for more '\n",
      "                 'information, or see the [launch '\n",
      "                 'announcement](https://api-docs.deepseek.com/news/news1226).',\n",
      "  'hugging_face_id': 'deepseek-ai/DeepSeek-V3',\n",
      "  'id': 'deepseek/deepseek-chat',\n",
      "  'name': 'DeepSeek: DeepSeek V3',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000800064',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000001999188',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 163840,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Grok'},\n",
      "  'canonical_slug': 'x-ai/grok-2-vision-1212',\n",
      "  'context_length': 32768,\n",
      "  'created': 1734237338,\n",
      "  'description': 'Grok 2 Vision 1212 advances image-based AI with stronger '\n",
      "                 'visual comprehension, refined instruction-following, and '\n",
      "                 'multilingual support. From object recognition to style '\n",
      "                 'analysis, it empowers developers to build more intuitive, '\n",
      "                 'visually aware applications. Its enhanced steerability and '\n",
      "                 'reasoning establish a robust foundation for next-generation '\n",
      "                 'image solutions.\\n'\n",
      "                 '\\n'\n",
      "                 \"To read more about this model, check out [xAI's \"\n",
      "                 'announcement](https://x.ai/blog/grok-1212).',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'x-ai/grok-2-vision-1212',\n",
      "  'name': 'xAI: Grok 2 Vision 1212',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00001',\n",
      "              'image': '0.0036',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000002',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Grok'},\n",
      "  'canonical_slug': 'x-ai/grok-2-1212',\n",
      "  'context_length': 131072,\n",
      "  'created': 1734232814,\n",
      "  'description': 'Grok 2 1212 introduces significant enhancements to accuracy, '\n",
      "                 'instruction adherence, and multilingual support, making it a '\n",
      "                 'powerful and flexible choice for developers seeking a highly '\n",
      "                 'steerable, intelligent model.',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'x-ai/grok-2-1212',\n",
      "  'name': 'xAI: Grok 2 1212',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00001',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000002',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'llama3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'meta-llama/llama-3.3-70b-instruct',\n",
      "  'context_length': 131072,\n",
      "  'created': 1733506137,\n",
      "  'description': 'The Meta Llama 3.3 multilingual large language model (LLM) '\n",
      "                 'is a pretrained and instruction tuned generative model in '\n",
      "                 '70B (text in/text out). The Llama 3.3 instruction tuned text '\n",
      "                 'only model is optimized for multilingual dialogue use cases '\n",
      "                 'and outperforms many of the available open source and closed '\n",
      "                 'chat models on common industry benchmarks.\\n'\n",
      "                 '\\n'\n",
      "                 'Supported languages: English, German, French, Italian, '\n",
      "                 'Portuguese, Hindi, Spanish, and Thai.\\n'\n",
      "                 '\\n'\n",
      "                 '[Model '\n",
      "                 'Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)',\n",
      "  'hugging_face_id': 'meta-llama/Llama-3.3-70B-Instruct',\n",
      "  'id': 'meta-llama/llama-3.3-70b-instruct',\n",
      "  'name': 'Meta: Llama 3.3 70B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000012',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000000038',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'deepseek-r1',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'qwen/qwq-32b-preview',\n",
      "  'context_length': 32768,\n",
      "  'created': 1732754541,\n",
      "  'description': 'QwQ-32B-Preview is an experimental research model focused on '\n",
      "                 'AI reasoning capabilities developed by the Qwen Team. As a '\n",
      "                 'preview release, it demonstrates promising analytical '\n",
      "                 'abilities while having several important limitations:\\n'\n",
      "                 '\\n'\n",
      "                 '1. **Language Mixing and Code-Switching**: The model may mix '\n",
      "                 'languages or switch between them unexpectedly, affecting '\n",
      "                 'response clarity.\\n'\n",
      "                 '2. **Recursive Reasoning Loops**: The model may enter '\n",
      "                 'circular reasoning patterns, leading to lengthy responses '\n",
      "                 'without a conclusive answer.\\n'\n",
      "                 '3. **Safety and Ethical Considerations**: The model requires '\n",
      "                 'enhanced safety measures to ensure reliable and secure '\n",
      "                 'performance, and users should exercise caution when '\n",
      "                 'deploying it.\\n'\n",
      "                 '4. **Performance and Benchmark Limitations**: The model '\n",
      "                 'excels in math and coding but has room for improvement in '\n",
      "                 'other areas, such as common sense reasoning and nuanced '\n",
      "                 'language understanding.\\n'\n",
      "                 '\\n',\n",
      "  'hugging_face_id': 'Qwen/QwQ-32B-Preview',\n",
      "  'id': 'qwen/qwq-32b-preview',\n",
      "  'name': 'Qwen: QwQ 32B Preview',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000002',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000002',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text', 'image', 'file'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4o-2024-11-20',\n",
      "  'context_length': 128000,\n",
      "  'created': 1732127594,\n",
      "  'description': 'The 2024-11-20 version of GPT-4o offers a leveled-up '\n",
      "                 'creative writing ability with more natural, engaging, and '\n",
      "                 'tailored writing to improve relevance & readability. It’s '\n",
      "                 'also better at working with uploaded files, providing deeper '\n",
      "                 'insights & more thorough responses.\\n'\n",
      "                 '\\n'\n",
      "                 'GPT-4o (\"o\" for \"omni\") is OpenAI\\'s latest AI model, '\n",
      "                 'supporting both text and image inputs with text outputs. It '\n",
      "                 'maintains the intelligence level of [GPT-4 '\n",
      "                 'Turbo](/models/openai/gpt-4-turbo) while being twice as fast '\n",
      "                 'and 50% more cost-effective. GPT-4o also offers improved '\n",
      "                 'performance in processing non-English languages and enhanced '\n",
      "                 'visual capabilities.',\n",
      "  'hugging_face_id': '',\n",
      "  'id': 'openai/gpt-4o-2024-11-20',\n",
      "  'name': 'OpenAI: GPT-4o (2024-11-20)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00001',\n",
      "              'image': '0.003613',\n",
      "              'input_cache_read': '0.00000125',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000025',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p',\n",
      "                           'web_search_options'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'chatml',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'qwen/qwen-2.5-coder-32b-instruct',\n",
      "  'context_length': 32768,\n",
      "  'created': 1731368400,\n",
      "  'description': 'Qwen2.5-Coder is the latest series of Code-Specific Qwen '\n",
      "                 'large language models (formerly known as CodeQwen). '\n",
      "                 'Qwen2.5-Coder brings the following improvements upon '\n",
      "                 'CodeQwen1.5:\\n'\n",
      "                 '\\n'\n",
      "                 '- Significantly improvements in **code generation**, **code '\n",
      "                 'reasoning** and **code fixing**. \\n'\n",
      "                 '- A more comprehensive foundation for real-world '\n",
      "                 'applications such as **Code Agents**. Not only enhancing '\n",
      "                 'coding capabilities but also maintaining its strengths in '\n",
      "                 'mathematics and general competencies.\\n'\n",
      "                 '\\n'\n",
      "                 'To read more about its evaluation results, check out [Qwen '\n",
      "                 \"2.5 Coder's \"\n",
      "                 'blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).',\n",
      "  'hugging_face_id': 'Qwen/Qwen2.5-Coder-32B-Instruct',\n",
      "  'id': 'qwen/qwen-2.5-coder-32b-instruct:free',\n",
      "  'name': 'Qwen2.5 Coder 32B Instruct (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'chatml',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'qwen/qwen-2.5-coder-32b-instruct',\n",
      "  'context_length': 32768,\n",
      "  'created': 1731368400,\n",
      "  'description': 'Qwen2.5-Coder is the latest series of Code-Specific Qwen '\n",
      "                 'large language models (formerly known as CodeQwen). '\n",
      "                 'Qwen2.5-Coder brings the following improvements upon '\n",
      "                 'CodeQwen1.5:\\n'\n",
      "                 '\\n'\n",
      "                 '- Significantly improvements in **code generation**, **code '\n",
      "                 'reasoning** and **code fixing**. \\n'\n",
      "                 '- A more comprehensive foundation for real-world '\n",
      "                 'applications such as **Code Agents**. Not only enhancing '\n",
      "                 'coding capabilities but also maintaining its strengths in '\n",
      "                 'mathematics and general competencies.\\n'\n",
      "                 '\\n'\n",
      "                 'To read more about its evaluation results, check out [Qwen '\n",
      "                 \"2.5 Coder's \"\n",
      "                 'blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).',\n",
      "  'hugging_face_id': 'Qwen/Qwen2.5-Coder-32B-Instruct',\n",
      "  'id': 'qwen/qwen-2.5-coder-32b-instruct',\n",
      "  'name': 'Qwen2.5 Coder 32B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000200016',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000000499797',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'mistral',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'thedrummer/unslopnemo-12b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1731103448,\n",
      "  'description': 'UnslopNemo v4.1 is the latest addition from the creator of '\n",
      "                 'Rocinante, designed for adventure writing and role-play '\n",
      "                 'scenarios.',\n",
      "  'hugging_face_id': 'TheDrummer/UnslopNemo-12B-v4.1',\n",
      "  'id': 'thedrummer/unslopnemo-12b',\n",
      "  'name': 'TheDrummer: UnslopNemo 12B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000004',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000004',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'chatml',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'thedrummer/rocinante-12b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1727654400,\n",
      "  'description': 'Rocinante 12B is designed for engaging storytelling and rich '\n",
      "                 'prose.\\n'\n",
      "                 '\\n'\n",
      "                 'Early testers have reported:\\n'\n",
      "                 '- Expanded vocabulary with unique and expressive word '\n",
      "                 'choices\\n'\n",
      "                 '- Enhanced creativity for vivid narratives\\n'\n",
      "                 '- Adventure-filled and captivating stories',\n",
      "  'hugging_face_id': 'TheDrummer/Rocinante-12B-v1.1',\n",
      "  'id': 'thedrummer/rocinante-12b',\n",
      "  'name': 'TheDrummer: Rocinante 12B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000043',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000017',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'llama3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'meta-llama/llama-3.2-3b-instruct',\n",
      "  'context_length': 131072,\n",
      "  'created': 1727222400,\n",
      "  'description': 'Llama 3.2 3B is a 3-billion-parameter multilingual large '\n",
      "                 'language model, optimized for advanced natural language '\n",
      "                 'processing tasks like dialogue generation, reasoning, and '\n",
      "                 'summarization. Designed with the latest transformer '\n",
      "                 'architecture, it supports eight languages, including '\n",
      "                 'English, Spanish, and Hindi, and is adaptable for additional '\n",
      "                 'languages.\\n'\n",
      "                 '\\n'\n",
      "                 'Trained on 9 trillion tokens, the Llama 3.2 3B model excels '\n",
      "                 'in instruction-following, complex reasoning, and tool use. '\n",
      "                 'Its balanced performance makes it ideal for applications '\n",
      "                 'needing accuracy and efficiency in text generation across '\n",
      "                 'multilingual settings.\\n'\n",
      "                 '\\n'\n",
      "                 'Click here for the [original model '\n",
      "                 'card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\\n'\n",
      "                 '\\n'\n",
      "                 \"Usage of this model is subject to [Meta's Acceptable Use \"\n",
      "                 'Policy](https://www.llama.com/llama3/use-policy/).',\n",
      "  'hugging_face_id': 'meta-llama/Llama-3.2-3B-Instruct',\n",
      "  'id': 'meta-llama/llama-3.2-3b-instruct',\n",
      "  'name': 'Meta: Llama 3.2 3B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000024',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000000012',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'chatml',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'qwen/qwen-2.5-72b-instruct',\n",
      "  'context_length': 32768,\n",
      "  'created': 1726704000,\n",
      "  'description': 'Qwen2.5 72B is the latest series of Qwen large language '\n",
      "                 'models. Qwen2.5 brings the following improvements upon '\n",
      "                 'Qwen2:\\n'\n",
      "                 '\\n'\n",
      "                 '- Significantly more knowledge and has greatly improved '\n",
      "                 'capabilities in coding and mathematics, thanks to our '\n",
      "                 'specialized expert models in these domains.\\n'\n",
      "                 '\\n'\n",
      "                 '- Significant improvements in instruction following, '\n",
      "                 'generating long texts (over 8K tokens), understanding '\n",
      "                 'structured data (e.g, tables), and generating structured '\n",
      "                 'outputs especially JSON. More resilient to the diversity of '\n",
      "                 'system prompts, enhancing role-play implementation and '\n",
      "                 'condition-setting for chatbots.\\n'\n",
      "                 '\\n'\n",
      "                 '- Long-context Support up to 128K tokens and can generate up '\n",
      "                 'to 8K tokens.\\n'\n",
      "                 '\\n'\n",
      "                 '- Multilingual support for over 29 languages, including '\n",
      "                 'Chinese, English, French, Spanish, Portuguese, German, '\n",
      "                 'Italian, Russian, Japanese, Korean, Vietnamese, Thai, '\n",
      "                 'Arabic, and more.\\n'\n",
      "                 '\\n'\n",
      "                 'Usage of this model is subject to [Tongyi Qianwen LICENSE '\n",
      "                 'AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).',\n",
      "  'hugging_face_id': 'Qwen/Qwen2.5-72B-Instruct',\n",
      "  'id': 'qwen/qwen-2.5-72b-instruct:free',\n",
      "  'name': 'Qwen2.5 72B Instruct (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'chatml',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'qwen/qwen-2.5-72b-instruct',\n",
      "  'context_length': 32768,\n",
      "  'created': 1726704000,\n",
      "  'description': 'Qwen2.5 72B is the latest series of Qwen large language '\n",
      "                 'models. Qwen2.5 brings the following improvements upon '\n",
      "                 'Qwen2:\\n'\n",
      "                 '\\n'\n",
      "                 '- Significantly more knowledge and has greatly improved '\n",
      "                 'capabilities in coding and mathematics, thanks to our '\n",
      "                 'specialized expert models in these domains.\\n'\n",
      "                 '\\n'\n",
      "                 '- Significant improvements in instruction following, '\n",
      "                 'generating long texts (over 8K tokens), understanding '\n",
      "                 'structured data (e.g, tables), and generating structured '\n",
      "                 'outputs especially JSON. More resilient to the diversity of '\n",
      "                 'system prompts, enhancing role-play implementation and '\n",
      "                 'condition-setting for chatbots.\\n'\n",
      "                 '\\n'\n",
      "                 '- Long-context Support up to 128K tokens and can generate up '\n",
      "                 'to 8K tokens.\\n'\n",
      "                 '\\n'\n",
      "                 '- Multilingual support for over 29 languages, including '\n",
      "                 'Chinese, English, French, Spanish, Portuguese, German, '\n",
      "                 'Italian, Russian, Japanese, Korean, Vietnamese, Thai, '\n",
      "                 'Arabic, and more.\\n'\n",
      "                 '\\n'\n",
      "                 'Usage of this model is subject to [Tongyi Qianwen LICENSE '\n",
      "                 'AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).',\n",
      "  'hugging_face_id': 'Qwen/Qwen2.5-72B-Instruct',\n",
      "  'id': 'qwen/qwen-2.5-72b-instruct',\n",
      "  'name': 'Qwen2.5 72B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000000207424',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000000518308',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/pixtral-12b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1725926400,\n",
      "  'description': 'The first multi-modal, text+image-to-text model from Mistral '\n",
      "                 'AI. Its weights were launched via torrent: '\n",
      "                 'https://x.com/mistralai/status/1833758285167722836.',\n",
      "  'hugging_face_id': 'mistralai/Pixtral-12B-2409',\n",
      "  'id': 'mistralai/pixtral-12b',\n",
      "  'name': 'Mistral: Pixtral 12B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000001',\n",
      "              'image': '0.0001445',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Qwen'},\n",
      "  'canonical_slug': 'qwen/qwen-2-vl-7b-instruct',\n",
      "  'context_length': 32768,\n",
      "  'created': 1724803200,\n",
      "  'description': 'Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with '\n",
      "                 'the following key enhancements:\\n'\n",
      "                 '\\n'\n",
      "                 '- SoTA understanding of images of various resolution & '\n",
      "                 'ratio: Qwen2.5-VL achieves state-of-the-art performance on '\n",
      "                 'visual understanding benchmarks, including MathVista, '\n",
      "                 'DocVQA, RealWorldQA, MTVQA, etc.\\n'\n",
      "                 '\\n'\n",
      "                 '- Understanding videos of 20min+: Qwen2.5-VL can understand '\n",
      "                 'videos over 20 minutes for high-quality video-based question '\n",
      "                 'answering, dialog, content creation, etc.\\n'\n",
      "                 '\\n'\n",
      "                 '- Agent that can operate your mobiles, robots, etc.: with '\n",
      "                 'the abilities of complex reasoning and decision making, '\n",
      "                 'Qwen2.5-VL can be integrated with devices like mobile '\n",
      "                 'phones, robots, etc., for automatic operation based on '\n",
      "                 'visual environment and text instructions.\\n'\n",
      "                 '\\n'\n",
      "                 '- Multilingual Support: to serve global users, besides '\n",
      "                 'English and Chinese, Qwen2.5-VL now supports the '\n",
      "                 'understanding of texts in different languages inside images, '\n",
      "                 'including most European languages, Japanese, Korean, Arabic, '\n",
      "                 'Vietnamese, etc.\\n'\n",
      "                 '\\n'\n",
      "                 'For more details, see this [blog '\n",
      "                 'post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub '\n",
      "                 'repo](https://github.com/QwenLM/Qwen2-VL).\\n'\n",
      "                 '\\n'\n",
      "                 'Usage of this model is subject to [Tongyi Qianwen LICENSE '\n",
      "                 'AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).',\n",
      "  'hugging_face_id': 'Qwen/Qwen2.5-VL-7B-Instruct',\n",
      "  'id': 'qwen/qwen-2.5-vl-7b-instruct',\n",
      "  'name': 'Qwen: Qwen2.5-VL 7B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000002',\n",
      "              'image': '0.0001445',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000002',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'chatml',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'nousresearch/hermes-3-llama-3.1-70b',\n",
      "  'context_length': 131072,\n",
      "  'created': 1723939200,\n",
      "  'description': 'Hermes 3 is a generalist language model with many '\n",
      "                 'improvements over [Hermes '\n",
      "                 '2](/models/nousresearch/nous-hermes-2-mistral-7b-dpo), '\n",
      "                 'including advanced agentic capabilities, much better '\n",
      "                 'roleplaying, reasoning, multi-turn conversation, long '\n",
      "                 'context coherence, and improvements across the board.\\n'\n",
      "                 '\\n'\n",
      "                 'Hermes 3 70B is a competitive, if not superior finetune of '\n",
      "                 'the [Llama-3.1 70B foundation '\n",
      "                 'model](/models/meta-llama/llama-3.1-70b-instruct), focused '\n",
      "                 'on aligning LLMs to the user, with powerful steering '\n",
      "                 'capabilities and control given to the end user.\\n'\n",
      "                 '\\n'\n",
      "                 'The Hermes 3 series builds and expands on the Hermes 2 set '\n",
      "                 'of capabilities, including more powerful and reliable '\n",
      "                 'function calling and structured output capabilities, '\n",
      "                 'generalist assistant capabilities, and improved code '\n",
      "                 'generation skills.',\n",
      "  'hugging_face_id': 'NousResearch/Hermes-3-Llama-3.1-70B',\n",
      "  'id': 'nousresearch/hermes-3-llama-3.1-70b',\n",
      "  'name': 'Nous: Hermes 3 70B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000028',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'chatml',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'nousresearch/hermes-3-llama-3.1-405b',\n",
      "  'context_length': 131072,\n",
      "  'created': 1723766400,\n",
      "  'description': 'Hermes 3 is a generalist language model with many '\n",
      "                 'improvements over Hermes 2, including advanced agentic '\n",
      "                 'capabilities, much better roleplaying, reasoning, multi-turn '\n",
      "                 'conversation, long context coherence, and improvements '\n",
      "                 'across the board.\\n'\n",
      "                 '\\n'\n",
      "                 'Hermes 3 405B is a frontier-level, full-parameter finetune '\n",
      "                 'of the Llama-3.1 405B foundation model, focused on aligning '\n",
      "                 'LLMs to the user, with powerful steering capabilities and '\n",
      "                 'control given to the end user.\\n'\n",
      "                 '\\n'\n",
      "                 'The Hermes 3 series builds and expands on the Hermes 2 set '\n",
      "                 'of capabilities, including more powerful and reliable '\n",
      "                 'function calling and structured output capabilities, '\n",
      "                 'generalist assistant capabilities, and improved code '\n",
      "                 'generation skills.\\n'\n",
      "                 '\\n'\n",
      "                 'Hermes 3 is competitive, if not superior, to Llama-3.1 '\n",
      "                 'Instruct models at general capabilities, with varying '\n",
      "                 'strengths and weaknesses attributable between the two.',\n",
      "  'hugging_face_id': 'NousResearch/Hermes-3-Llama-3.1-405B',\n",
      "  'id': 'nousresearch/hermes-3-llama-3.1-405b',\n",
      "  'name': 'Nous: Hermes 3 405B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000008',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000007',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/chatgpt-4o-latest',\n",
      "  'context_length': 128000,\n",
      "  'created': 1723593600,\n",
      "  'description': 'OpenAI ChatGPT 4o is continually updated by OpenAI to point '\n",
      "                 'to the current version of GPT-4o used by ChatGPT. It '\n",
      "                 'therefore differs slightly from the API version of '\n",
      "                 '[GPT-4o](/models/openai/gpt-4o) in that it has additional '\n",
      "                 'RLHF. It is intended for research and evaluation.\\n'\n",
      "                 '\\n'\n",
      "                 'OpenAI notes that this model is not suited for production '\n",
      "                 'use-cases as it may be removed or redirected to another '\n",
      "                 'model in the future.',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/chatgpt-4o-latest',\n",
      "  'name': 'OpenAI: ChatGPT-4o',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000015',\n",
      "              'image': '0.007225',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000005',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text', 'image', 'file'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4o-2024-08-06',\n",
      "  'context_length': 128000,\n",
      "  'created': 1722902400,\n",
      "  'description': 'The 2024-08-06 version of GPT-4o offers improved performance '\n",
      "                 'in structured outputs, with the ability to supply a JSON '\n",
      "                 'schema in the respone_format. Read more '\n",
      "                 '[here](https://openai.com/index/introducing-structured-outputs-in-the-api/).\\n'\n",
      "                 '\\n'\n",
      "                 'GPT-4o (\"o\" for \"omni\") is OpenAI\\'s latest AI model, '\n",
      "                 'supporting both text and image inputs with text outputs. It '\n",
      "                 'maintains the intelligence level of [GPT-4 '\n",
      "                 'Turbo](/models/openai/gpt-4-turbo) while being twice as fast '\n",
      "                 'and 50% more cost-effective. GPT-4o also offers improved '\n",
      "                 'performance in processing non-English languages and enhanced '\n",
      "                 'visual capabilities.\\n'\n",
      "                 '\\n'\n",
      "                 'For benchmarking against other models, it was briefly called '\n",
      "                 '[\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-4o-2024-08-06',\n",
      "  'name': 'OpenAI: GPT-4o (2024-08-06)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00001',\n",
      "              'image': '0.003613',\n",
      "              'input_cache_read': '0.00000125',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000025',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p',\n",
      "                           'web_search_options'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'none',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'meta-llama/llama-3.1-405b',\n",
      "  'context_length': 32768,\n",
      "  'created': 1722556800,\n",
      "  'description': \"Meta's latest class of model (Llama 3.1) launched with a \"\n",
      "                 'variety of sizes & flavors. This is the base 405B '\n",
      "                 'pre-trained version.\\n'\n",
      "                 '\\n'\n",
      "                 'It has demonstrated strong performance compared to leading '\n",
      "                 'closed-source models in human evaluations.\\n'\n",
      "                 '\\n'\n",
      "                 'To read more about the model release, [click '\n",
      "                 'here](https://ai.meta.com/blog/meta-llama-3/). Usage of this '\n",
      "                 \"model is subject to [Meta's Acceptable Use \"\n",
      "                 'Policy](https://llama.meta.com/llama3/use-policy/).',\n",
      "  'hugging_face_id': 'meta-llama/llama-3.1-405B',\n",
      "  'id': 'meta-llama/llama-3.1-405b',\n",
      "  'name': 'Meta: Llama 3.1 405B (base)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000002',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000002',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'llama3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'meta-llama/llama-3.1-8b-instruct',\n",
      "  'context_length': 131072,\n",
      "  'created': 1721692800,\n",
      "  'description': \"Meta's latest class of model (Llama 3.1) launched with a \"\n",
      "                 'variety of sizes & flavors. This 8B instruct-tuned version '\n",
      "                 'is fast and efficient.\\n'\n",
      "                 '\\n'\n",
      "                 'It has demonstrated strong performance compared to leading '\n",
      "                 'closed-source models in human evaluations.\\n'\n",
      "                 '\\n'\n",
      "                 'To read more about the model release, [click '\n",
      "                 'here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of '\n",
      "                 \"this model is subject to [Meta's Acceptable Use \"\n",
      "                 'Policy](https://llama.meta.com/llama3/use-policy/).',\n",
      "  'hugging_face_id': 'meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
      "  'id': 'meta-llama/llama-3.1-8b-instruct',\n",
      "  'name': 'Meta: Llama 3.1 8B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000002',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000000015',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'llama3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'meta-llama/llama-3.1-405b-instruct',\n",
      "  'context_length': 32768,\n",
      "  'created': 1721692800,\n",
      "  'description': 'The highly anticipated 400B class of Llama3 is here! '\n",
      "                 'Clocking in at 128k context with impressive eval scores, the '\n",
      "                 'Meta AI team continues to push the frontier of open-source '\n",
      "                 'LLMs.\\n'\n",
      "                 '\\n'\n",
      "                 \"Meta's latest class of model (Llama 3.1) launched with a \"\n",
      "                 'variety of sizes & flavors. This 405B instruct-tuned version '\n",
      "                 'is optimized for high quality dialogue usecases.\\n'\n",
      "                 '\\n'\n",
      "                 'It has demonstrated strong performance compared to leading '\n",
      "                 'closed-source models including GPT-4o and Claude 3.5 Sonnet '\n",
      "                 'in evaluations.\\n'\n",
      "                 '\\n'\n",
      "                 'To read more about the model release, [click '\n",
      "                 'here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of '\n",
      "                 \"this model is subject to [Meta's Acceptable Use \"\n",
      "                 'Policy](https://llama.meta.com/llama3/use-policy/).',\n",
      "  'hugging_face_id': 'meta-llama/Meta-Llama-3.1-405B-Instruct',\n",
      "  'id': 'meta-llama/llama-3.1-405b-instruct',\n",
      "  'name': 'Meta: Llama 3.1 405B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000008',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000008',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 32768,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'llama3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'meta-llama/llama-3.1-70b-instruct',\n",
      "  'context_length': 131072,\n",
      "  'created': 1721692800,\n",
      "  'description': \"Meta's latest class of model (Llama 3.1) launched with a \"\n",
      "                 'variety of sizes & flavors. This 70B instruct-tuned version '\n",
      "                 'is optimized for high quality dialogue usecases.\\n'\n",
      "                 '\\n'\n",
      "                 'It has demonstrated strong performance compared to leading '\n",
      "                 'closed-source models in human evaluations.\\n'\n",
      "                 '\\n'\n",
      "                 'To read more about the model release, [click '\n",
      "                 'here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of '\n",
      "                 \"this model is subject to [Meta's Acceptable Use \"\n",
      "                 'Policy](https://llama.meta.com/llama3/use-policy/).',\n",
      "  'hugging_face_id': 'meta-llama/Meta-Llama-3.1-70B-Instruct',\n",
      "  'id': 'meta-llama/llama-3.1-70b-instruct',\n",
      "  'name': 'Meta: Llama 3.1 70B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000028',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'mistral',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/mistral-nemo',\n",
      "  'context_length': 131072,\n",
      "  'created': 1721347200,\n",
      "  'description': 'A 12B parameter model with a 128k token context length built '\n",
      "                 'by Mistral in collaboration with NVIDIA.\\n'\n",
      "                 '\\n'\n",
      "                 'The model is multilingual, supporting English, French, '\n",
      "                 'German, Spanish, Italian, Portuguese, Chinese, Japanese, '\n",
      "                 'Korean, Arabic, and Hindi.\\n'\n",
      "                 '\\n'\n",
      "                 'It supports function calling and is released under the '\n",
      "                 'Apache 2.0 license.',\n",
      "  'hugging_face_id': 'mistralai/Mistral-Nemo-Instruct-2407',\n",
      "  'id': 'mistralai/mistral-nemo:free',\n",
      "  'name': 'Mistral: Mistral Nemo (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 128000}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'mistral',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/mistral-nemo',\n",
      "  'context_length': 131072,\n",
      "  'created': 1721347200,\n",
      "  'description': 'A 12B parameter model with a 128k token context length built '\n",
      "                 'by Mistral in collaboration with NVIDIA.\\n'\n",
      "                 '\\n'\n",
      "                 'The model is multilingual, supporting English, French, '\n",
      "                 'German, Spanish, Italian, Portuguese, Chinese, Japanese, '\n",
      "                 'Korean, Arabic, and Hindi.\\n'\n",
      "                 '\\n'\n",
      "                 'It supports function calling and is released under the '\n",
      "                 'Apache 2.0 license.',\n",
      "  'hugging_face_id': 'mistralai/Mistral-Nemo-Instruct-2407',\n",
      "  'id': 'mistralai/mistral-nemo',\n",
      "  'name': 'Mistral: Mistral Nemo',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000400032',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 128000}},\n",
      " {'architecture': {'input_modalities': ['text', 'image', 'file'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4o-mini',\n",
      "  'context_length': 128000,\n",
      "  'created': 1721260800,\n",
      "  'description': \"GPT-4o mini is OpenAI's newest model after [GPT-4 \"\n",
      "                 'Omni](/models/openai/gpt-4o), supporting both text and image '\n",
      "                 'inputs with text outputs.\\n'\n",
      "                 '\\n'\n",
      "                 'As their most advanced small model, it is many multiples '\n",
      "                 'more affordable than other recent frontier models, and more '\n",
      "                 'than 60% cheaper than [GPT-3.5 '\n",
      "                 'Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA '\n",
      "                 'intelligence, while being significantly more '\n",
      "                 'cost-effective.\\n'\n",
      "                 '\\n'\n",
      "                 'GPT-4o mini achieves an 82% score on MMLU and presently '\n",
      "                 'ranks higher than GPT-4 on chat preferences [common '\n",
      "                 'leaderboards](https://arena.lmsys.org/).\\n'\n",
      "                 '\\n'\n",
      "                 'Check out the [launch '\n",
      "                 'announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) '\n",
      "                 'to learn more.\\n'\n",
      "                 '\\n'\n",
      "                 '#multimodal',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-4o-mini',\n",
      "  'name': 'OpenAI: GPT-4o-mini',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000006',\n",
      "              'image': '0.000217',\n",
      "              'input_cache_read': '0.000000075',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000015',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p',\n",
      "                           'web_search_options'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text', 'image', 'file'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4o-mini-2024-07-18',\n",
      "  'context_length': 128000,\n",
      "  'created': 1721260800,\n",
      "  'description': \"GPT-4o mini is OpenAI's newest model after [GPT-4 \"\n",
      "                 'Omni](/models/openai/gpt-4o), supporting both text and image '\n",
      "                 'inputs with text outputs.\\n'\n",
      "                 '\\n'\n",
      "                 'As their most advanced small model, it is many multiples '\n",
      "                 'more affordable than other recent frontier models, and more '\n",
      "                 'than 60% cheaper than [GPT-3.5 '\n",
      "                 'Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA '\n",
      "                 'intelligence, while being significantly more '\n",
      "                 'cost-effective.\\n'\n",
      "                 '\\n'\n",
      "                 'GPT-4o mini achieves an 82% score on MMLU and presently '\n",
      "                 'ranks higher than GPT-4 on chat preferences [common '\n",
      "                 'leaderboards](https://arena.lmsys.org/).\\n'\n",
      "                 '\\n'\n",
      "                 'Check out the [launch '\n",
      "                 'announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) '\n",
      "                 'to learn more.\\n'\n",
      "                 '\\n'\n",
      "                 '#multimodal',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-4o-mini-2024-07-18',\n",
      "  'name': 'OpenAI: GPT-4o-mini (2024-07-18)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000006',\n",
      "              'image': '0.007225',\n",
      "              'input_cache_read': '0.000000075',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000015',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p',\n",
      "                           'web_search_options'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'gemma',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Gemini'},\n",
      "  'canonical_slug': 'google/gemma-2-9b-it',\n",
      "  'context_length': 8192,\n",
      "  'created': 1719532800,\n",
      "  'description': 'Gemma 2 9B by Google is an advanced, open-source language '\n",
      "                 'model that sets a new standard for efficiency and '\n",
      "                 'performance in its size class.\\n'\n",
      "                 '\\n'\n",
      "                 'Designed for a wide variety of tasks, it empowers developers '\n",
      "                 'and researchers to build innovative applications, while '\n",
      "                 'maintaining accessibility, safety, and cost-effectiveness.\\n'\n",
      "                 '\\n'\n",
      "                 'See the [launch '\n",
      "                 'announcement](https://blog.google/technology/developers/google-gemma-2/) '\n",
      "                 \"for more details. Usage of Gemma is subject to Google's \"\n",
      "                 '[Gemma Terms of Use](https://ai.google.dev/gemma/terms).',\n",
      "  'hugging_face_id': 'google/gemma-2-9b-it',\n",
      "  'id': 'google/gemma-2-9b-it:free',\n",
      "  'name': 'Google: Gemma 2 9B (free)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 8192,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 8192}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'gemma',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Gemini'},\n",
      "  'canonical_slug': 'google/gemma-2-9b-it',\n",
      "  'context_length': 8192,\n",
      "  'created': 1719532800,\n",
      "  'description': 'Gemma 2 9B by Google is an advanced, open-source language '\n",
      "                 'model that sets a new standard for efficiency and '\n",
      "                 'performance in its size class.\\n'\n",
      "                 '\\n'\n",
      "                 'Designed for a wide variety of tasks, it empowers developers '\n",
      "                 'and researchers to build innovative applications, while '\n",
      "                 'maintaining accessibility, safety, and cost-effectiveness.\\n'\n",
      "                 '\\n'\n",
      "                 'See the [launch '\n",
      "                 'announcement](https://blog.google/technology/developers/google-gemma-2/) '\n",
      "                 \"for more details. Usage of Gemma is subject to Google's \"\n",
      "                 '[Gemma Terms of Use](https://ai.google.dev/gemma/terms).',\n",
      "  'hugging_face_id': 'google/gemma-2-9b-it',\n",
      "  'id': 'google/gemma-2-9b-it',\n",
      "  'name': 'Google: Gemma 2 9B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000000100008',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00000001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 8192,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 8192}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'chatml',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'nousresearch/hermes-2-pro-llama-3-8b',\n",
      "  'context_length': 131072,\n",
      "  'created': 1716768000,\n",
      "  'description': 'Hermes 2 Pro is an upgraded, retrained version of Nous '\n",
      "                 'Hermes 2, consisting of an updated and cleaned version of '\n",
      "                 'the OpenHermes 2.5 Dataset, as well as a newly introduced '\n",
      "                 'Function Calling and JSON Mode dataset developed in-house.',\n",
      "  'hugging_face_id': 'NousResearch/Hermes-2-Pro-Llama-3-8B',\n",
      "  'id': 'nousresearch/hermes-2-pro-llama-3-8b',\n",
      "  'name': 'NousResearch: Hermes 2 Pro - Llama-3 8B',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00000004',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000000025',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 131072,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 131072}},\n",
      " {'architecture': {'input_modalities': ['text', 'image', 'file'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4o',\n",
      "  'context_length': 128000,\n",
      "  'created': 1715558400,\n",
      "  'description': 'GPT-4o (\"o\" for \"omni\") is OpenAI\\'s latest AI model, '\n",
      "                 'supporting both text and image inputs with text outputs. It '\n",
      "                 'maintains the intelligence level of [GPT-4 '\n",
      "                 'Turbo](/models/openai/gpt-4-turbo) while being twice as fast '\n",
      "                 'and 50% more cost-effective. GPT-4o also offers improved '\n",
      "                 'performance in processing non-English languages and enhanced '\n",
      "                 'visual capabilities.\\n'\n",
      "                 '\\n'\n",
      "                 'For benchmarking against other models, it was briefly called '\n",
      "                 '[\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\\n'\n",
      "                 '\\n'\n",
      "                 '#multimodal',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-4o',\n",
      "  'name': 'OpenAI: GPT-4o',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00001',\n",
      "              'image': '0.003613',\n",
      "              'input_cache_read': '0.00000125',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000025',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p',\n",
      "                           'web_search_options'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text', 'image', 'file'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4o',\n",
      "  'context_length': 128000,\n",
      "  'created': 1715558400,\n",
      "  'description': 'GPT-4o (\"o\" for \"omni\") is OpenAI\\'s latest AI model, '\n",
      "                 'supporting both text and image inputs with text outputs. It '\n",
      "                 'maintains the intelligence level of [GPT-4 '\n",
      "                 'Turbo](/models/openai/gpt-4-turbo) while being twice as fast '\n",
      "                 'and 50% more cost-effective. GPT-4o also offers improved '\n",
      "                 'performance in processing non-English languages and enhanced '\n",
      "                 'visual capabilities.\\n'\n",
      "                 '\\n'\n",
      "                 'For benchmarking against other models, it was briefly called '\n",
      "                 '[\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\\n'\n",
      "                 '\\n'\n",
      "                 '#multimodal',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-4o:extended',\n",
      "  'name': 'OpenAI: GPT-4o (extended)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000018',\n",
      "              'image': '0.007225',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000006',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p',\n",
      "                           'web_search_options'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 64000}},\n",
      " {'architecture': {'input_modalities': ['text', 'image', 'file'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4o-2024-05-13',\n",
      "  'context_length': 128000,\n",
      "  'created': 1715558400,\n",
      "  'description': 'GPT-4o (\"o\" for \"omni\") is OpenAI\\'s latest AI model, '\n",
      "                 'supporting both text and image inputs with text outputs. It '\n",
      "                 'maintains the intelligence level of [GPT-4 '\n",
      "                 'Turbo](/models/openai/gpt-4-turbo) while being twice as fast '\n",
      "                 'and 50% more cost-effective. GPT-4o also offers improved '\n",
      "                 'performance in processing non-English languages and enhanced '\n",
      "                 'visual capabilities.\\n'\n",
      "                 '\\n'\n",
      "                 'For benchmarking against other models, it was briefly called '\n",
      "                 '[\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\\n'\n",
      "                 '\\n'\n",
      "                 '#multimodal',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-4o-2024-05-13',\n",
      "  'name': 'OpenAI: GPT-4o (2024-05-13)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000015',\n",
      "              'image': '0.007225',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000005',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p',\n",
      "                           'web_search_options'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 4096}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'llama3',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Llama3'},\n",
      "  'canonical_slug': 'meta-llama/llama-3-70b-instruct',\n",
      "  'context_length': 8192,\n",
      "  'created': 1713398400,\n",
      "  'description': \"Meta's latest class of model (Llama 3) launched with a \"\n",
      "                 'variety of sizes & flavors. This 70B instruct-tuned version '\n",
      "                 'was optimized for high quality dialogue usecases.\\n'\n",
      "                 '\\n'\n",
      "                 'It has demonstrated strong performance compared to leading '\n",
      "                 'closed-source models in human evaluations.\\n'\n",
      "                 '\\n'\n",
      "                 'To read more about the model release, [click '\n",
      "                 'here](https://ai.meta.com/blog/meta-llama-3/). Usage of this '\n",
      "                 \"model is subject to [Meta's Acceptable Use \"\n",
      "                 'Policy](https://llama.meta.com/llama3/use-policy/).',\n",
      "  'hugging_face_id': 'meta-llama/Meta-Llama-3-70B-Instruct',\n",
      "  'id': 'meta-llama/llama-3-70b-instruct',\n",
      "  'name': 'Meta: Llama 3 70B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000004',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000003',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'min_p',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 8192,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 16384}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'mistral',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'Mistral'},\n",
      "  'canonical_slug': 'mistralai/mixtral-8x22b-instruct',\n",
      "  'context_length': 65536,\n",
      "  'created': 1713312000,\n",
      "  'description': \"Mistral's official instruct fine-tuned version of [Mixtral \"\n",
      "                 '8x22B](/models/mistralai/mixtral-8x22b). It uses 39B active '\n",
      "                 'parameters out of 141B, offering unparalleled cost '\n",
      "                 'efficiency for its size. Its strengths include:\\n'\n",
      "                 '- strong math, coding, and reasoning\\n'\n",
      "                 '- large context length (64k)\\n'\n",
      "                 '- fluency in English, French, Italian, German, and Spanish\\n'\n",
      "                 '\\n'\n",
      "                 'See benchmarks on the launch announcement '\n",
      "                 '[here](https://mistral.ai/news/mixtral-8x22b/).\\n'\n",
      "                 '#moe',\n",
      "  'hugging_face_id': 'mistralai/Mixtral-8x22B-Instruct-v0.1',\n",
      "  'id': 'mistralai/mixtral-8x22b-instruct',\n",
      "  'name': 'Mistral: Mixtral 8x22B Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000009',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000009',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'repetition_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_k',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 65536,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': None}},\n",
      " {'architecture': {'input_modalities': ['text', 'image'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text+image->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4-turbo',\n",
      "  'context_length': 128000,\n",
      "  'created': 1712620800,\n",
      "  'description': 'The latest GPT-4 Turbo model with vision capabilities. '\n",
      "                 'Vision requests can now use JSON mode and function calling.\\n'\n",
      "                 '\\n'\n",
      "                 'Training data: up to December 2023.',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-4-turbo',\n",
      "  'name': 'OpenAI: GPT-4 Turbo',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00003',\n",
      "              'image': '0.01445',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 4096}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-3.5-turbo-0613',\n",
      "  'context_length': 4095,\n",
      "  'created': 1706140800,\n",
      "  'description': \"GPT-3.5 Turbo is OpenAI's fastest model. It can understand \"\n",
      "                 'and generate natural language or code, and is optimized for '\n",
      "                 'chat and traditional completion tasks.\\n'\n",
      "                 '\\n'\n",
      "                 'Training data up to Sep 2021.',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-3.5-turbo-0613',\n",
      "  'name': 'OpenAI: GPT-3.5 Turbo (older v0613)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000002',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 4095,\n",
      "                   'is_moderated': False,\n",
      "                   'max_completion_tokens': 4096}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4-turbo-preview',\n",
      "  'context_length': 128000,\n",
      "  'created': 1706140800,\n",
      "  'description': 'The preview GPT-4 model with improved instruction following, '\n",
      "                 'JSON mode, reproducible outputs, parallel function calling, '\n",
      "                 'and more. Training data: up to Dec 2023.\\n'\n",
      "                 '\\n'\n",
      "                 '**Note:** heavily rate limited by OpenAI while in preview.',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-4-turbo-preview',\n",
      "  'name': 'OpenAI: GPT-4 Turbo Preview',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00003',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 4096}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4-1106-preview',\n",
      "  'context_length': 128000,\n",
      "  'created': 1699228800,\n",
      "  'description': 'The latest GPT-4 Turbo model with vision capabilities. '\n",
      "                 'Vision requests can now use JSON mode and function calling.\\n'\n",
      "                 '\\n'\n",
      "                 'Training data: up to April 2023.',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-4-1106-preview',\n",
      "  'name': 'OpenAI: GPT-4 Turbo (older v1106)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00003',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00001',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 128000,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 4096}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': 'chatml',\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-3.5-turbo-instruct',\n",
      "  'context_length': 4095,\n",
      "  'created': 1695859200,\n",
      "  'description': 'This model is a variant of GPT-3.5 Turbo tuned for '\n",
      "                 'instructional prompts and omitting chat-related '\n",
      "                 'optimizations. Training data: up to Sep 2021.',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-3.5-turbo-instruct',\n",
      "  'name': 'OpenAI: GPT-3.5 Turbo Instruct',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000002',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000015',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 4095,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 4096}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-3.5-turbo-16k',\n",
      "  'context_length': 16385,\n",
      "  'created': 1693180800,\n",
      "  'description': 'This model offers four times the context length of '\n",
      "                 'gpt-3.5-turbo, allowing it to support approximately 20 pages '\n",
      "                 'of text in a single request at a higher cost. Training data: '\n",
      "                 'up to Sep 2021.',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-3.5-turbo-16k',\n",
      "  'name': 'OpenAI: GPT-3.5 Turbo 16k',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.000004',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.000003',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 16385,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 4096}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-3.5-turbo',\n",
      "  'context_length': 16385,\n",
      "  'created': 1685232000,\n",
      "  'description': \"GPT-3.5 Turbo is OpenAI's fastest model. It can understand \"\n",
      "                 'and generate natural language or code, and is optimized for '\n",
      "                 'chat and traditional completion tasks.\\n'\n",
      "                 '\\n'\n",
      "                 'Training data up to Sep 2021.',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-3.5-turbo',\n",
      "  'name': 'OpenAI: GPT-3.5 Turbo',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.0000015',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.0000005',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 16385,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 4096}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4',\n",
      "  'context_length': 8191,\n",
      "  'created': 1685232000,\n",
      "  'description': \"OpenAI's flagship model, GPT-4 is a large-scale multimodal \"\n",
      "                 'language model capable of solving difficult problems with '\n",
      "                 'greater accuracy than previous models due to its broader '\n",
      "                 'general knowledge and advanced reasoning capabilities. '\n",
      "                 'Training data: up to Sep 2021.',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-4',\n",
      "  'name': 'OpenAI: GPT-4',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00006',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00003',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 8191,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 4096}},\n",
      " {'architecture': {'input_modalities': ['text'],\n",
      "                   'instruct_type': None,\n",
      "                   'modality': 'text->text',\n",
      "                   'output_modalities': ['text'],\n",
      "                   'tokenizer': 'GPT'},\n",
      "  'canonical_slug': 'openai/gpt-4-0314',\n",
      "  'context_length': 8191,\n",
      "  'created': 1685232000,\n",
      "  'description': 'GPT-4-0314 is the first version of GPT-4 released, with a '\n",
      "                 'context length of 8,192 tokens, and was supported until June '\n",
      "                 '14. Training data: up to Sep 2021.',\n",
      "  'hugging_face_id': None,\n",
      "  'id': 'openai/gpt-4-0314',\n",
      "  'name': 'OpenAI: GPT-4 (older v0314)',\n",
      "  'per_request_limits': None,\n",
      "  'pricing': {'completion': '0.00006',\n",
      "              'image': '0',\n",
      "              'internal_reasoning': '0',\n",
      "              'prompt': '0.00003',\n",
      "              'request': '0',\n",
      "              'web_search': '0'},\n",
      "  'supported_parameters': ['frequency_penalty',\n",
      "                           'logit_bias',\n",
      "                           'logprobs',\n",
      "                           'max_tokens',\n",
      "                           'presence_penalty',\n",
      "                           'response_format',\n",
      "                           'seed',\n",
      "                           'stop',\n",
      "                           'structured_outputs',\n",
      "                           'temperature',\n",
      "                           'tool_choice',\n",
      "                           'tools',\n",
      "                           'top_logprobs',\n",
      "                           'top_p'],\n",
      "  'top_provider': {'context_length': 8191,\n",
      "                   'is_moderated': True,\n",
      "                   'max_completion_tokens': 4096}}]\n"
     ]
    }
   ],
   "source": [
    "logprobs_models = [m for m in models if \"logprobs\" in m[\"supported_parameters\"]]\n",
    "not_pricy_models = [m for m in logprobs_models if m[\"pricing\"][\"prompt\"] < ]\n",
    "prompt\n",
    "completion\n",
    "pricing\n",
    "import pprint\n",
    "pprint.pprint(logprobs_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a9b1969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "print(len(logprobs_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17be22c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meaningful-transparency (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
