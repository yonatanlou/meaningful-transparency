{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6642d8a6",
   "metadata": {},
   "source": [
    "## LLM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7782631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "ANNOTATION_GLOB = \"prompts/annotation_guidelines\"\n",
    "OUT_CSV = \"antisemitism_placeholders_dataset.csv\"\n",
    "\n",
    "NUM_POSTS_PER_DEFINITION = 1\n",
    "TASK_PROMPT = f\"\"\"Generate {NUM_POSTS_PER_DEFINITION} social media post templates that is antisemitic. IMPORTANT, make sure this post complies with the definition below of antisemitism.\n",
    "First, read the definition carefully. Then, generate the post. \n",
    "keep the posts short (1-2 sentences), and make them as realistic as possible. \n",
    "The posts should be in English and should not contain any hashtags or mentions. Each post should be different in terms of content and style.\"\"\"\n",
    "\n",
    "\n",
    "def call_llm(messages: List[Dict[str, str]], provider: str, model: str) -> Dict:\n",
    "    if provider == \"grok\":\n",
    "        client = OpenAI(\n",
    "        api_key=os.getenv(\"GROK_API_KEY\"),\n",
    "        base_url=\"https://api.x.ai/v1\",\n",
    "        )\n",
    "    elif provider == \"openai\":\n",
    "        client = OpenAI()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.2,\n",
    "        messages=messages,\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    txt = resp.choices[0].message.content\n",
    "    return txt,resp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "def load_definitions(definitions_dir: str) -> Dict[str, str]:\n",
    "    \"\"\"Load antisemitism definitions from markdown files.\"\"\"\n",
    "    definitions = {}\n",
    "    definitions_path = Path(definitions_dir)\n",
    "    \n",
    "    if not definitions_path.exists():\n",
    "        raise FileNotFoundError(f\"Directory {definitions_dir} not found\")\n",
    "    \n",
    "    for md_file in definitions_path.glob(\"*.md\"):\n",
    "        definition_name = md_file.stem\n",
    "        with open(md_file, 'r', encoding='utf-8') as f:\n",
    "            definitions[definition_name] = f.read().strip()\n",
    "    return definitions\n",
    "\n",
    "\n",
    "annotations = load_definitions(ANNOTATION_GLOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2056208c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TweetID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Username",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreateDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Biased",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Keyword",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "c58a06fc-5e7a-4915-b583-51e3afcb8378",
       "rows": [
        [
         "0",
         "1228740093357092865",
         "Celtic_Films",
         "AIPAC should be registered as a foreign agent meddling in US elections.\n\nAmerican Israel Political Action Committee.\n\nIt is interfering in the US electoral process and should be put on trial and it's leaders imprisoned.  @benshapiro @charliekirk11 https://t.co/ebO4iPUah8",
         "2020-02-15 17:57:21+00:00",
         "1",
         "Israel"
        ],
        [
         "1",
         "1239547900012589056",
         "zariths__",
         "RT @qiss0rkid: go to israel pls , we don't need you",
         "2020-03-16 13:43:43+00:00",
         "1",
         "Israel"
        ],
        [
         "2",
         "1216559517887954945",
         "kelownascott",
         "The world, including Canada, has given Israel a green light to take Palestinian land, destroy Palestinian homes, and strengthen a system of #Apartheid. Shame on us!",
         "2020-01-13 03:16:06+00:00",
         "1",
         "Israel"
        ],
        [
         "3",
         "1217123508754534400",
         "KeishaJake",
         "These children avoid walking over the US &amp; Israel flags. Supposedly, this is a generation which was taught to hate America &amp; Israel, but it is the opposite!\nIranians seek freedom, peace &amp; a dignified life unlike the oppressive Iranian regime.\nWHY IS THE MEDIA PRO IRAN REGIME??? https://t.co/ym3Zt8jlfy",
         "2020-01-14 16:37:12+00:00",
         "0",
         "Israel"
        ],
        [
         "4",
         "1232258532273090560",
         "amit2nirvana",
         "RT @IDF: The last 24 hours in Israel: https://t.co/0YNwvMsYvL",
         "2020-02-25 10:58:23+00:00",
         "0",
         "Israel"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetID</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>CreateDate</th>\n",
       "      <th>Biased</th>\n",
       "      <th>Keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1228740093357092865</td>\n",
       "      <td>Celtic_Films</td>\n",
       "      <td>AIPAC should be registered as a foreign agent ...</td>\n",
       "      <td>2020-02-15 17:57:21+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1239547900012589056</td>\n",
       "      <td>zariths__</td>\n",
       "      <td>RT @qiss0rkid: go to israel pls , we don't nee...</td>\n",
       "      <td>2020-03-16 13:43:43+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1216559517887954945</td>\n",
       "      <td>kelownascott</td>\n",
       "      <td>The world, including Canada, has given Israel ...</td>\n",
       "      <td>2020-01-13 03:16:06+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1217123508754534400</td>\n",
       "      <td>KeishaJake</td>\n",
       "      <td>These children avoid walking over the US &amp;amp;...</td>\n",
       "      <td>2020-01-14 16:37:12+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1232258532273090560</td>\n",
       "      <td>amit2nirvana</td>\n",
       "      <td>RT @IDF: The last 24 hours in Israel: https://...</td>\n",
       "      <td>2020-02-25 10:58:23+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Israel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TweetID      Username  \\\n",
       "0  1228740093357092865  Celtic_Films   \n",
       "1  1239547900012589056     zariths__   \n",
       "2  1216559517887954945  kelownascott   \n",
       "3  1217123508754534400    KeishaJake   \n",
       "4  1232258532273090560  amit2nirvana   \n",
       "\n",
       "                                                Text  \\\n",
       "0  AIPAC should be registered as a foreign agent ...   \n",
       "1  RT @qiss0rkid: go to israel pls , we don't nee...   \n",
       "2  The world, including Canada, has given Israel ...   \n",
       "3  These children avoid walking over the US &amp;...   \n",
       "4  RT @IDF: The last 24 hours in Israel: https://...   \n",
       "\n",
       "                  CreateDate  Biased Keyword  \n",
       "0  2020-02-15 17:57:21+00:00       1  Israel  \n",
       "1  2020-03-16 13:43:43+00:00       1  Israel  \n",
       "2  2020-01-13 03:16:06+00:00       1  Israel  \n",
       "3  2020-01-14 16:37:12+00:00       0  Israel  \n",
       "4  2020-02-25 10:58:23+00:00       0  Israel  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\"\"\"\n",
    "‘TweetID’: Represents the tweet ID. \n",
    "\n",
    "‘Username’: Represents the username who published the tweet.  \n",
    "\n",
    "‘Text’: Represents the full text of the tweet (not pre-processed).\n",
    "\n",
    "‘CreateDate’: Represents the date the tweet was created.  \n",
    "\n",
    "‘Biased’: Represents the labeled by our annotations if the tweet is antisemitic or non-antisemitic. \n",
    "\n",
    "‘Keyword’: Represents the keyword that was used in the query. The keyword can be in the text, including mentioned names, or the username.  \n",
    "\n",
    "https://zenodo.org/records/7932888\n",
    " \n",
    "\"\"\"\n",
    "twitter_df = pd.read_csv(\"GoldStanderDataSet.csv\",encoding='cp1252')\n",
    "twitter_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99bca6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Keyword",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "77fdea5a-5a52-46a7-9cac-da88a6afed3d",
       "rows": [
        [
         "Jews",
         "4605"
        ],
        [
         "Israel",
         "1524"
        ],
        [
         "ZioNazi",
         "529"
        ],
        [
         "Kikes",
         "283"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 4
       }
      },
      "text/plain": [
       "Keyword\n",
       "Jews       4605\n",
       "Israel     1524\n",
       "ZioNazi     529\n",
       "Kikes       283\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df[\"Keyword\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d056c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TweetID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Username",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CreateDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Biased",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Keyword",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "395a9672-6b82-4736-bc77-3c197f710867",
       "rows": [
        [
         "6036",
         "1465147721724178432",
         "rulajebreal",
         "Herzog lit Hanukkah candles in the heart of the segregated regime, Hebron, where public roads are available only to Jews.\nHe bestowed his office’s legitimacy to flagrant Israeli apartheid, celebrating normalization of this darkness.\nHe is now President of The Violent Kahanists. https://t.co/p2teWXtp1W",
         "2021-11-29 02:36:52+00:00",
         "1",
         "Jews"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetID</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>CreateDate</th>\n",
       "      <th>Biased</th>\n",
       "      <th>Keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>1465147721724178432</td>\n",
       "      <td>rulajebreal</td>\n",
       "      <td>Herzog lit Hanukkah candles in the heart of th...</td>\n",
       "      <td>2021-11-29 02:36:52+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Jews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  TweetID     Username  \\\n",
       "6036  1465147721724178432  rulajebreal   \n",
       "\n",
       "                                                   Text  \\\n",
       "6036  Herzog lit Hanukkah candles in the heart of th...   \n",
       "\n",
       "                     CreateDate  Biased Keyword  \n",
       "6036  2021-11-29 02:36:52+00:00       1    Jews  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = twitter_df[twitter_df[\"Biased\"]==1].sample(1, random_state=42)\n",
    "samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ed4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations:\n",
      "{\n",
      "  \"Jerusalem Declaration\": \"# Annotation Guidelines for Identifying Antisemitism Based on the Jerusalem Declaration\\n\\n## 1. Task Description\\n\\n### Overview\\nThese guidelines are designed to help annotators identify antisemitic content based on the Jerusalem Declaration on Antisemitism (JDA). The JDA provides a clear, concise, and historically-informed definition that aims to strengthen the fight against antisemitism while protecting space for legitimate political discourse, particularly regarding Israel and Palestine.\\n\\n### Purpose\\nThe purpose of these annotations is to accurately identify content that constitutes discrimination, prejudice, hostility, or violence against Jews as Jews, while distinguishing this from legitimate criticism of states, policies, or political ideologies. These guidelines will help create consistent, nuanced assessments that recognize both the serious nature of antisemitism and the importance of protecting free political speech.\\n\\n### What You Will Do\\nYou will examine text passages and determine whether they contain antisemitic content according to the JDA definition. This involves identifying classical antisemitic tropes, recognizing coded language, understanding context, and distinguishing between antisemitism and legitimate political criticism, particularly regarding Israel-Palestine issues.\\n\\n## 2. Label Descriptions\\n\\n### **ANTISEMITIC**\\nContent that demonstrates discrimination, prejudice, hostility, or violence against Jews as Jews (or Jewish institutions as Jewish). This includes:\\n- Essentializing Jews or making sweeping negative generalizations\\n- Linking Jews to forces of evil or conspiracy theories\\n- Using classical antisemitic imagery or stereotypes\\n- Denying or minimizing the Holocaust\\n- Applying antisemitic symbols to Israel\\n- Holding Jews collectively responsible for Israel's actions\\n- Requiring Jews to condemn Israel/Zionism because they are Jewish\\n- Assuming dual loyalty of non-Israeli Jews\\n- Denying Jews' right to exist and flourish in Israel\\n\\n### **NOT_ANTISEMITIC**\\nContent that includes:\\n- Support for Palestinian rights and justice according to international law\\n- Criticism of Zionism as a form of nationalism\\n- Evidence-based criticism of Israeli state policies and practices\\n- Advocacy for various constitutional arrangements (one-state, two-state, etc.)\\n- Comparisons of Israel to other historical cases when evidence-based\\n- Boycott, divestment, and sanctions as forms of political protest\\n- Political speech that may be excessive or reflect double standards but doesn't invoke antisemitic tropes\\n\\n### **UNCLEAR**\\nContent where:\\n- The speaker's intent is ambiguous\\n- Coded language could have multiple interpretations\\n- Context is insufficient to determine if antisemitic tropes are being invoked\\n- The identity of the speaker and broader context would significantly impact interpretation\\n\\n## 3. Annotation Examples\\n\\n### Clear Examples of ANTISEMITIC Content\\n| Example Text | Reasoning |\\n|--------------|-----------|\\n| \\\"The Rothschilds control the world's banks\\\" | Classic conspiracy theory linking Jews to hidden financial power |\\n| \\\"Jews were behind COVID-19 to profit from vaccines\\\" | Links Jews to spreading disease and evil conspiracies |\\n| \\\"The Holocaust numbers have been greatly inflated\\\" | Holocaust minimization |\\n| \\\"All American Jews are more loyal to Israel than the US\\\" | Dual loyalty accusation |\\n| \\\"Jewish money controls our government\\\" | Conspiracy theory about hidden Jewish power |\\n| \\\"Israel is poisoning Palestinian wells, just like medieval Jews\\\" | Applying blood libel to Israel |\\n\\n### Clear Examples of NOT_ANTISEMITIC Content\\n| Example Text | Reasoning |\\n|--------------|-----------|\\n| \\\"Israel's settlement policy violates the Fourth Geneva Convention\\\" | Evidence-based criticism of state policy |\\n| \\\"Palestinians deserve full civil and political rights\\\" | Supporting human rights |\\n| \\\"BDS is a legitimate form of non-violent protest\\\" | Political protest method |\\n| \\\"Israel practices apartheid in the West Bank\\\" | Evidence-based comparison with citation |\\n| \\\"Zionism as implemented has displaced Palestinians\\\" | Historical criticism of political movement |\\n| \\\"A binational state with equal rights for all is the best solution\\\" | Political position on state arrangements |\\n\\n### Edge Cases and Ambiguous Examples\\n| Example Text | Label | Reasoning |\\n|--------------|-------|-----------|\\n| \\\"They control too much\\\" | UNCLEAR | Without context, \\\"they\\\" is ambiguous |\\n| \\\"Israel acts like a Nazi state\\\" | ANTISEMITIC/UNCLEAR | Depends on context - if systematic comparison with evidence vs. inflammatory rhetoric |\\n| \\\"Zionist lobby has too much influence\\\" | UNCLEAR | Could be legitimate political criticism or coded antisemitism depending on context |\\n| \\\"Questions need to be asked about historical events\\\" | UNCLEAR | Could be innocent inquiry or Holocaust denial |\\n\\n## 4. Handling Ambiguous Cases\\n\\n### Decision Tree for Ambiguous Content\\n\\n1. **Does the text reference Jews as a collective?**\\n   - Contains conspiracy theories or evil linkage → **ANTISEMITIC**\\n   - Makes factual or positive statements → **NOT_ANTISEMITIC**\\n   - Ambiguous generalization → Check for essentializing\\n\\n2. **Is Israel or Zionism criticized?**\\n   - Uses evidence-based arguments → **NOT_ANTISEMITIC**\\n   - Applies classical antisemitic tropes → **ANTISEMITIC**\\n   - Compares to other states/movements fairly → **NOT_ANTISEMITIC**\\n   - Singles out Israel uniquely without justification → Consider context\\n\\n3. **Is coded language present?**\\n   - \\\"Rothschilds,\\\" \\\"globalists,\\\" \\\"cosmopolitans\\\" in conspiratorial context → **ANTISEMITIC**\\n   - Technical financial/political terms → **NOT_ANTISEMITIC**\\n   - Ambiguous references → **UNCLEAR**\\n\\n### Context Considerations\\n- **Speaker identity**: A Palestinian describing personal experience vs. others making same statements\\n- **Pattern over time**: Single statement vs. pattern of statements\\n- **Emotional context**: Reaction to specific events vs. general statements\\n- **Intent indicators**: Academic analysis vs. inflammatory rhetoric\\n\\n## 5. Special Instructions\\n\\n### Coded Language\\nCommon antisemitic dog whistles include:\\n- \\\"Globalists\\\" or \\\"cosmopolitans\\\" (when implying Jewish conspiracy)\\n- \\\"Cultural Marxists\\\" (often coded reference to Jews)\\n- \\\"(((Triple parentheses)))\\\" around names\\n- References to \\\"puppet masters\\\" or \\\"hidden hands\\\"\\n- \\\"Khazars\\\" or \\\"fake Jews\\\" rhetoric\\n\\n### Cultural and Regional Differences\\n- Consider that criticism styles vary across cultures\\n- Historical contexts (e.g., post-colonial perspectives) matter\\n- Academic discourse may use different standards than public discourse\\n- Religious criticism requires special sensitivity to distinguish from ethnic prejudice\\n\\n## 6. Quality Checklist\\n\\nBefore finalizing your annotation, verify:\\n- [ ] Have you considered the full context available?\\n- [ ] Have you distinguished between criticism of Israel/Zionism and antisemitism?\\n- [ ] Have you checked for classical antisemitic tropes?\\n- [ ] Have you considered whether similar criticism would be applied to other states?\\n- [ ] Have you identified any coded language?\\n- [ ] Is your annotation consistent with the JDA definition?\\n\\n## 7. Common Mistakes to Avoid\\n\\n1. **Conflating all criticism of Israel with antisemitism** - Evidence-based criticism is protected political speech\\n2. **Missing coded antisemitism** - \\\"Globalist\\\" rhetoric often masks antisemitic conspiracy theories\\n3. **Ignoring context** - The same statement can have different meanings depending on speaker and situation\\n4. **Over-interpreting ambiguity** - When genuinely unclear, mark as UNCLEAR rather than guessing\\n5. **Applying different standards** - Use same criteria for Israel as for other states\\n6. **Confusing anti-Zionism with antisemitism** - Opposition to political ideology vs. hatred of Jews\\n7. **Missing Holocaust denial/distortion** - Even \\\"subtle\\\" questioning often indicates denialism\\n\\n## 8. Final Reminders\\n\\n- **The core definition is key**: Antisemitism targets Jews *as Jews*\\n- **Context matters significantly**: Consider speaker, audience, and broader discourse\\n- **Protect legitimate political speech**: Criticism, even harsh criticism, isn't automatically antisemitic\\n- **Recognize classical tropes**: Conspiracy theories, blood libel, and dual loyalty remain central to antisemitism\\n- **When uncertain, choose UNCLEAR**: Better to acknowledge ambiguity than make incorrect determinations\\n- **Apply consistent standards**: What's acceptable criticism of other states should be acceptable for Israel\\n- **Remember the dual purpose**: Fight antisemitism AND protect open political debate\\n\\nThe goal is to create accurate, consistent annotations that help identify genuine antisemitism while preserving space for legitimate political discourse and human rights advocacy.\",\n",
      "  \"IHRA Definition\": \"# Annotation Guidelines for Identifying Antisemitism Based on IHRA Working Definition\\n\\n## 1. Task Description\\n\\n### Overview\\nThese guidelines are designed to help annotators identify antisemitic content in text according to the International Holocaust Remembrance Alliance (IHRA) working definition of antisemitism, adopted on May 26, 2016. The IHRA definition provides a comprehensive framework for recognizing various forms of antisemitism in contemporary discourse.\\n\\n### Purpose\\nThe purpose of these annotations is to systematically identify content that perpetuates antisemitic views, stereotypes, or hatred toward Jewish people, while distinguishing such content from legitimate political criticism and factual discourse. This work supports efforts to combat antisemitism and promote understanding of how it manifests in various forms of communication.\\n\\n### What You Will Do\\nYou will examine text passages and assign one of three labels (ANTISEMITIC, NOT_ANTISEMITIC, or UNCLEAR) based on whether the content meets the IHRA criteria for antisemitism. Your analysis should consider the overall context and apply the specific examples and manifestations outlined in the IHRA definition.\\n\\n## 2. Label Descriptions\\n\\n### ANTISEMITIC\\nContent that contains one or more of the following elements as defined by the IHRA:\\n- Calls for, aids, or justifies killing or harming Jews in the name of radical ideology or extremist religious views\\n- Makes mendacious, dehumanizing, demonizing, or stereotypical allegations about Jews as a collective\\n- Promotes myths about Jewish world conspiracy or Jewish control of media, economy, government, or other institutions\\n- Accuses Jews collectively of wrongdoing committed by individuals\\n- Denies or distorts the Holocaust (fact, scope, mechanisms, or intentionality)\\n- Accuses Jews or Israel of inventing or exaggerating the Holocaust\\n- Questions Jewish citizens' loyalty (dual loyalty accusations)\\n- Denies Jewish people's right to self-determination\\n- Applies double standards to Israel not expected of other democratic nations\\n- Uses classic antisemitic symbols/images (blood libel, deicide) against Israel/Israelis\\n- Compares Israeli policy to Nazi actions\\n- Holds Jews collectively responsible for Israel's actions\\n\\n### NOT_ANTISEMITIC\\nContent that:\\n- Contains criticism of Israel similar to that leveled against any other country\\n- Discusses Jewish people, Judaism, or Israel in factual, neutral, or positive terms\\n- Engages in legitimate political discourse without employing antisemitic tropes\\n- Addresses Israeli policies or actions without targeting Jews as a collective\\n- Contains historical, educational, or analytical content free from antisemitic elements\\n\\n### UNCLEAR\\nContent where:\\n- The meaning or intent is ambiguous and could be interpreted multiple ways\\n- Insufficient context prevents definitive classification\\n- The text contains coded language that might or might not be antisemitic\\n- Additional information would be needed to determine the presence of antisemitism\\n\\n## 3. Annotation Examples\\n\\n### Clear Examples of ANTISEMITIC Content\\n\\n| Example Text | Explanation |\\n|--------------|-------------|\\n| \\\"Jews control all the banks and media worldwide\\\" | Classic conspiracy theory about Jewish collective power |\\n| \\\"The Holocaust never happened the way they claim\\\" | Holocaust denial |\\n| \\\"Jewish Americans care more about Israel than America\\\" | Dual loyalty accusation |\\n| \\\"Israel is doing to Palestinians what Nazis did to Jews\\\" | Nazi comparison |\\n| \\\"The Rothschilds and other Jewish bankers orchestrate global financial crises\\\" | Conspiracy theory about Jewish control |\\n| \\\"Jews use the Holocaust to gain sympathy and money\\\" | Accusation of exploiting the Holocaust |\\n\\n### Clear Examples of NOT_ANTISEMITIC Content\\n\\n| Example Text | Explanation |\\n|--------------|-------------|\\n| \\\"Israel's settlement policy violates international law\\\" | Legitimate policy criticism |\\n| \\\"The Israeli government should reconsider its approach to the two-state solution\\\" | Political criticism without antisemitic elements |\\n| \\\"Jewish contributions to science and arts have enriched our culture\\\" | Positive factual statement |\\n| \\\"The Israeli Supreme Court's recent decision raises constitutional questions\\\" | Factual analysis of legal matters |\\n| \\\"Many Israeli citizens oppose the current government's policies\\\" | Factual reporting about political diversity |\\n\\n### Edge Cases and Ambiguous Examples\\n\\n| Example Text | Label | Explanation |\\n|--------------|-------|-------------|\\n| \\\"They have too much influence in Hollywood\\\" | UNCLEAR | Ambiguous pronoun \\\"they\\\" - needs context to determine if referring to Jews |\\n| \\\"Israel shouldn't exist\\\" | ANTISEMITIC | Denies Jewish right to self-determination |\\n| \\\"Israel shouldn't exist in its current form\\\" | NOT_ANTISEMITIC | Political opinion about state structure, not denying right to exist |\\n| \\\"Powerful lobbies control our foreign policy\\\" | UNCLEAR | Could be antisemitic dog whistle or general political commentary |\\n| \\\"Questions need to be asked about historical narratives\\\" | UNCLEAR | Too vague - could be innocent inquiry or Holocaust denial |\\n\\n## 4. Handling Ambiguous Cases\\n\\n### Decision Tree for Ambiguous Content\\n\\n1. **Does the text reference the Holocaust?**\\n   - If denial, distortion, or minimization → **ANTISEMITIC**\\n   - If factual historical discussion → **NOT_ANTISEMITIC**\\n   - If vague questioning without context → **UNCLEAR**\\n\\n2. **Is criticism directed at Israel/Israeli government?**\\n   - Does it use Nazi comparisons or blood libel? → **ANTISEMITIC**\\n   - Does it deny Israel's right to exist? → **ANTISEMITIC**\\n   - Is it similar to criticism of other countries? → **NOT_ANTISEMITIC**\\n   - Is it about specific policies without antisemitic tropes? → **NOT_ANTISEMITIC**\\n\\n3. **Does the text reference Jewish people collectively?**\\n   - Does it attribute negative characteristics to all Jews? → **ANTISEMITIC**\\n   - Does it promote conspiracy theories? → **ANTISEMITIC**\\n   - Is it factual or statistical without stereotyping? → **NOT_ANTISEMITIC**\\n\\n4. **Are there coded words or dog whistles?**\\n   - \\\"Globalists,\\\" \\\"cosmopolitans,\\\" \\\"international bankers\\\" with negative connotation → Consider context, likely **ANTISEMITIC**\\n   - If meaning is genuinely ambiguous → **UNCLEAR**\\n\\n### Context Considerations\\n- Examine surrounding text for clarifying information\\n- Consider the source and platform if known\\n- Look for patterns across multiple statements\\n- Identify whether criticism targets actions/policies or people/identity\\n- Distinguish between ignorance and intentional antisemitism when possible\\n\\n## 5. Special Instructions\\n\\n### Coded Language\\nBe aware of common antisemitic dog whistles and coded language:\\n- \\\"Globalists\\\" or \\\"internationalists\\\" (when used pejoratively)\\n- \\\"Cultural Marxists\\\"\\n- \\\"(((Triple parentheses)))\\\" around names\\n- \\\"Cosmopolitan elite\\\"\\n- \\\"Puppet masters\\\" or \\\"string pullers\\\"\\n- References to \\\"Soros\\\" as controlling events\\n- \\\"Zionist Occupied Government\\\" (ZOG)\\n\\n### Cultural and Regional Differences\\n- Consider that antisemitic tropes may vary by region\\n- Be aware of translation issues that might affect meaning\\n- Recognize that some criticism may stem from different political contexts\\n- Account for historical and cultural factors that might influence expression\\n\\n## 6. Quality Checklist\\n\\nBefore finalizing your annotation, verify:\\n- [ ] You've read the entire text passage carefully\\n- [ ] You've considered the overall context\\n- [ ] You've checked against IHRA's specific examples\\n- [ ] You've distinguished between legitimate criticism and antisemitism\\n- [ ] You've identified any coded language or dog whistles\\n- [ ] You've applied the decision tree for ambiguous cases\\n- [ ] Your annotation is based on content, not assumptions about intent\\n- [ ] You've been consistent with similar cases\\n\\n## 7. Common Mistakes to Avoid\\n\\n1. **Over-labeling**: Not all criticism of Israel or Israeli policies is antisemitic\\n2. **Under-labeling**: Missing subtle dog whistles or coded antisemitic language\\n3. **Ignoring context**: Failing to consider surrounding text that clarifies meaning\\n4. **Intent confusion**: Focusing on presumed intent rather than actual content\\n5. **False equivalence**: Treating all negative mentions of Jews/Israel as equal\\n6. **Historical ignorance**: Not recognizing classical antisemitic tropes in modern forms\\n7. **Binary thinking**: Forcing unclear cases into ANTISEMITIC or NOT_ANTISEMITIC when UNCLEAR is appropriate\\n8. **Personal bias**: Allowing personal political views to influence annotation\\n\\n## 8. Final Reminders\\n\\n- **Consistency is key**: Apply the same standards across all annotations\\n- **When in doubt, refer to IHRA examples**: The definition provides specific illustrations to guide decisions\\n- **Document uncertainty**: Use UNCLEAR when genuinely ambiguous\\n- **Focus on content**: Analyze what is actually written, not what might be implied\\n- **Maintain objectivity**: Your role is to identify antisemitism as defined by IHRA, not to make broader political judgments\\n- **Remember the purpose**: These annotations help identify and combat antisemitism while preserving space for legitimate discourse\\n- **Context matters**: A statement's meaning can change significantly based on surrounding content\\n- **Stay updated**: Antisemitic language and tactics evolve; be aware of emerging patterns\\n\\nThe goal is to create reliable, consistent annotations that accurately identify antisemitic content according to the IHRA working definition while maintaining space for legitimate political criticism and discourse.\"\n",
      "}\n",
      "\n",
      "text:\n",
      "Herzog lit Hanukkah candles in the heart of the segregated regime, Hebron, where public roads are available only to Jews.\n",
      "He bestowed his office’s legitimacy to flagrant Israeli apartheid, celebrating normalization of this darkness.\n",
      "He is now President of The Violent Kahanists. https://t.co/p2teWXtp1W\n",
      "\n",
      "Respond with JSON only matching the schema.\n",
      "ChatCompletion(id='7a06808a-a867-ff2b-cf1d-99e5ea71da57_us-east-1', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content='First, the user provided annotations for two keys: \"Jerusalem Declaration\" and \"IHRA Definition\". These are guidelines for identifying antisemitism based on different definitions.\\n\\nMy task is to classify the given text and pick exactly one key from the annotations as the best match. The keys are \"Jerusalem Declaration\" and \"IHRA Definition\".\\n\\nThe text to classify is: \"Herzog lit Hanukkah candles in the heart of the segregated regime, Hebron, where public roads are available only to Jews. He bestowed his office’s legitimacy to flagrant Israeli apartheid, celebrating normalization of this darkness. He is now President of The Violent Kahanists. https://t.co/p2teWXtp1W\"\\n\\nI need to determine which guideline this text aligns with more closely. The instructions say: \"Pick exactly one key from annotations as the best match for text. If several could fit, choose the stricter or more specific one.\"\\n\\nBoth guidelines are about identifying antisemitism, but they differ in how they handle criticism of Israel. The Jerusalem Declaration (JDA) is more protective of political speech regarding Israel, while the IHRA definition is broader and might label more criticisms as antisemitic.\\n\\nLet me analyze the text:\\n\\n- It criticizes Israeli policies in Hebron, calling it a \"segregated regime\" and \"flagrant Israeli apartheid\".\\n\\n- It mentions that public roads are only for Jews, which is a factual claim about segregation.\\n\\n- It accuses Herzog of legitimizing apartheid and being associated with \"The Violent Kahanists\", which might refer to Kahanism, a far-right ideology.\\n\\nUnder JDA:\\n\\n- Criticism of Israeli policies, like apartheid, is NOT_ANTISEMITIC if evidence-based.\\n\\n- This text seems to be evidence-based criticism of state practices.\\n\\nUnder IHRA:\\n\\n- It might be seen as applying double standards to Israel or comparing it to apartheid, which could be labeled ANTISEMITIC if not applied to other countries.\\n\\n- IHRA specifically mentions: \"Applying double standards to Israel by requiring of it a behavior not expected or demanded of any other democratic nation.\"\\n\\nThe text calls it \"Israeli apartheid\", which is a comparison. IHRA might view this as antisemitic, especially if it\\'s not balanced with criticisms of other nations.\\n\\nHowever, the JDA explicitly allows for evidence-based criticism of Israeli state policies and practices, and advocacy for Palestinian rights.\\n\\nThe text doesn\\'t seem to target Jews as Jews; it\\'s targeting Israeli policies and Herzog\\'s actions.\\n\\nNow, which one is the best match? I need to choose the stricter or more specific one if both fit.\\n\\nThe task is to pick the key that best matches for classifying the text, meaning which guideline would classify this text as antisemitic or not.\\n\\nRe-reading the task: \"Pick exactly one key from annotations as the best match for text.\" And the output is for the classification system.\\n\\nIn the context, I think it means which annotation guideline applies best to this text for classification purposes.\\n\\nBut the'))], created=1756980188, model='grok-3-mini', object='chat.completion', service_tier=None, system_fingerprint='fp_6a09108ff5', usage=CompletionUsage(completion_tokens=0, prompt_tokens=4036, total_tokens=4636, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=600, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=6, text_tokens=4036, image_tokens=0), num_sources_used=0))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Minimal Grok classifier pipeline\n",
    "# =========================\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Tuple\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "ANNOTATION_GLOB = \"prompts/annotation_guidelines\"\n",
    "MODEL = \"grok-3-mini\"  # use only Grok\n",
    "CSV_PATH = \"GoldStanderDataSet.csv\"  # update if needed\n",
    "\n",
    "# -------------------------\n",
    "# Load defs\n",
    "# -------------------------\n",
    "def load_definitions(definitions_dir: str) -> Dict[str, str]:\n",
    "    d: Dict[str, str] = {}\n",
    "    p = Path(definitions_dir)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Directory {definitions_dir} not found\")\n",
    "    for md in p.glob(\"*.md\"):\n",
    "        d[md.stem] = md.read_text(encoding=\"utf-8\").strip()\n",
    "    if not d:\n",
    "        raise RuntimeError(\"No definitions found\")\n",
    "    return d\n",
    "\n",
    "annotations = load_definitions(ANNOTATION_GLOB)\n",
    "\n",
    "# -------------------------\n",
    "# Prompt builder\n",
    "# -------------------------\n",
    "CLASSIFIER_SYSTEM = \"\"\"You are a careful text classifier.\n",
    "\n",
    "You will be given:\n",
    "- a JSON object named \"annotations\" mapping names to definitions\n",
    "- a \"text\" string to classify\n",
    "\n",
    "Task:\n",
    "- Pick exactly one key from annotations as the best match for text.\n",
    "- If several could fit, choose the stricter or more specific one.\n",
    "- Output JSON only, no extra words.\n",
    "\n",
    "Output schema:\n",
    "{\n",
    "  \"answer\": \"<one key from annotations>\",\n",
    "  \"description\": \"<1 to 3 sentences explaining the choice>\"\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- answer must be exactly one of annotations keys\n",
    "- description must cite concrete cues from the text\n",
    "\"\"\"\n",
    "\n",
    "def generate_prompt(text: str, annotations: Dict[str, str]) -> str:\n",
    "    \"\"\"Return the user prompt content as one string (easy to log).\"\"\"\n",
    "    return (\n",
    "        \"annotations:\\n\"\n",
    "        + json.dumps(annotations, ensure_ascii=False, indent=2)\n",
    "        + \"\\n\\ntext:\\n\"\n",
    "        + text\n",
    "        + \"\\n\\nRespond with JSON only matching the schema.\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Grok client + call\n",
    "# -------------------------\n",
    "def grok_client() -> OpenAI:\n",
    "    # Needs GROK_API_KEY in env\n",
    "    return OpenAI(api_key=os.getenv(\"GROK_API_KEY\"), base_url=\"https://api.x.ai/v1\")\n",
    "\n",
    "def llm(prompt: str, model: str = MODEL) -> str:\n",
    "    \"\"\"Send a single-turn chat with fixed system prompt. Return text.\"\"\"\n",
    "    client = grok_client()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.0,\n",
    "        max_tokens=600,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": CLASSIFIER_SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    # return resp.choices[0].message.content or \"\"\n",
    "    return resp\n",
    "\n",
    "# -------------------------\n",
    "# Robust JSON extraction\n",
    "# -------------------------\n",
    "def _first_json_object(s: str) -> str:\n",
    "    \"\"\"Extract the first top-level JSON object by tracking braces. No regex recursion.\"\"\"\n",
    "    start = s.find(\"{\")\n",
    "    if start == -1:\n",
    "        raise ValueError(\"No '{' found in response\")\n",
    "    depth, i, in_str, esc = 0, start, False, False\n",
    "    while i < len(s):\n",
    "        ch = s[i]\n",
    "        if in_str:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == \"\\\\\":\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_str = True\n",
    "            elif ch == \"{\":\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    return s[start : i + 1]\n",
    "        i += 1\n",
    "    raise ValueError(\"Unbalanced JSON braces in response\")\n",
    "\n",
    "def structure_output(raw_resp: str) -> str:\n",
    "    \"\"\"Return the JSON text only, stripping any extra prose.\"\"\"\n",
    "    raw_resp = raw_resp.strip()\n",
    "    try:\n",
    "        # Fast path: already clean JSON\n",
    "        json.loads(raw_resp)\n",
    "        return raw_resp\n",
    "    except Exception:\n",
    "        return _first_json_object(raw_resp)\n",
    "\n",
    "def extract_pred_and_desc(json_text: str, allowed_keys: Dict[str, str]) -> Tuple[str, str]:\n",
    "    \"\"\"Parse JSON and validate 'answer' against provided annotations keys.\"\"\"\n",
    "    obj = json.loads(json_text)\n",
    "    answer = obj.get(\"answer\", \"\")\n",
    "    desc = obj.get(\"description\", \"\")\n",
    "    if answer not in allowed_keys:\n",
    "        # Choose a stable fallback so the pipeline does not crash\n",
    "        answer = next(iter(allowed_keys.keys()))\n",
    "        if not desc:\n",
    "            desc = \"Model answer not in annotation keys; fell back to the first key.\"\n",
    "    return answer, desc\n",
    "\n",
    "# -------------------------\n",
    "# One-sample demo (simple to debug)\n",
    "# -------------------------\n",
    "# Load data (only once)\n",
    "twitter_df = pd.read_csv(CSV_PATH, encoding=\"cp1252\", low_memory=False)\n",
    "\n",
    "# Pick one positive example\n",
    "one_sample_df = twitter_df[twitter_df[\"Biased\"] == 1].sample(1, random_state=42)\n",
    "row = one_sample_df.iloc[0]\n",
    "text = str(row[\"Text\"])\n",
    "keyword = row.get(\"Keyword\", None)\n",
    "\n",
    "# Build prompt -> call Grok -> parse -> print\n",
    "prompt = generate_prompt(text, annotations)\n",
    "print(prompt)\n",
    "resp = llm(prompt, MODEL)\n",
    "print(resp)\n",
    "resp_text = structure_output(resp)\n",
    "# prediction, desc = extract_pred_and_desc(resp_text, annotations)\n",
    "\n",
    "# print(\"TEXT:\", text)\n",
    "# print(\"KEYWORD:\", keyword)\n",
    "# print(\"PREDICTION:\", prediction)\n",
    "# print(\"DESC:\", desc)\n",
    "\n",
    "# # -------------------------\n",
    "# # If you want a tiny loop to make a DF from N samples:\n",
    "# # -------------------------\n",
    "# def classify_samples_df(samples_df: pd.DataFrame, annotations: Dict[str, str]) -> pd.DataFrame:\n",
    "#     rows = []\n",
    "#     for _, r in samples_df.iterrows():\n",
    "#         t = str(r[\"Text\"])\n",
    "#         k = r.get(\"Keyword\", None)\n",
    "#         p = generate_prompt(t, annotations)\n",
    "#         raw = llm(p, MODEL)\n",
    "#         try:\n",
    "#             jt = structure_output(raw)\n",
    "#             pred, d = extract_pred_and_desc(jt, annotations)\n",
    "#         except Exception as e:\n",
    "#             pred, d = \"\", f\"Parse error: {e}\"\n",
    "#         rows.append({\"Text\": t, \"Keyword\": k, \"Definition_pred\": pred, \"description\": d})\n",
    "#     return pd.DataFrame(rows, columns=[\"Text\", \"Keyword\", \"Definition_pred\", \"description\"])\n",
    "\n",
    "# Example:\n",
    "# samples = twitter_df[twitter_df[\"Biased\"] == 1].sample(5, random_state=42)\n",
    "# out_df = classify_samples_df(samples, annotations)\n",
    "# print(out_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meaningful-transparency (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
